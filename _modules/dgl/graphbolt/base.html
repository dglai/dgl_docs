<!DOCTYPE html>

<html class="writer-html5" data-content_root="../../../" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>dgl.graphbolt.base — DGL 2.4 documentation</title>
<link href="../../../_static/pygments.css?v=80d5e7a1" rel="stylesheet" type="text/css"/>
<link href="../../../_static/css/theme.css?v=19f00094" rel="stylesheet" type="text/css"/>
<link href="../../../_static/graphviz.css?v=fd3f3429" rel="stylesheet" type="text/css"/>
<link href="../../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="../../../_static/css/custom.css?v=0bf289b5" rel="stylesheet" type="text/css"/>
<!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script src="../../../_static/jquery.js?v=5d32c60e"></script>
<script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
<script src="../../../_static/documentation_options.js?v=9caaf7ed"></script>
<script src="../../../_static/doctools.js?v=9a2dae69"></script>
<script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../../_static/copybutton.js?v=ccdb6887"></script>
<script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script src="../../../_static/js/theme.js"></script>
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../../../index.html">
            DGL
          </a>
<div class="version">
                2.4
              </div>
<div role="search">
<form action="../../../search.html" class="wy-form" id="rtd-search-form" method="get">
<input aria-label="Search docs" name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div><div aria-label="Navigation menu" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../stochastic_training/index.html">🆕 Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide_cn/index.html">用户指南【包含过时信息】</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide_ko/index.html">사용자 가이드[시대에 뒤쳐진]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../graphtransformer/index.html">🆕 Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.graphbolt.html">🆕 dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources.html">Resources</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift"><nav aria-label="Mobile navigation menu" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../../../index.html">DGL</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="Page navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a aria-label="Home" class="icon icon-home" href="../../../index.html"></a></li>
<li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
<li class="breadcrumb-item active">dgl.graphbolt.base</li>
<li class="wy-breadcrumbs-aside">
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<h1>Source code for dgl.graphbolt.base</h1><div class="highlight"><pre>
<span></span><span class="sd">"""Base types and utilities for Graph Bolt."""</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.torch_version</span> <span class="kn">import</span> <span class="n">TorchVersion</span>

<span class="k">if</span> <span class="p">(</span>
    <span class="n">TorchVersion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="s2">"2.3.0"</span>
    <span class="ow">and</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="s2">"2.3.1"</span>
<span class="p">):</span>
    <span class="c1"># Due to https://github.com/dmlc/dgl/issues/7380, for torch 2.3.0, we need</span>
    <span class="c1"># to check if dill is available before using it.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">datapipes</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">DILL_AVAILABLE</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">_import_utils</span><span class="o">.</span><span class="n">dill_available</span><span class="p">()</span>
    <span class="p">)</span>

<span class="c1"># pylint: disable=wrong-import-position</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">functional_datapipe</span><span class="p">,</span> <span class="n">IterDataPipe</span>

<span class="kn">from</span> <span class="nn">.internal_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_nonproperty_attributes</span><span class="p">,</span>
    <span class="n">recursive_apply</span><span class="p">,</span>
    <span class="n">recursive_apply_reduce_all</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"CANONICAL_ETYPE_DELIMITER"</span><span class="p">,</span>
    <span class="s2">"ORIGINAL_EDGE_ID"</span><span class="p">,</span>
    <span class="s2">"etype_str_to_tuple"</span><span class="p">,</span>
    <span class="s2">"etype_tuple_to_str"</span><span class="p">,</span>
    <span class="s2">"CopyTo"</span><span class="p">,</span>
    <span class="s2">"Waiter"</span><span class="p">,</span>
    <span class="s2">"Bufferer"</span><span class="p">,</span>
    <span class="s2">"EndMarker"</span><span class="p">,</span>
    <span class="s2">"isin"</span><span class="p">,</span>
    <span class="s2">"index_select"</span><span class="p">,</span>
    <span class="s2">"expand_indptr"</span><span class="p">,</span>
    <span class="s2">"indptr_edge_ids"</span><span class="p">,</span>
    <span class="s2">"CSCFormatBase"</span><span class="p">,</span>
    <span class="s2">"seed"</span><span class="p">,</span>
    <span class="s2">"seed_type_str_to_ntypes"</span><span class="p">,</span>
    <span class="s2">"get_host_to_device_uva_stream"</span><span class="p">,</span>
    <span class="s2">"get_device_to_host_uva_stream"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">CANONICAL_ETYPE_DELIMITER</span> <span class="o">=</span> <span class="s2">":"</span>
<span class="n">ORIGINAL_EDGE_ID</span> <span class="o">=</span> <span class="s2">"_ORIGINAL_EDGE_ID"</span>


<span class="c1"># There needs to be a single instance of the uva_stream, if it is created</span>
<span class="c1"># multiple times, it leads to multiple CUDA memory pools and memory leaks.</span>
<span class="k">def</span> <span class="nf">get_host_to_device_uva_stream</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""The host to device copy stream to be used for pipeline parallelism."""</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">get_host_to_device_uva_stream</span><span class="p">,</span> <span class="s2">"stream"</span><span class="p">):</span>
        <span class="n">get_host_to_device_uva_stream</span><span class="o">.</span><span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">(</span><span class="n">priority</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">get_host_to_device_uva_stream</span><span class="o">.</span><span class="n">stream</span>


<span class="k">def</span> <span class="nf">get_device_to_host_uva_stream</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""The device to host copy stream to be used for pipeline parallelism."""</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">get_device_to_host_uva_stream</span><span class="p">,</span> <span class="s2">"stream"</span><span class="p">):</span>
        <span class="n">get_device_to_host_uva_stream</span><span class="o">.</span><span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">(</span><span class="n">priority</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">get_device_to_host_uva_stream</span><span class="o">.</span><span class="n">stream</span>


<div class="viewcode-block" id="seed">
<a class="viewcode-back" href="../../../generated/dgl.graphbolt.seed.html#dgl.graphbolt.seed">[docs]</a>
<span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Set the random seed of Graphbolt.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    val : int</span>
<span class="sd">        The seed.</span>
<span class="sd">    """</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">graphbolt</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="n">val</span><span class="p">)</span></div>



<div class="viewcode-block" id="isin">
<a class="viewcode-back" href="../../../generated/dgl.graphbolt.isin.html#dgl.graphbolt.isin">[docs]</a>
<span class="k">def</span> <span class="nf">isin</span><span class="p">(</span><span class="n">elements</span><span class="p">,</span> <span class="n">test_elements</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Tests if each element of elements is in test_elements. Returns a boolean</span>
<span class="sd">    tensor of the same shape as elements that is True for elements in</span>
<span class="sd">    test_elements and False otherwise.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    elements : torch.Tensor</span>
<span class="sd">        A 1D tensor represents the input elements.</span>
<span class="sd">    test_elements : torch.Tensor</span>
<span class="sd">        A 1D tensor represents the values to test against for each input.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; isin(torch.tensor([1, 2, 3, 4]), torch.tensor([2, 3]))</span>
<span class="sd">    tensor([[False,  True,  True,  False]])</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="n">elements</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"Elements should be 1D tensor."</span>
    <span class="k">assert</span> <span class="n">test_elements</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"Test_elements should be 1D tensor."</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">graphbolt</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">elements</span><span class="p">,</span> <span class="n">test_elements</span><span class="p">)</span></div>



<span class="k">if</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="s2">"2.2.0a0"</span><span class="p">):</span>

    <span class="n">torch_fake_decorator</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">impl_abstract</span>
        <span class="k">if</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="s2">"2.4.0a0"</span><span class="p">)</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">register_fake</span>
    <span class="p">)</span>

    <span class="nd">@torch_fake_decorator</span><span class="p">(</span><span class="s2">"graphbolt::expand_indptr"</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">expand_indptr_fake</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">node_ids</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Fake implementation of expand_indptr for torch.compile() support."""</span>
        <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">get_ctx</span><span class="p">()</span><span class="o">.</span><span class="n">new_dynamic_size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">node_ids</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">return</span> <span class="n">indptr</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<div class="viewcode-block" id="expand_indptr">
<a class="viewcode-back" href="../../../generated/dgl.graphbolt.expand_indptr.html#dgl.graphbolt.expand_indptr">[docs]</a>
<span class="k">def</span> <span class="nf">expand_indptr</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">node_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Converts a given indptr offset tensor to a COO format tensor. If</span>
<span class="sd">    node_ids is not given, it is assumed to be equal to</span>
<span class="sd">    torch.arange(indptr.size(0) - 1, dtype=dtype, device=indptr.device).</span>

<span class="sd">    This is equivalent to</span>

<span class="sd">    .. code:: python</span>

<span class="sd">       if node_ids is None:</span>
<span class="sd">           node_ids = torch.arange(len(indptr) - 1, dtype=dtype, device=indptr.device)</span>
<span class="sd">       return node_ids.to(dtype).repeat_interleave(indptr.diff())</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    indptr : torch.Tensor</span>
<span class="sd">        A 1D tensor represents the csc_indptr tensor.</span>
<span class="sd">    dtype : Optional[torch.dtype]</span>
<span class="sd">        The dtype of the returned output tensor.</span>
<span class="sd">    node_ids : Optional[torch.Tensor]</span>
<span class="sd">        A 1D tensor represents the column node ids that the returned tensor will</span>
<span class="sd">        be populated with.</span>
<span class="sd">    output_size : Optional[int]</span>
<span class="sd">        The size of the output tensor. Should be equal to indptr[-1]. Using this</span>
<span class="sd">        argument avoids a stream synchronization to calculate the output shape.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The converted COO tensor with values from node_ids.</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="n">indptr</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"Indptr should be 1D tensor."</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span>
        <span class="n">node_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="p">),</span> <span class="s2">"One of node_ids or dtype must be given."</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">node_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">node_ids</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="p">),</span> <span class="s2">"Node_ids should be 1D tensor."</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">node_ids</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">graphbolt</span><span class="o">.</span><span class="n">expand_indptr</span><span class="p">(</span>
        <span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">node_ids</span><span class="p">,</span> <span class="n">output_size</span>
    <span class="p">)</span></div>



<span class="k">if</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="s2">"2.2.0a0"</span><span class="p">):</span>

    <span class="n">torch_fake_decorator</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">impl_abstract</span>
        <span class="k">if</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">TorchVersion</span><span class="p">(</span><span class="s2">"2.4.0a0"</span><span class="p">)</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">register_fake</span>
    <span class="p">)</span>

    <span class="nd">@torch_fake_decorator</span><span class="p">(</span><span class="s2">"graphbolt::indptr_edge_ids"</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">indptr_edge_ids_fake</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Fake implementation of indptr_edge_ids for torch.compile() support."""</span>
        <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">get_ctx</span><span class="p">()</span><span class="o">.</span><span class="n">new_dynamic_size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">offset</span><span class="o">.</span><span class="n">dtype</span>
        <span class="k">return</span> <span class="n">indptr</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">indptr_edge_ids</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Converts a given indptr offset tensor to a COO format tensor for the edge</span>
<span class="sd">    ids. For a given indptr [0, 2, 5, 7] and offset tensor [0, 100, 200], the</span>
<span class="sd">    output will be [0, 1, 100, 101, 102, 201, 202]. If offset was not provided,</span>
<span class="sd">    the output would be [0, 1, 0, 1, 2, 0, 1].</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    indptr : torch.Tensor</span>
<span class="sd">        A 1D tensor represents the csc_indptr tensor.</span>
<span class="sd">    dtype : Optional[torch.dtype]</span>
<span class="sd">        The dtype of the returned output tensor.</span>
<span class="sd">    offset : Optional[torch.Tensor]</span>
<span class="sd">        A 1D tensor represents the offsets that the returned tensor will be</span>
<span class="sd">        populated with.</span>
<span class="sd">    output_size : Optional[int]</span>
<span class="sd">        The size of the output tensor. Should be equal to indptr[-1]. Using this</span>
<span class="sd">        argument avoids a stream synchronization to calculate the output shape.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The converted COO edge ids tensor.</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="n">indptr</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"Indptr should be 1D tensor."</span>
    <span class="k">assert</span> <span class="n">offset</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">offset</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"Offset should be 1D tensor."</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">offset</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">graphbolt</span><span class="o">.</span><span class="n">indptr_edge_ids</span><span class="p">(</span>
        <span class="n">indptr</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">output_size</span>
    <span class="p">)</span>


<div class="viewcode-block" id="index_select">
<a class="viewcode-back" href="../../../generated/dgl.graphbolt.index_select.html#dgl.graphbolt.index_select">[docs]</a>
<span class="k">def</span> <span class="nf">index_select</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Returns a new tensor which indexes the input tensor along dimension dim</span>
<span class="sd">    using the entries in index.</span>

<span class="sd">    The returned tensor has the same number of dimensions as the original tensor</span>
<span class="sd">    (tensor). The first dimension has the same size as the length of index;</span>
<span class="sd">    other dimensions have the same size as in the original tensor.</span>

<span class="sd">    When tensor is a pinned tensor and index.is_cuda is True, the operation runs</span>
<span class="sd">    on the CUDA device and the returned tensor will also be on CUDA.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    tensor : torch.Tensor</span>
<span class="sd">        The input tensor.</span>
<span class="sd">    index : torch.Tensor</span>
<span class="sd">        The 1-D tensor containing the indices to index.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        The indexed input tensor, equivalent to tensor[index]. If index is in</span>
<span class="sd">        pinned memory, then the result is placed into pinned memory as well.</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="n">index</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"Index should be 1D tensor."</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">graphbolt</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>



<div class="viewcode-block" id="etype_tuple_to_str">
<a class="viewcode-back" href="../../../generated/dgl.graphbolt.etype_tuple_to_str.html#dgl.graphbolt.etype_tuple_to_str">[docs]</a>
<span class="k">def</span> <span class="nf">etype_tuple_to_str</span><span class="p">(</span><span class="n">c_etype</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Convert canonical etype from tuple to string.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; c_etype = ("user", "like", "item")</span>
<span class="sd">    &gt;&gt;&gt; c_etype_str = _etype_tuple_to_str(c_etype)</span>
<span class="sd">    &gt;&gt;&gt; print(c_etype_str)</span>
<span class="sd">    "user:like:item"</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c_etype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">c_etype</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">"Passed-in canonical etype should be in format of (str, str, str). "</span>
        <span class="sa">f</span><span class="s2">"But got </span><span class="si">{</span><span class="n">c_etype</span><span class="si">}</span><span class="s2">."</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">CANONICAL_ETYPE_DELIMITER</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">c_etype</span><span class="p">)</span></div>



<div class="viewcode-block" id="etype_str_to_tuple">
<a class="viewcode-back" href="../../../generated/dgl.graphbolt.etype_str_to_tuple.html#dgl.graphbolt.etype_str_to_tuple">[docs]</a>
<span class="k">def</span> <span class="nf">etype_str_to_tuple</span><span class="p">(</span><span class="n">c_etype</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Convert canonical etype from string to tuple.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; c_etype_str = "user:like:item"</span>
<span class="sd">    &gt;&gt;&gt; c_etype = _etype_str_to_tuple(c_etype_str)</span>
<span class="sd">    &gt;&gt;&gt; print(c_etype)</span>
<span class="sd">    ("user", "like", "item")</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c_etype</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">c_etype</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">c_etype</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">CANONICAL_ETYPE_DELIMITER</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span>
        <span class="s2">"Passed-in canonical etype should be in format of 'str:str:str'. "</span>
        <span class="sa">f</span><span class="s2">"But got </span><span class="si">{</span><span class="n">c_etype</span><span class="si">}</span><span class="s2">."</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>



<span class="k">def</span> <span class="nf">seed_type_str_to_ntypes</span><span class="p">(</span><span class="n">seed_type</span><span class="p">,</span> <span class="n">seed_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Convert seeds type to node types from string to list.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    1. node pairs</span>

<span class="sd">    &gt;&gt;&gt; seed_type = "user:like:item"</span>
<span class="sd">    &gt;&gt;&gt; seed_size = 2</span>
<span class="sd">    &gt;&gt;&gt; node_type = seed_type_str_to_ntypes(seed_type, seed_size)</span>
<span class="sd">    &gt;&gt;&gt; print(node_type)</span>
<span class="sd">    ["user", "item"]</span>

<span class="sd">    2. hyperlink</span>

<span class="sd">    &gt;&gt;&gt; seed_type = "query:user:item"</span>
<span class="sd">    &gt;&gt;&gt; seed_size = 3</span>
<span class="sd">    &gt;&gt;&gt; node_type = seed_type_str_to_ntypes(seed_type, seed_size)</span>
<span class="sd">    &gt;&gt;&gt; print(node_type)</span>
<span class="sd">    ["query", "user", "item"]</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">seed_type</span><span class="p">,</span> <span class="nb">str</span>
    <span class="p">),</span> <span class="sa">f</span><span class="s2">"Passed-in seed type should be string, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">seed_type</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">ntypes</span> <span class="o">=</span> <span class="n">seed_type</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">CANONICAL_ETYPE_DELIMITER</span><span class="p">)</span>
    <span class="n">is_hyperlink</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ntypes</span><span class="p">)</span> <span class="o">==</span> <span class="n">seed_size</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_hyperlink</span><span class="p">:</span>
        <span class="n">ntypes</span> <span class="o">=</span> <span class="n">ntypes</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">ntypes</span>


<span class="k">def</span> <span class="nf">apply_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Apply `to` function to object x only if it has `to`."""</span>

    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">"pinned"</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">"pin_memory"</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">"to"</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">non_blocking</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_object_pinned</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Recursively check all members of the object and return True if only if</span>
<span class="sd">    all are pinned."""</span>

    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">get_nonproperty_attributes</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
        <span class="n">member_result</span> <span class="o">=</span> <span class="n">recursive_apply_reduce_all</span><span class="p">(</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">x</span><span class="o">.</span><span class="n">is_pinned</span><span class="p">(),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">member_result</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="CopyTo">
<a class="viewcode-back" href="../../../generated/dgl.graphbolt.CopyTo.html#dgl.graphbolt.CopyTo">[docs]</a>
<span class="nd">@functional_datapipe</span><span class="p">(</span><span class="s2">"copy_to"</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">CopyTo</span><span class="p">(</span><span class="n">IterDataPipe</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""DataPipe that transfers each element yielded from the previous DataPipe</span>
<span class="sd">    to the given device. For MiniBatch, only the related attributes</span>
<span class="sd">    (automatically inferred) will be transferred by default.</span>

<span class="sd">    Functional name: :obj:`copy_to`.</span>

<span class="sd">    When ``data`` has ``to`` method implemented, ``CopyTo`` will be equivalent</span>
<span class="sd">    to</span>

<span class="sd">    .. code:: python</span>

<span class="sd">       for data in datapipe:</span>
<span class="sd">           yield data.to(device)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    datapipe : DataPipe</span>
<span class="sd">        The DataPipe.</span>
<span class="sd">    device : torch.device</span>
<span class="sd">        The PyTorch CUDA device.</span>
<span class="sd">    non_blocking : bool</span>
<span class="sd">        Whether the copy should be performed without blocking. All elements have</span>
<span class="sd">        to be already in pinned system memory if enabled. Default is False.</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">datapipe</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span> <span class="o">=</span> <span class="n">datapipe</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">non_blocking</span> <span class="o">=</span> <span class="n">non_blocking</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">recursive_apply</span><span class="p">(</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">apply_to</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_blocking</span>
            <span class="p">)</span></div>



<span class="nd">@functional_datapipe</span><span class="p">(</span><span class="s2">"mark_end"</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">EndMarker</span><span class="p">(</span><span class="n">IterDataPipe</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Used to mark the end of a datapipe and is a no-op."""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">datapipe</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span> <span class="o">=</span> <span class="n">datapipe</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span>


<span class="nd">@functional_datapipe</span><span class="p">(</span><span class="s2">"buffer"</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Bufferer</span><span class="p">(</span><span class="n">IterDataPipe</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Buffers items before yielding them.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    datapipe : DataPipe</span>
<span class="sd">        The data pipeline.</span>
<span class="sd">    buffer_size : int, optional</span>
<span class="sd">        The size of the buffer which stores the fetched samples. If data coming</span>
<span class="sd">        from datapipe has latency spikes, consider setting to a higher value.</span>
<span class="sd">        Default is 1.</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">datapipe</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span> <span class="o">=</span> <span class="n">datapipe</span>
        <span class="k">if</span> <span class="n">buffer_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"'buffer_size' is required to be a positive integer."</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">maxlen</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">return_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="k">yield</span> <span class="n">return_data</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">maxlen</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">IterDataPipe</span><span class="o">.</span><span class="n">getstate_hook</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">IterDataPipe</span><span class="o">.</span><span class="n">getstate_hook</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span><span class="p">,</span> <span class="n">buffer_size</span> <span class="o">=</span> <span class="n">state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Resets the state of the datapipe."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>


<span class="nd">@functional_datapipe</span><span class="p">(</span><span class="s2">"wait"</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Waiter</span><span class="p">(</span><span class="n">IterDataPipe</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Calls the wait function of all items."""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">datapipe</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span> <span class="o">=</span> <span class="n">datapipe</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">datapipe</span><span class="p">:</span>
            <span class="n">data</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">data</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">CSCFormatBase</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Basic class representing data in Compressed Sparse Column (CSC) format.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; indptr = torch.tensor([0, 1, 3])</span>
<span class="sd">    &gt;&gt;&gt; indices = torch.tensor([1, 4, 2])</span>
<span class="sd">    &gt;&gt;&gt; csc_foramt_base = CSCFormatBase(indptr=indptr, indices=indices)</span>
<span class="sd">    &gt;&gt;&gt; print(csc_format_base.indptr)</span>
<span class="sd">    ... torch.tensor([0, 1, 3])</span>
<span class="sd">    &gt;&gt;&gt; print(csc_foramt_base)</span>
<span class="sd">    ... torch.tensor([1, 4, 2])</span>
<span class="sd">    """</span>

    <span class="n">indptr</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indptr</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indptr</span> <span class="o">=</span> <span class="n">indptr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">indptr</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">indptr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">indices</span>
            <span class="p">),</span> <span class="s2">"The last element of indptr should be the same as the length of indices."</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_csc_format_base_str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span>  <span class="c1"># pylint: disable=invalid-name</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Copy `CSCFormatBase` to the specified device using reflection."""</span>

        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="c1"># Only copy member variables.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">attr</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"__"</span><span class="p">):</span>
                <span class="nb">setattr</span><span class="p">(</span>
                    <span class="bp">self</span><span class="p">,</span>
                    <span class="n">attr</span><span class="p">,</span>
                    <span class="n">recursive_apply</span><span class="p">(</span>
                        <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">),</span>
                        <span class="n">apply_to</span><span class="p">,</span>
                        <span class="n">device</span><span class="p">,</span>
                        <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">pin_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Copy `SampledSubgraph` to the pinned memory using reflection."""</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"pinned"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_pinned</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Check whether `SampledSubgraph` is pinned using reflection."""</span>

        <span class="k">return</span> <span class="n">is_object_pinned</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_csc_format_base_str</span><span class="p">(</span><span class="n">csc_format_base</span><span class="p">:</span> <span class="n">CSCFormatBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">final_str</span> <span class="o">=</span> <span class="s2">"CSCFormatBase("</span>

    <span class="k">def</span> <span class="nf">_add_indent</span><span class="p">(</span><span class="n">_str</span><span class="p">,</span> <span class="n">indent</span><span class="p">):</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">_str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">" "</span> <span class="o">*</span> <span class="n">indent</span> <span class="o">+</span> <span class="n">line</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="k">return</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>

    <span class="n">final_str</span> <span class="o">+=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">"indptr=</span><span class="si">{</span><span class="n">_add_indent</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">csc_format_base</span><span class="o">.</span><span class="n">indptr</span><span class="p">),</span><span class="w"> </span><span class="mi">21</span><span class="p">)</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">" "</span> <span class="o">*</span> <span class="mi">14</span>
    <span class="p">)</span>
    <span class="n">final_str</span> <span class="o">+=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">"indices=</span><span class="si">{</span><span class="n">_add_indent</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">csc_format_base</span><span class="o">.</span><span class="n">indices</span><span class="p">),</span><span class="w"> </span><span class="mi">22</span><span class="p">)</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="s2">")"</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">final_str</span>
</pre></div>
</div>
</div>
<footer>
<hr/>
<div role="contentinfo">
<p>© Copyright 2018, DGL Team.</p>
</div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
</div>
</div>
</section>
</div>
<div aria-label="Versions" class="rst-versions" data-toggle="rst-versions" role="note">
<span class="rst-current-version" data-toggle="rst-current-version">
<span class="fa fa-book"> Read the Docs</span>
<span id="version-placeholder">v: latest</span>
<span class="fa fa-caret-down"></span>
</span>
<div class="rst-other-versions">
<dl>
<dt>Versions</dt>
<div id="version-list">
<!-- 动态插入的版本列表将出现在这里 -->
</div>
</dl>
<dl>
<dt>Downloads</dt>
<!-- 下载内容 -->
</dl>
<dl>
<dt>On Read the Docs</dt>
<dd><a href="//doc-build.dgl.ai/projects/dgl/?fromdocs=dgl">Project Home</a></dd>
<dd><a href="//doc-build.dgl.ai/builds/dgl/?fromdocs=dgl">Builds</a></dd>
</dl>
</div>
</div>
<script>
        document.addEventListener("DOMContentLoaded", function() {
            fetch('/dgl_docs/branches.json')
                .then(response => response.json())
                .then(data => {
                    var versionListDiv = document.getElementById('version-list');
                    data.branches.forEach(function(branch) {
                        var dd = document.createElement('dd');
                        var a = document.createElement('a');
                        a.href = branch.url;
                        a.textContent = branch.name;
                        dd.appendChild(a);
                        versionListDiv.appendChild(dd);
                    });
                })
                .catch(error => console.error('Error loading branches:', error));
        });
        document.addEventListener("DOMContentLoaded", function() {
            // 获取当前路径
            var path = window.location.pathname;
            var versionPlaceholder = document.getElementById('version-placeholder');

            // 检查路径中是否包含 'en'
            if (path.includes('/en/')) {
                // 提取 'en' 后的文件夹作为版本号
                var parts = path.split('/en/');
                if (parts[1]) {
                    var folders = parts[1].split('/');
                    if (folders.length > 0 && folders[0]) {
                        versionPlaceholder.textContent = 'v: ' + folders[0];
                    } else {
                        versionPlaceholder.textContent = 'v: latest';
                    }
                } else {
                    versionPlaceholder.textContent = 'v: latest';
                }
            } else {
                versionPlaceholder.textContent = 'v: latest';
            }
        });
    </script>

<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
</body>
</html>