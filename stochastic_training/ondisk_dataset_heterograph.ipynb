{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnFhPMaAfLtJ"
   },
   "source": [
    "# OnDiskDataset for Heterogeneous Graph\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/stochastic_training/ondisk_dataset_heterograph.ipynb) [![GitHub](https://img.shields.io/badge/-View%20on%20GitHub-181717?logo=github&logoColor=ffffff)](https://github.com/dmlc/dgl/blob/master/notebooks/stochastic_training/ondisk_dataset_heterograph.ipynb)\n",
    "\n",
    "This tutorial shows how to create `OnDiskDataset` for heterogeneous graph that could be used in **GraphBolt** framework. The major difference from creating dataset for homogeneous graph is that we need to specify node/edge types for edges, feature data, training/validation/test sets.\n",
    "\n",
    "By the end of this tutorial, you will be able to\n",
    "\n",
    "- organize graph structure data.\n",
    "- organize feature data.\n",
    "- organize training/validation/test set for specific tasks.\n",
    "\n",
    "To create an ``OnDiskDataset`` object, you need to organize all the data including graph structure, feature data and tasks into a directory. The directory should contain a ``metadata.yaml`` file that describes the metadata of the dataset.\n",
    "\n",
    "Now let's generate various data step by step and organize them together to instantiate `OnDiskDataset` finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlb19DtWgtzq"
   },
   "source": [
    "## Install DGL package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:19:43.153840Z",
     "iopub.status.busy": "2024-10-31T13:19:43.153490Z",
     "iopub.status.idle": "2024-10-31T13:19:46.202946Z",
     "shell.execute_reply": "2024-10-31T13:19:46.201897Z"
    },
    "id": "UojlT9ZGgyr9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels-test/repo.html\r\n",
      "Requirement already satisfied: dgl in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (2.2a240410)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (1.14.1)\r\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (3.4.2)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (4.66.6)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (6.1.0)\r\n",
      "Requirement already satisfied: torchdata>=0.5.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (0.9.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (2.2.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2024.8.30)\r\n",
      "Requirement already satisfied: torch>=2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torchdata>=0.5.0->dgl) (2.1.0+cpu)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.10.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGL installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "os.environ['DGLBACKEND'] = \"pytorch\"\n",
    "\n",
    "# Install the CPU version.\n",
    "device = torch.device(\"cpu\")\n",
    "!pip install --pre dgl -f https://data.dgl.ai/wheels-test/repo.html\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    import dgl.graphbolt as gb\n",
    "    installed = True\n",
    "except ImportError as error:\n",
    "    installed = False\n",
    "    print(error)\n",
    "print(\"DGL installed!\" if installed else \"DGL not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2R7WnSbjsfbr"
   },
   "source": [
    "## Data preparation\n",
    "In order to demonstrate how to organize various data, let's create a base directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:19:46.205952Z",
     "iopub.status.busy": "2024-10-31T13:19:46.205260Z",
     "iopub.status.idle": "2024-10-31T13:19:46.210811Z",
     "shell.execute_reply": "2024-10-31T13:19:46.209895Z"
    },
    "id": "SZipbzyltLfO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created base directory: ./ondisk_dataset_heterograph\n"
     ]
    }
   ],
   "source": [
    "base_dir = './ondisk_dataset_heterograph'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "print(f\"Created base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhNtIn_xhlnl"
   },
   "source": [
    "### Generate graph structure data\n",
    "For heterogeneous graph, we need to save different edge edges(namely seeds) into separate **Numpy** or **CSV** files.\n",
    "\n",
    "Note:\n",
    "- when saving to **Numpy**, the array requires to be in shape of `(2, N)`. This format is recommended as constructing graph from it is much faster than **CSV** file.\n",
    "- when saving to **CSV** file, do not save index and header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:19:46.213119Z",
     "iopub.status.busy": "2024-10-31T13:19:46.212779Z",
     "iopub.status.idle": "2024-10-31T13:19:46.235626Z",
     "shell.execute_reply": "2024-10-31T13:19:46.234659Z"
    },
    "id": "HcBt4G5BmSjr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of [user:like:item] edges: [[461 584]\n",
      " [410 546]\n",
      " [972 165]\n",
      " [196 121]\n",
      " [753 138]]\n",
      "\n",
      "[user:like:item] edges are saved into ./ondisk_dataset_heterograph/like-edges.csv\n",
      "\n",
      "Part of [user:follow:user] edges: [[367  77]\n",
      " [169 971]\n",
      " [529 825]\n",
      " [646 762]\n",
      " [419 390]]\n",
      "\n",
      "[user:follow:user] edges are saved into ./ondisk_dataset_heterograph/follow-edges.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For simplicity, we create a heterogeneous graph with\n",
    "# 2 node types: `user`, `item`\n",
    "# 2 edge types: `user:like:item`, `user:follow:user`\n",
    "# And each node/edge type has the same number of nodes/edges.\n",
    "num_nodes = 1000\n",
    "num_edges = 10 * num_nodes\n",
    "\n",
    "# Edge type: \"user:like:item\"\n",
    "like_edges_path = os.path.join(base_dir, \"like-edges.csv\")\n",
    "like_edges = np.random.randint(0, num_nodes, size=(num_edges, 2))\n",
    "print(f\"Part of [user:like:item] edges: {like_edges[:5, :]}\\n\")\n",
    "\n",
    "df = pd.DataFrame(like_edges)\n",
    "df.to_csv(like_edges_path, index=False, header=False)\n",
    "print(f\"[user:like:item] edges are saved into {like_edges_path}\\n\")\n",
    "\n",
    "# Edge type: \"user:follow:user\"\n",
    "follow_edges_path = os.path.join(base_dir, \"follow-edges.csv\")\n",
    "follow_edges = np.random.randint(0, num_nodes, size=(num_edges, 2))\n",
    "print(f\"Part of [user:follow:user] edges: {follow_edges[:5, :]}\\n\")\n",
    "\n",
    "df = pd.DataFrame(follow_edges)\n",
    "df.to_csv(follow_edges_path, index=False, header=False)\n",
    "print(f\"[user:follow:user] edges are saved into {follow_edges_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh-4cPtzpcaH"
   },
   "source": [
    "### Generate feature data for graph\n",
    "For feature data, numpy arrays and torch tensors are supported for now. Let's generate feature data for each node/edge type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:19:46.238262Z",
     "iopub.status.busy": "2024-10-31T13:19:46.238061Z",
     "iopub.status.idle": "2024-10-31T13:19:46.267024Z",
     "shell.execute_reply": "2024-10-31T13:19:46.266095Z"
    },
    "id": "_PVu1u5brBhF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of node[user] feature [feat_0]: [[5.08304272e-01 4.36510551e-01 4.09028773e-01 9.14763095e-01\n",
      "  1.90239776e-01]\n",
      " [6.91697233e-01 9.95156718e-01 3.80313374e-01 3.01551681e-01\n",
      "  3.51825471e-01]\n",
      " [2.03688311e-04 1.32732765e-01 7.45540690e-01 1.80033957e-01\n",
      "  5.94881342e-01]]\n",
      "Node[user] feature [feat_0] is saved to ./ondisk_dataset_heterograph/node-user-feat-0.npy\n",
      "\n",
      "Part of node[user] feature [feat_1]: tensor([[0.1608, 0.2967, 0.8187, 0.2544, 0.7551],\n",
      "        [0.2143, 0.8636, 0.8861, 0.9285, 0.7388],\n",
      "        [0.8312, 0.0995, 0.6964, 0.1435, 0.9796]])\n",
      "Node[user] feature [feat_1] is saved to ./ondisk_dataset_heterograph/node-user-feat-1.pt\n",
      "\n",
      "Part of node[item] feature [feat_0]: [[0.5018886  0.24838805 0.88404237 0.44298118 0.45951516]\n",
      " [0.85974658 0.22871301 0.43946282 0.57990309 0.94954837]\n",
      " [0.4729829  0.41566155 0.59517138 0.16655103 0.23297136]]\n",
      "Node[item] feature [feat_0] is saved to ./ondisk_dataset_heterograph/node-item-feat-0.npy\n",
      "\n",
      "Part of node[item] feature [feat_1]: tensor([[0.8306, 0.0837, 0.4483, 0.0664, 0.7155],\n",
      "        [0.0271, 0.7066, 0.2251, 0.7602, 0.0130],\n",
      "        [0.1976, 0.5580, 0.3053, 0.9255, 0.5328]])\n",
      "Node[item] feature [feat_1] is saved to ./ondisk_dataset_heterograph/node-item-feat-1.pt\n",
      "\n",
      "Part of edge[user:like:item] feature [feat_0]: [[0.52403607 0.90964843 0.38749073 0.81808374 0.85332206]\n",
      " [0.34126185 0.48533213 0.03612859 0.19981978 0.55586171]\n",
      " [0.06401665 0.78392506 0.47058384 0.6873787  0.8577438 ]]\n",
      "Edge[user:like:item] feature [feat_0] is saved to ./ondisk_dataset_heterograph/edge-like-feat-0.npy\n",
      "\n",
      "Part of edge[user:like:item] feature [feat_1]: tensor([[0.2863, 0.3543, 0.7422, 0.8763, 0.5493],\n",
      "        [0.3886, 0.4070, 0.3869, 0.1812, 0.0482],\n",
      "        [0.2553, 0.6988, 0.3900, 0.3999, 0.4420]])\n",
      "Edge[user:like:item] feature [feat_1] is saved to ./ondisk_dataset_heterograph/edge-like-feat-1.pt\n",
      "\n",
      "Part of edge[user:follow:user] feature [feat_0]: [[0.98875324 0.14723806 0.69739598 0.98015133 0.80062802]\n",
      " [0.17788264 0.294399   0.36112873 0.02766777 0.28485939]\n",
      " [0.94986667 0.96560754 0.48114921 0.53619103 0.34514   ]]\n",
      "Edge[user:follow:user] feature [feat_0] is saved to ./ondisk_dataset_heterograph/edge-follow-feat-0.npy\n",
      "\n",
      "Part of edge[user:follow:user] feature [feat_1]: tensor([[0.4457, 0.8728, 0.1664, 0.4060, 0.7022],\n",
      "        [0.9301, 0.8870, 0.4664, 0.1720, 0.7307],\n",
      "        [0.2583, 0.8503, 0.9456, 0.8187, 0.6244]])\n",
      "Edge[user:follow:user] feature [feat_1] is saved to ./ondisk_dataset_heterograph/edge-follow-feat-1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate node[user] feature in numpy array.\n",
    "node_user_feat_0_path = os.path.join(base_dir, \"node-user-feat-0.npy\")\n",
    "node_user_feat_0 = np.random.rand(num_nodes, 5)\n",
    "print(f\"Part of node[user] feature [feat_0]: {node_user_feat_0[:3, :]}\")\n",
    "np.save(node_user_feat_0_path, node_user_feat_0)\n",
    "print(f\"Node[user] feature [feat_0] is saved to {node_user_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another node[user] feature in torch tensor\n",
    "node_user_feat_1_path = os.path.join(base_dir, \"node-user-feat-1.pt\")\n",
    "node_user_feat_1 = torch.rand(num_nodes, 5)\n",
    "print(f\"Part of node[user] feature [feat_1]: {node_user_feat_1[:3, :]}\")\n",
    "torch.save(node_user_feat_1, node_user_feat_1_path)\n",
    "print(f\"Node[user] feature [feat_1] is saved to {node_user_feat_1_path}\\n\")\n",
    "\n",
    "# Generate node[item] feature in numpy array.\n",
    "node_item_feat_0_path = os.path.join(base_dir, \"node-item-feat-0.npy\")\n",
    "node_item_feat_0 = np.random.rand(num_nodes, 5)\n",
    "print(f\"Part of node[item] feature [feat_0]: {node_item_feat_0[:3, :]}\")\n",
    "np.save(node_item_feat_0_path, node_item_feat_0)\n",
    "print(f\"Node[item] feature [feat_0] is saved to {node_item_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another node[item] feature in torch tensor\n",
    "node_item_feat_1_path = os.path.join(base_dir, \"node-item-feat-1.pt\")\n",
    "node_item_feat_1 = torch.rand(num_nodes, 5)\n",
    "print(f\"Part of node[item] feature [feat_1]: {node_item_feat_1[:3, :]}\")\n",
    "torch.save(node_item_feat_1, node_item_feat_1_path)\n",
    "print(f\"Node[item] feature [feat_1] is saved to {node_item_feat_1_path}\\n\")\n",
    "\n",
    "# Generate edge[user:like:item] feature in numpy array.\n",
    "edge_like_feat_0_path = os.path.join(base_dir, \"edge-like-feat-0.npy\")\n",
    "edge_like_feat_0 = np.random.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:like:item] feature [feat_0]: {edge_like_feat_0[:3, :]}\")\n",
    "np.save(edge_like_feat_0_path, edge_like_feat_0)\n",
    "print(f\"Edge[user:like:item] feature [feat_0] is saved to {edge_like_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another edge[user:like:item] feature in torch tensor\n",
    "edge_like_feat_1_path = os.path.join(base_dir, \"edge-like-feat-1.pt\")\n",
    "edge_like_feat_1 = torch.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:like:item] feature [feat_1]: {edge_like_feat_1[:3, :]}\")\n",
    "torch.save(edge_like_feat_1, edge_like_feat_1_path)\n",
    "print(f\"Edge[user:like:item] feature [feat_1] is saved to {edge_like_feat_1_path}\\n\")\n",
    "\n",
    "# Generate edge[user:follow:user] feature in numpy array.\n",
    "edge_follow_feat_0_path = os.path.join(base_dir, \"edge-follow-feat-0.npy\")\n",
    "edge_follow_feat_0 = np.random.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:follow:user] feature [feat_0]: {edge_follow_feat_0[:3, :]}\")\n",
    "np.save(edge_follow_feat_0_path, edge_follow_feat_0)\n",
    "print(f\"Edge[user:follow:user] feature [feat_0] is saved to {edge_follow_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another edge[user:follow:user] feature in torch tensor\n",
    "edge_follow_feat_1_path = os.path.join(base_dir, \"edge-follow-feat-1.pt\")\n",
    "edge_follow_feat_1 = torch.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:follow:user] feature [feat_1]: {edge_follow_feat_1[:3, :]}\")\n",
    "torch.save(edge_follow_feat_1, edge_follow_feat_1_path)\n",
    "print(f\"Edge[user:follow:user] feature [feat_1] is saved to {edge_follow_feat_1_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyqgOtsIwzh_"
   },
   "source": [
    "### Generate tasks\n",
    "`OnDiskDataset` supports multiple tasks. For each task, we need to prepare training/validation/test sets respectively. Such sets usually vary among different tasks. In this tutorial, let's create a **Node Classification** task and **Link Prediction** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVxHaDIfzCkr"
   },
   "source": [
    "#### Node Classification Task\n",
    "For node classification task, we need **node IDs** and corresponding **labels** for each training/validation/test set. Like feature data, numpy arrays and torch tensors are supported for these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:19:46.269441Z",
     "iopub.status.busy": "2024-10-31T13:19:46.269072Z",
     "iopub.status.idle": "2024-10-31T13:19:46.288179Z",
     "shell.execute_reply": "2024-10-31T13:19:46.287270Z"
    },
    "id": "S5-fyBbHzTCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of train ids[user] for node classification: [449 329 333]\n",
      "NC train ids[user] are saved to ./ondisk_dataset_heterograph/nc-train-user-ids.npy\n",
      "\n",
      "Part of train labels[user] for node classification: tensor([2, 9, 7])\n",
      "NC train labels[user] are saved to ./ondisk_dataset_heterograph/nc-train-user-labels.pt\n",
      "\n",
      "Part of train ids[item] for node classification: [421 331  96]\n",
      "NC train ids[item] are saved to ./ondisk_dataset_heterograph/nc-train-item-ids.npy\n",
      "\n",
      "Part of train labels[item] for node classification: tensor([9, 6, 2])\n",
      "NC train labels[item] are saved to ./ondisk_dataset_heterograph/nc-train-item-labels.pt\n",
      "\n",
      "Part of val ids[user] for node classification: [710 921 649]\n",
      "NC val ids[user] are saved to ./ondisk_dataset_heterograph/nc-val-user-ids.npy\n",
      "\n",
      "Part of val labels[user] for node classification: tensor([4, 8, 0])\n",
      "NC val labels[user] are saved to ./ondisk_dataset_heterograph/nc-val-user-labels.pt\n",
      "\n",
      "Part of val ids[item] for node classification: [ 88 274 294]\n",
      "NC val ids[item] are saved to ./ondisk_dataset_heterograph/nc-val-item-ids.npy\n",
      "\n",
      "Part of val labels[item] for node classification: tensor([4, 6, 4])\n",
      "NC val labels[item] are saved to ./ondisk_dataset_heterograph/nc-val-item-labels.pt\n",
      "\n",
      "Part of test ids[user] for node classification: [240 628  39]\n",
      "NC test ids[user] are saved to ./ondisk_dataset_heterograph/nc-test-user-ids.npy\n",
      "\n",
      "Part of test labels[user] for node classification: tensor([2, 3, 7])\n",
      "NC test labels[user] are saved to ./ondisk_dataset_heterograph/nc-test-user-labels.pt\n",
      "\n",
      "Part of test ids[item] for node classification: [225 688 357]\n",
      "NC test ids[item] are saved to ./ondisk_dataset_heterograph/nc-test-item-ids.npy\n",
      "\n",
      "Part of test labels[item] for node classification: tensor([7, 7, 8])\n",
      "NC test labels[item] are saved to ./ondisk_dataset_heterograph/nc-test-item-labels.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration, let's generate item sets for each node type.\n",
    "num_trains = int(num_nodes * 0.6)\n",
    "num_vals = int(num_nodes * 0.2)\n",
    "num_tests = num_nodes - num_trains - num_vals\n",
    "\n",
    "user_ids = np.arange(num_nodes)\n",
    "np.random.shuffle(user_ids)\n",
    "\n",
    "item_ids = np.arange(num_nodes)\n",
    "np.random.shuffle(item_ids)\n",
    "\n",
    "# Train IDs for user.\n",
    "nc_train_user_ids_path = os.path.join(base_dir, \"nc-train-user-ids.npy\")\n",
    "nc_train_user_ids = user_ids[:num_trains]\n",
    "print(f\"Part of train ids[user] for node classification: {nc_train_user_ids[:3]}\")\n",
    "np.save(nc_train_user_ids_path, nc_train_user_ids)\n",
    "print(f\"NC train ids[user] are saved to {nc_train_user_ids_path}\\n\")\n",
    "\n",
    "# Train labels for user.\n",
    "nc_train_user_labels_path = os.path.join(base_dir, \"nc-train-user-labels.pt\")\n",
    "nc_train_user_labels = torch.randint(0, 10, (num_trains,))\n",
    "print(f\"Part of train labels[user] for node classification: {nc_train_user_labels[:3]}\")\n",
    "torch.save(nc_train_user_labels, nc_train_user_labels_path)\n",
    "print(f\"NC train labels[user] are saved to {nc_train_user_labels_path}\\n\")\n",
    "\n",
    "# Train IDs for item.\n",
    "nc_train_item_ids_path = os.path.join(base_dir, \"nc-train-item-ids.npy\")\n",
    "nc_train_item_ids = item_ids[:num_trains]\n",
    "print(f\"Part of train ids[item] for node classification: {nc_train_item_ids[:3]}\")\n",
    "np.save(nc_train_item_ids_path, nc_train_item_ids)\n",
    "print(f\"NC train ids[item] are saved to {nc_train_item_ids_path}\\n\")\n",
    "\n",
    "# Train labels for item.\n",
    "nc_train_item_labels_path = os.path.join(base_dir, \"nc-train-item-labels.pt\")\n",
    "nc_train_item_labels = torch.randint(0, 10, (num_trains,))\n",
    "print(f\"Part of train labels[item] for node classification: {nc_train_item_labels[:3]}\")\n",
    "torch.save(nc_train_item_labels, nc_train_item_labels_path)\n",
    "print(f\"NC train labels[item] are saved to {nc_train_item_labels_path}\\n\")\n",
    "\n",
    "# Val IDs for user.\n",
    "nc_val_user_ids_path = os.path.join(base_dir, \"nc-val-user-ids.npy\")\n",
    "nc_val_user_ids = user_ids[num_trains:num_trains+num_vals]\n",
    "print(f\"Part of val ids[user] for node classification: {nc_val_user_ids[:3]}\")\n",
    "np.save(nc_val_user_ids_path, nc_val_user_ids)\n",
    "print(f\"NC val ids[user] are saved to {nc_val_user_ids_path}\\n\")\n",
    "\n",
    "# Val labels for user.\n",
    "nc_val_user_labels_path = os.path.join(base_dir, \"nc-val-user-labels.pt\")\n",
    "nc_val_user_labels = torch.randint(0, 10, (num_vals,))\n",
    "print(f\"Part of val labels[user] for node classification: {nc_val_user_labels[:3]}\")\n",
    "torch.save(nc_val_user_labels, nc_val_user_labels_path)\n",
    "print(f\"NC val labels[user] are saved to {nc_val_user_labels_path}\\n\")\n",
    "\n",
    "# Val IDs for item.\n",
    "nc_val_item_ids_path = os.path.join(base_dir, \"nc-val-item-ids.npy\")\n",
    "nc_val_item_ids = item_ids[num_trains:num_trains+num_vals]\n",
    "print(f\"Part of val ids[item] for node classification: {nc_val_item_ids[:3]}\")\n",
    "np.save(nc_val_item_ids_path, nc_val_item_ids)\n",
    "print(f\"NC val ids[item] are saved to {nc_val_item_ids_path}\\n\")\n",
    "\n",
    "# Val labels for item.\n",
    "nc_val_item_labels_path = os.path.join(base_dir, \"nc-val-item-labels.pt\")\n",
    "nc_val_item_labels = torch.randint(0, 10, (num_vals,))\n",
    "print(f\"Part of val labels[item] for node classification: {nc_val_item_labels[:3]}\")\n",
    "torch.save(nc_val_item_labels, nc_val_item_labels_path)\n",
    "print(f\"NC val labels[item] are saved to {nc_val_item_labels_path}\\n\")\n",
    "\n",
    "# Test IDs for user.\n",
    "nc_test_user_ids_path = os.path.join(base_dir, \"nc-test-user-ids.npy\")\n",
    "nc_test_user_ids = user_ids[-num_tests:]\n",
    "print(f\"Part of test ids[user] for node classification: {nc_test_user_ids[:3]}\")\n",
    "np.save(nc_test_user_ids_path, nc_test_user_ids)\n",
    "print(f\"NC test ids[user] are saved to {nc_test_user_ids_path}\\n\")\n",
    "\n",
    "# Test labels for user.\n",
    "nc_test_user_labels_path = os.path.join(base_dir, \"nc-test-user-labels.pt\")\n",
    "nc_test_user_labels = torch.randint(0, 10, (num_tests,))\n",
    "print(f\"Part of test labels[user] for node classification: {nc_test_user_labels[:3]}\")\n",
    "torch.save(nc_test_user_labels, nc_test_user_labels_path)\n",
    "print(f\"NC test labels[user] are saved to {nc_test_user_labels_path}\\n\")\n",
    "\n",
    "# Test IDs for item.\n",
    "nc_test_item_ids_path = os.path.join(base_dir, \"nc-test-item-ids.npy\")\n",
    "nc_test_item_ids = item_ids[-num_tests:]\n",
    "print(f\"Part of test ids[item] for node classification: {nc_test_item_ids[:3]}\")\n",
    "np.save(nc_test_item_ids_path, nc_test_item_ids)\n",
    "print(f\"NC test ids[item] are saved to {nc_test_item_ids_path}\\n\")\n",
    "\n",
    "# Test labels for item.\n",
    "nc_test_item_labels_path = os.path.join(base_dir, \"nc-test-item-labels.pt\")\n",
    "nc_test_item_labels = torch.randint(0, 10, (num_tests,))\n",
    "print(f\"Part of test labels[item] for node classification: {nc_test_item_labels[:3]}\")\n",
    "torch.save(nc_test_item_labels, nc_test_item_labels_path)\n",
    "print(f\"NC test labels[item] are saved to {nc_test_item_labels_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhAcDCHQ_KJ0"
   },
   "source": [
    "#### Link Prediction Task\n",
    "For link prediction task, we need **seeds** or **corresponding labels and indexes** which representing the pos/neg property and group of the seeds for each training/validation/test set. Like feature data, numpy arrays and torch tensors are supported for these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:19:46.290540Z",
     "iopub.status.busy": "2024-10-31T13:19:46.290166Z",
     "iopub.status.idle": "2024-10-31T13:19:46.322695Z",
     "shell.execute_reply": "2024-10-31T13:19:46.321747Z"
    },
    "id": "u0jCnXIcAQy4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of train seeds[user:like:item] for link prediction: [[461 584]\n",
      " [410 546]\n",
      " [972 165]]\n",
      "LP train seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-train-like-seeds.npy\n",
      "\n",
      "Part of train seeds[user:follow:user] for link prediction: [[367  77]\n",
      " [169 971]\n",
      " [529 825]]\n",
      "LP train seeds[user:follow:user] are saved to ./ondisk_dataset_heterograph/lp-train-follow-seeds.npy\n",
      "\n",
      "Part of val seeds[user:like:item] for link prediction: [[ 78 352]\n",
      " [514 127]\n",
      " [164 780]]\n",
      "LP val seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-seeds.npy\n",
      "\n",
      "Part of val labels[user:like:item] for link prediction: [1. 1. 1.]\n",
      "LP val labels[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-labels.npy\n",
      "\n",
      "Part of val indexes[user:like:item] for link prediction: [0 1 2]\n",
      "LP val indexes[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-indexes.npy\n",
      "\n",
      "Part of val seeds[user:follow:item] for link prediction: [[901 957]\n",
      " [227  52]\n",
      " [769 857]]\n",
      "LP val seeds[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-seeds.npy\n",
      "\n",
      "Part of val labels[user:follow:item] for link prediction: [1. 1. 1.]\n",
      "LP val labels[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-labels.npy\n",
      "\n",
      "Part of val indexes[user:follow:item] for link prediction: [0 1 2]\n",
      "LP val indexes[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-indexes.npy\n",
      "\n",
      "Part of test seeds[user:like:item] for link prediction: [[833 430]\n",
      " [924 894]\n",
      " [998 574]]\n",
      "LP test seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-seeds.npy\n",
      "\n",
      "Part of test labels[user:like:item] for link prediction: [1. 1. 1.]\n",
      "LP test labels[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-labels.npy\n",
      "\n",
      "Part of test indexes[user:like:item] for link prediction: [0 1 2]\n",
      "LP test indexes[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-indexes.npy\n",
      "\n",
      "Part of test seeds[user:follow:item] for link prediction: [[353 847]\n",
      " [552 260]\n",
      " [ 40 759]]\n",
      "LP test seeds[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-seeds.npy\n",
      "\n",
      "Part of test labels[user:follow:item] for link prediction: [1. 1. 1.]\n",
      "LP test labels[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-labels.npy\n",
      "\n",
      "Part of test indexes[user:follow:item] for link prediction: [0 1 2]\n",
      "LP test indexes[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-indexes.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration, let's generate item sets for each edge type.\n",
    "num_trains = int(num_edges * 0.6)\n",
    "num_vals = int(num_edges * 0.2)\n",
    "num_tests = num_edges - num_trains - num_vals\n",
    "\n",
    "# Train seeds for user:like:item.\n",
    "lp_train_like_seeds_path = os.path.join(base_dir, \"lp-train-like-seeds.npy\")\n",
    "lp_train_like_seeds = like_edges[:num_trains, :]\n",
    "print(f\"Part of train seeds[user:like:item] for link prediction: {lp_train_like_seeds[:3]}\")\n",
    "np.save(lp_train_like_seeds_path, lp_train_like_seeds)\n",
    "print(f\"LP train seeds[user:like:item] are saved to {lp_train_like_seeds_path}\\n\")\n",
    "\n",
    "# Train seeds for user:follow:user.\n",
    "lp_train_follow_seeds_path = os.path.join(base_dir, \"lp-train-follow-seeds.npy\")\n",
    "lp_train_follow_seeds = follow_edges[:num_trains, :]\n",
    "print(f\"Part of train seeds[user:follow:user] for link prediction: {lp_train_follow_seeds[:3]}\")\n",
    "np.save(lp_train_follow_seeds_path, lp_train_follow_seeds)\n",
    "print(f\"LP train seeds[user:follow:user] are saved to {lp_train_follow_seeds_path}\\n\")\n",
    "\n",
    "# Val seeds for user:like:item.\n",
    "lp_val_like_seeds_path = os.path.join(base_dir, \"lp-val-like-seeds.npy\")\n",
    "lp_val_like_seeds = like_edges[num_trains:num_trains+num_vals, :]\n",
    "lp_val_like_neg_dsts = np.random.randint(0, num_nodes, (num_vals, 10)).reshape(-1)\n",
    "lp_val_like_neg_srcs = np.repeat(lp_val_like_seeds[:,0], 10)\n",
    "lp_val_like_neg_seeds = np.concatenate((lp_val_like_neg_srcs, lp_val_like_neg_dsts)).reshape(2,-1).T\n",
    "lp_val_like_seeds = np.concatenate((lp_val_like_seeds, lp_val_like_neg_seeds))\n",
    "print(f\"Part of val seeds[user:like:item] for link prediction: {lp_val_like_seeds[:3]}\")\n",
    "np.save(lp_val_like_seeds_path, lp_val_like_seeds)\n",
    "print(f\"LP val seeds[user:like:item] are saved to {lp_val_like_seeds_path}\\n\")\n",
    "\n",
    "# Val labels for user:like:item.\n",
    "lp_val_like_labels_path = os.path.join(base_dir, \"lp-val-like-labels.npy\")\n",
    "lp_val_like_labels = np.empty(num_vals * (10 + 1))\n",
    "lp_val_like_labels[:num_vals] = 1\n",
    "lp_val_like_labels[num_vals:] = 0\n",
    "print(f\"Part of val labels[user:like:item] for link prediction: {lp_val_like_labels[:3]}\")\n",
    "np.save(lp_val_like_labels_path, lp_val_like_labels)\n",
    "print(f\"LP val labels[user:like:item] are saved to {lp_val_like_labels_path}\\n\")\n",
    "\n",
    "# Val indexes for user:like:item.\n",
    "lp_val_like_indexes_path = os.path.join(base_dir, \"lp-val-like-indexes.npy\")\n",
    "lp_val_like_indexes = np.arange(0, num_vals)\n",
    "lp_val_like_neg_indexes = np.repeat(lp_val_like_indexes, 10)\n",
    "lp_val_like_indexes = np.concatenate([lp_val_like_indexes, lp_val_like_neg_indexes])\n",
    "print(f\"Part of val indexes[user:like:item] for link prediction: {lp_val_like_indexes[:3]}\")\n",
    "np.save(lp_val_like_indexes_path, lp_val_like_indexes)\n",
    "print(f\"LP val indexes[user:like:item] are saved to {lp_val_like_indexes_path}\\n\")\n",
    "\n",
    "# Val seeds for user:follow:item.\n",
    "lp_val_follow_seeds_path = os.path.join(base_dir, \"lp-val-follow-seeds.npy\")\n",
    "lp_val_follow_seeds = follow_edges[num_trains:num_trains+num_vals, :]\n",
    "lp_val_follow_neg_dsts = np.random.randint(0, num_nodes, (num_vals, 10)).reshape(-1)\n",
    "lp_val_follow_neg_srcs = np.repeat(lp_val_follow_seeds[:,0], 10)\n",
    "lp_val_follow_neg_seeds = np.concatenate((lp_val_follow_neg_srcs, lp_val_follow_neg_dsts)).reshape(2,-1).T\n",
    "lp_val_follow_seeds = np.concatenate((lp_val_follow_seeds, lp_val_follow_neg_seeds))\n",
    "print(f\"Part of val seeds[user:follow:item] for link prediction: {lp_val_follow_seeds[:3]}\")\n",
    "np.save(lp_val_follow_seeds_path, lp_val_follow_seeds)\n",
    "print(f\"LP val seeds[user:follow:item] are saved to {lp_val_follow_seeds_path}\\n\")\n",
    "\n",
    "# Val labels for user:follow:item.\n",
    "lp_val_follow_labels_path = os.path.join(base_dir, \"lp-val-follow-labels.npy\")\n",
    "lp_val_follow_labels = np.empty(num_vals * (10 + 1))\n",
    "lp_val_follow_labels[:num_vals] = 1\n",
    "lp_val_follow_labels[num_vals:] = 0\n",
    "print(f\"Part of val labels[user:follow:item] for link prediction: {lp_val_follow_labels[:3]}\")\n",
    "np.save(lp_val_follow_labels_path, lp_val_follow_labels)\n",
    "print(f\"LP val labels[user:follow:item] are saved to {lp_val_follow_labels_path}\\n\")\n",
    "\n",
    "# Val indexes for user:follow:item.\n",
    "lp_val_follow_indexes_path = os.path.join(base_dir, \"lp-val-follow-indexes.npy\")\n",
    "lp_val_follow_indexes = np.arange(0, num_vals)\n",
    "lp_val_follow_neg_indexes = np.repeat(lp_val_follow_indexes, 10)\n",
    "lp_val_follow_indexes = np.concatenate([lp_val_follow_indexes, lp_val_follow_neg_indexes])\n",
    "print(f\"Part of val indexes[user:follow:item] for link prediction: {lp_val_follow_indexes[:3]}\")\n",
    "np.save(lp_val_follow_indexes_path, lp_val_follow_indexes)\n",
    "print(f\"LP val indexes[user:follow:item] are saved to {lp_val_follow_indexes_path}\\n\")\n",
    "\n",
    "# Test seeds for user:like:item.\n",
    "lp_test_like_seeds_path = os.path.join(base_dir, \"lp-test-like-seeds.npy\")\n",
    "lp_test_like_seeds = like_edges[-num_tests:, :]\n",
    "lp_test_like_neg_dsts = np.random.randint(0, num_nodes, (num_tests, 10)).reshape(-1)\n",
    "lp_test_like_neg_srcs = np.repeat(lp_test_like_seeds[:,0], 10)\n",
    "lp_test_like_neg_seeds = np.concatenate((lp_test_like_neg_srcs, lp_test_like_neg_dsts)).reshape(2,-1).T\n",
    "lp_test_like_seeds = np.concatenate((lp_test_like_seeds, lp_test_like_neg_seeds))\n",
    "print(f\"Part of test seeds[user:like:item] for link prediction: {lp_test_like_seeds[:3]}\")\n",
    "np.save(lp_test_like_seeds_path, lp_test_like_seeds)\n",
    "print(f\"LP test seeds[user:like:item] are saved to {lp_test_like_seeds_path}\\n\")\n",
    "\n",
    "# Test labels for user:like:item.\n",
    "lp_test_like_labels_path = os.path.join(base_dir, \"lp-test-like-labels.npy\")\n",
    "lp_test_like_labels = np.empty(num_tests * (10 + 1))\n",
    "lp_test_like_labels[:num_tests] = 1\n",
    "lp_test_like_labels[num_tests:] = 0\n",
    "print(f\"Part of test labels[user:like:item] for link prediction: {lp_test_like_labels[:3]}\")\n",
    "np.save(lp_test_like_labels_path, lp_test_like_labels)\n",
    "print(f\"LP test labels[user:like:item] are saved to {lp_test_like_labels_path}\\n\")\n",
    "\n",
    "# Test indexes for user:like:item.\n",
    "lp_test_like_indexes_path = os.path.join(base_dir, \"lp-test-like-indexes.npy\")\n",
    "lp_test_like_indexes = np.arange(0, num_tests)\n",
    "lp_test_like_neg_indexes = np.repeat(lp_test_like_indexes, 10)\n",
    "lp_test_like_indexes = np.concatenate([lp_test_like_indexes, lp_test_like_neg_indexes])\n",
    "print(f\"Part of test indexes[user:like:item] for link prediction: {lp_test_like_indexes[:3]}\")\n",
    "np.save(lp_test_like_indexes_path, lp_test_like_indexes)\n",
    "print(f\"LP test indexes[user:like:item] are saved to {lp_test_like_indexes_path}\\n\")\n",
    "\n",
    "# Test seeds for user:follow:item.\n",
    "lp_test_follow_seeds_path = os.path.join(base_dir, \"lp-test-follow-seeds.npy\")\n",
    "lp_test_follow_seeds = follow_edges[-num_tests:, :]\n",
    "lp_test_follow_neg_dsts = np.random.randint(0, num_nodes, (num_tests, 10)).reshape(-1)\n",
    "lp_test_follow_neg_srcs = np.repeat(lp_test_follow_seeds[:,0], 10)\n",
    "lp_test_follow_neg_seeds = np.concatenate((lp_test_follow_neg_srcs, lp_test_follow_neg_dsts)).reshape(2,-1).T\n",
    "lp_test_follow_seeds = np.concatenate((lp_test_follow_seeds, lp_test_follow_neg_seeds))\n",
    "print(f\"Part of test seeds[user:follow:item] for link prediction: {lp_test_follow_seeds[:3]}\")\n",
    "np.save(lp_test_follow_seeds_path, lp_test_follow_seeds)\n",
    "print(f\"LP test seeds[user:follow:item] are saved to {lp_test_follow_seeds_path}\\n\")\n",
    "\n",
    "# Test labels for user:follow:item.\n",
    "lp_test_follow_labels_path = os.path.join(base_dir, \"lp-test-follow-labels.npy\")\n",
    "lp_test_follow_labels = np.empty(num_tests * (10 + 1))\n",
    "lp_test_follow_labels[:num_tests] = 1\n",
    "lp_test_follow_labels[num_tests:] = 0\n",
    "print(f\"Part of test labels[user:follow:item] for link prediction: {lp_test_follow_labels[:3]}\")\n",
    "np.save(lp_test_follow_labels_path, lp_test_follow_labels)\n",
    "print(f\"LP test labels[user:follow:item] are saved to {lp_test_follow_labels_path}\\n\")\n",
    "\n",
    "# Test indexes for user:follow:item.\n",
    "lp_test_follow_indexes_path = os.path.join(base_dir, \"lp-test-follow-indexes.npy\")\n",
    "lp_test_follow_indexes = np.arange(0, num_tests)\n",
    "lp_test_follow_neg_indexes = np.repeat(lp_test_follow_indexes, 10)\n",
    "lp_test_follow_indexes = np.concatenate([lp_test_follow_indexes, lp_test_follow_neg_indexes])\n",
    "print(f\"Part of test indexes[user:follow:item] for link prediction: {lp_test_follow_indexes[:3]}\")\n",
    "np.save(lp_test_follow_indexes_path, lp_test_follow_indexes)\n",
    "print(f\"LP test indexes[user:follow:item] are saved to {lp_test_follow_indexes_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbk6-wxRK-6S"
   },
   "source": [
    "## Organize Data into YAML File\n",
    "Now we need to create a `metadata.yaml` file which contains the paths, dadta types of graph structure, feature data, training/validation/test sets. Please note that all path should be relative to `metadata.yaml`.\n",
    "\n",
    "For heterogeneous graph, we need to specify the node/edge type in **type** fields. For edge type, canonical etype is required which is a string that's concatenated by source node type, etype, and destination node type together with `:`.\n",
    "\n",
    "Notes:\n",
    "- all path should be relative to `metadata.yaml`.\n",
    "- Below fields are optional and not specified in below example.\n",
    "  - `in_memory`: indicates whether to load dada into memory or `mmap`. Default is `True`.\n",
    "\n",
    "Please refer to [YAML specification](https://github.com/dmlc/dgl/blob/master/docs/source/stochastic_training/ondisk-dataset-specification.rst) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:19:46.325513Z",
     "iopub.status.busy": "2024-10-31T13:19:46.325123Z",
     "iopub.status.idle": "2024-10-31T13:19:46.333561Z",
     "shell.execute_reply": "2024-10-31T13:19:46.332930Z"
    },
    "id": "ddGTWW61Lpwp"
   },
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "    dataset_name: heterogeneous_graph_nc_lp\n",
    "    graph:\n",
    "      nodes:\n",
    "        - type: user\n",
    "          num: {num_nodes}\n",
    "        - type: item\n",
    "          num: {num_nodes}\n",
    "      edges:\n",
    "        - type: \"user:like:item\"\n",
    "          format: csv\n",
    "          path: {os.path.basename(like_edges_path)}\n",
    "        - type: \"user:follow:user\"\n",
    "          format: csv\n",
    "          path: {os.path.basename(follow_edges_path)}\n",
    "    feature_data:\n",
    "      - domain: node\n",
    "        type: user\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(node_user_feat_0_path)}\n",
    "      - domain: node\n",
    "        type: user\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(node_user_feat_1_path)}\n",
    "      - domain: node\n",
    "        type: item\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(node_item_feat_0_path)}\n",
    "      - domain: node\n",
    "        type: item\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(node_item_feat_1_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:like:item\"\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(edge_like_feat_0_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:like:item\"\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(edge_like_feat_1_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:follow:user\"\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(edge_follow_feat_0_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:follow:user\"\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(edge_follow_feat_1_path)}\n",
    "    tasks:\n",
    "      - name: node_classification\n",
    "        num_classes: 10\n",
    "        train_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_train_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_train_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_train_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_train_item_labels_path)}\n",
    "        validation_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_val_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_val_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_val_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_val_item_labels_path)}\n",
    "        test_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_test_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_test_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_test_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_test_item_labels_path)}\n",
    "      - name: link_prediction\n",
    "        num_classes: 10\n",
    "        train_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_train_like_seeds_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_train_follow_seeds_path)}\n",
    "        validation_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_indexes_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_indexes_path)}\n",
    "        test_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_indexes_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_indexes_path)}\n",
    "\"\"\"\n",
    "metadata_path = os.path.join(base_dir, \"metadata.yaml\")\n",
    "with open(metadata_path, \"w\") as f:\n",
    "  f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEfybHGhOW7O"
   },
   "source": [
    "## Instantiate `OnDiskDataset`\n",
    "Now we're ready to load dataset via `dgl.graphbolt.OnDiskDataset`. When instantiating, we just pass in the base directory where `metadata.yaml` file lies.\n",
    "\n",
    "During first instantiation, GraphBolt preprocesses the raw data such as constructing `FusedCSCSamplingGraph` from edges. All data including graph, feature data, training/validation/test sets are put into `preprocessed` directory after preprocessing. Any following dataset loading will skip the preprocess stage.\n",
    "\n",
    "After preprocessing, `load()` is required to be called explicitly in order to load graph, feature data and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:19:46.335386Z",
     "iopub.status.busy": "2024-10-31T13:19:46.335079Z",
     "iopub.status.idle": "2024-10-31T13:19:46.451287Z",
     "shell.execute_reply": "2024-10-31T13:19:46.450576Z"
    },
    "id": "W58CZoSzOiyo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to preprocess the on-disk dataset.\n",
      "Finish preprocessing the on-disk dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph: FusedCSCSamplingGraph(csc_indptr=tensor([    0,     8,    20,  ..., 19970, 19982, 20000], dtype=torch.int32),\n",
      "                      indices=tensor([1019, 1454, 1452,  ..., 1901, 1685, 1416], dtype=torch.int32),\n",
      "                      total_num_nodes=2000, num_edges={'user:follow:user': 10000, 'user:like:item': 10000},\n",
      "                      node_type_offset=tensor([   0, 1000, 2000], dtype=torch.int32),\n",
      "                      type_per_edge=tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.uint8),\n",
      "                      node_type_to_id={'item': 0, 'user': 1},\n",
      "                      edge_type_to_id={'user:follow:user': 0, 'user:like:item': 1},)\n",
      "\n",
      "Loaded feature store: TorchBasedFeatureStore(\n",
      "    {(<OnDiskFeatureDataDomain.NODE: 'node'>, 'user', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[5.0830e-01, 4.3651e-01, 4.0903e-01, 9.1476e-01, 1.9024e-01],\n",
      "                        [6.9170e-01, 9.9516e-01, 3.8031e-01, 3.0155e-01, 3.5183e-01],\n",
      "                        [2.0369e-04, 1.3273e-01, 7.4554e-01, 1.8003e-01, 5.9488e-01],\n",
      "                        ...,\n",
      "                        [1.2353e-01, 1.5877e-01, 5.4201e-01, 7.2073e-01, 3.7914e-02],\n",
      "                        [3.5377e-02, 8.0997e-02, 5.0600e-01, 2.4969e-01, 1.9350e-01],\n",
      "                        [9.3836e-01, 6.8624e-01, 2.1333e-01, 4.0253e-01, 4.5792e-01]],\n",
      "                       dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'user', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.1608, 0.2967, 0.8187, 0.2544, 0.7551],\n",
      "                        [0.2143, 0.8636, 0.8861, 0.9285, 0.7388],\n",
      "                        [0.8312, 0.0995, 0.6964, 0.1435, 0.9796],\n",
      "                        ...,\n",
      "                        [0.5359, 0.1015, 0.0423, 0.3973, 0.8760],\n",
      "                        [0.7041, 0.5500, 0.1823, 0.8107, 0.7770],\n",
      "                        [0.0471, 0.0881, 0.5771, 0.8511, 0.8447]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'item', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.5019, 0.2484, 0.8840, 0.4430, 0.4595],\n",
      "                        [0.8597, 0.2287, 0.4395, 0.5799, 0.9495],\n",
      "                        [0.4730, 0.4157, 0.5952, 0.1666, 0.2330],\n",
      "                        ...,\n",
      "                        [0.1702, 0.1260, 0.7676, 0.8503, 0.8167],\n",
      "                        [0.7887, 0.0707, 0.2899, 0.1221, 0.9802],\n",
      "                        [0.7791, 0.9187, 0.8226, 0.6609, 0.7466]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'item', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.8306, 0.0837, 0.4483, 0.0664, 0.7155],\n",
      "                        [0.0271, 0.7066, 0.2251, 0.7602, 0.0130],\n",
      "                        [0.1976, 0.5580, 0.3053, 0.9255, 0.5328],\n",
      "                        ...,\n",
      "                        [0.8725, 0.3064, 0.2949, 0.9256, 0.3683],\n",
      "                        [0.6844, 0.9085, 0.3716, 0.7529, 0.9151],\n",
      "                        [0.6231, 0.1034, 0.0340, 0.6548, 0.8307]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:like:item', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.5240, 0.9096, 0.3875, 0.8181, 0.8533],\n",
      "                        [0.3413, 0.4853, 0.0361, 0.1998, 0.5559],\n",
      "                        [0.0640, 0.7839, 0.4706, 0.6874, 0.8577],\n",
      "                        ...,\n",
      "                        [0.1527, 0.1797, 0.8617, 0.9481, 0.3129],\n",
      "                        [0.5343, 0.1747, 0.9117, 0.5120, 0.6277],\n",
      "                        [0.6562, 0.9193, 0.5756, 0.4703, 0.7879]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:like:item', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.2863, 0.3543, 0.7422, 0.8763, 0.5493],\n",
      "                        [0.3886, 0.4070, 0.3869, 0.1812, 0.0482],\n",
      "                        [0.2553, 0.6988, 0.3900, 0.3999, 0.4420],\n",
      "                        ...,\n",
      "                        [0.6460, 0.9617, 0.2179, 0.0102, 0.0578],\n",
      "                        [0.2340, 0.9769, 0.6430, 0.5848, 0.4092],\n",
      "                        [0.6932, 0.5044, 0.2852, 0.1045, 0.6047]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:follow:user', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.9888, 0.1472, 0.6974, 0.9802, 0.8006],\n",
      "                        [0.1779, 0.2944, 0.3611, 0.0277, 0.2849],\n",
      "                        [0.9499, 0.9656, 0.4811, 0.5362, 0.3451],\n",
      "                        ...,\n",
      "                        [0.8413, 0.5190, 0.0623, 0.9711, 0.9392],\n",
      "                        [0.6232, 0.2272, 0.9216, 0.1067, 0.7927],\n",
      "                        [0.4090, 0.7458, 0.3875, 0.5928, 0.6130]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:follow:user', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.4457, 0.8728, 0.1664, 0.4060, 0.7022],\n",
      "                        [0.9301, 0.8870, 0.4664, 0.1720, 0.7307],\n",
      "                        [0.2583, 0.8503, 0.9456, 0.8187, 0.6244],\n",
      "                        ...,\n",
      "                        [0.0466, 0.2596, 0.8004, 0.0652, 0.5579],\n",
      "                        [0.0887, 0.1932, 0.3230, 0.1477, 0.1207],\n",
      "                        [0.5230, 0.5375, 0.2686, 0.3251, 0.9167]]),\n",
      "        metadata={},\n",
      "    )}\n",
      ")\n",
      "\n",
      "Loaded node classification task: OnDiskTask(validation_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([710, 921, 649,   3, 292, 699, 229,  11, 895, 286, 488, 913,  52, 150,\n",
      "                                242, 231, 325, 331, 153, 669, 593, 744, 757, 998, 184, 946, 831,  63,\n",
      "                                594,  21, 860, 976, 788, 719, 595, 554, 665, 531, 256, 816, 351, 283,\n",
      "                                633, 743, 368, 596, 440, 820, 548, 567, 315,   9, 830, 268, 583, 898,\n",
      "                                777, 996, 838, 755, 300, 619, 919, 620,  51, 566, 624,  44, 908, 161,\n",
      "                                882, 439, 396, 597, 613, 991, 467, 909, 801, 791, 339, 277, 367, 189,\n",
      "                                857, 815, 196, 853, 880, 883, 702,  70, 118, 975, 109, 183, 342, 854,\n",
      "                                933, 122, 222, 321, 324, 682, 664, 497, 797, 851, 623, 423,  53, 187,\n",
      "                                713, 156, 617, 932, 200, 589, 192, 606, 347, 306, 172, 987, 956, 962,\n",
      "                                923, 407, 344, 821, 929, 445,  14, 611, 397, 289, 507, 614, 694, 770,\n",
      "                                436, 388,  75, 760, 866, 171, 696, 953, 672, 408, 887, 264, 481, 843,\n",
      "                                762, 936, 793, 515, 790,  25, 714, 382, 971, 685, 208, 168, 698, 674,\n",
      "                                469,  30, 104, 902, 480, 563, 330, 418, 270, 957, 795, 254, 943, 248,\n",
      "                                607, 907,  32, 881, 602, 715, 799, 125, 618, 588, 673,  65, 336, 354,\n",
      "                                 72, 720,  89, 973], dtype=torch.int32), tensor([4, 8, 0, 3, 8, 9, 4, 9, 5, 0, 2, 6, 7, 1, 6, 6, 9, 1, 3, 1, 3, 4, 6, 1,\n",
      "                                7, 8, 8, 8, 6, 0, 9, 1, 4, 9, 7, 4, 4, 2, 7, 7, 9, 8, 3, 5, 5, 9, 1, 3,\n",
      "                                4, 1, 6, 0, 5, 5, 3, 4, 1, 8, 2, 0, 2, 7, 7, 5, 5, 7, 9, 4, 9, 6, 8, 8,\n",
      "                                8, 3, 8, 7, 5, 7, 1, 7, 6, 1, 3, 8, 1, 2, 1, 8, 8, 7, 2, 7, 8, 4, 6, 4,\n",
      "                                8, 6, 8, 6, 8, 7, 4, 3, 2, 7, 0, 9, 8, 9, 4, 4, 1, 2, 8, 0, 0, 6, 8, 3,\n",
      "                                9, 9, 6, 7, 8, 3, 2, 7, 3, 5, 7, 6, 1, 5, 1, 9, 1, 6, 6, 6, 8, 8, 2, 4,\n",
      "                                8, 9, 0, 6, 4, 8, 8, 0, 5, 8, 0, 0, 8, 3, 4, 9, 0, 2, 1, 6, 6, 4, 5, 2,\n",
      "                                1, 1, 6, 1, 8, 0, 6, 2, 6, 3, 3, 0, 7, 6, 1, 7, 5, 9, 1, 2, 9, 0, 5, 7,\n",
      "                                1, 5, 2, 3, 6, 1, 9, 7])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([ 88, 274, 294, 463, 831,  32, 681, 124, 913, 895, 944, 880, 706, 875,\n",
      "                                634, 778, 997, 161,  78, 699, 935, 747, 382, 642, 270, 856, 543, 703,\n",
      "                                380, 523,  11, 263, 564, 800, 622,   6,   9, 121, 951, 656, 669, 947,\n",
      "                                441, 351, 820, 984, 690, 507, 151, 149, 686, 299, 757, 677, 814, 779,\n",
      "                                919, 361, 228, 651, 976,  57, 231, 470, 168, 209, 792, 824, 532,  26,\n",
      "                                 81, 589, 826,  72, 188, 370, 395, 398, 590, 104, 652, 727, 107, 126,\n",
      "                                909, 412, 816, 794, 524, 666, 990, 252, 451, 322, 282, 761, 290, 799,\n",
      "                                872, 930, 784, 936, 341, 482, 196, 781, 139, 751, 619,  23,  36, 389,\n",
      "                                588, 805, 514,  74, 668, 445, 870, 553, 974, 333, 667, 334, 741, 401,\n",
      "                                962, 254, 550, 698,  19, 432,  49, 626, 573, 363, 583, 241, 846, 678,\n",
      "                                871, 896, 425, 854, 402, 918,  80, 876, 430, 495, 155, 624,  45, 115,\n",
      "                                506, 904, 711, 314, 167, 130, 803, 908, 790,  89, 159, 198, 137, 317,\n",
      "                                592,  15, 165, 224, 310, 613, 410, 251, 345, 381, 208, 202, 862, 948,\n",
      "                                 44,  61, 618, 739, 774, 641, 938, 785, 249, 429, 329, 692, 954, 396,\n",
      "                                729, 857, 878, 464], dtype=torch.int32), tensor([4, 6, 4, 7, 5, 6, 3, 4, 2, 8, 2, 0, 5, 2, 0, 6, 7, 5, 4, 1, 0, 1, 9, 8,\n",
      "                                6, 4, 0, 6, 2, 0, 4, 0, 5, 0, 3, 0, 9, 0, 3, 1, 7, 5, 5, 9, 1, 1, 6, 6,\n",
      "                                0, 1, 4, 5, 7, 9, 3, 6, 6, 4, 5, 0, 2, 9, 9, 1, 6, 6, 0, 7, 4, 9, 4, 1,\n",
      "                                7, 1, 3, 8, 9, 0, 5, 9, 2, 6, 6, 7, 4, 8, 1, 2, 9, 1, 0, 5, 1, 8, 4, 0,\n",
      "                                6, 8, 1, 2, 6, 2, 5, 0, 4, 5, 2, 5, 9, 9, 2, 3, 8, 7, 9, 3, 6, 2, 0, 6,\n",
      "                                4, 7, 8, 6, 1, 8, 1, 5, 9, 6, 7, 5, 1, 4, 0, 0, 3, 1, 5, 5, 4, 6, 4, 0,\n",
      "                                5, 7, 5, 9, 7, 9, 8, 1, 9, 1, 0, 3, 9, 8, 7, 6, 0, 3, 3, 4, 5, 0, 7, 2,\n",
      "                                7, 5, 0, 4, 7, 9, 6, 0, 2, 5, 8, 7, 7, 0, 3, 0, 1, 3, 8, 1, 7, 8, 5, 6,\n",
      "                                8, 2, 7, 8, 5, 5, 2, 0])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           train_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([449, 329, 333, 112, 536, 474, 464, 733, 697, 466, 826, 960, 158, 616,\n",
      "                                201, 447, 847, 890, 379,  43,  49, 870, 245, 424, 735, 334, 149, 924,\n",
      "                                 16, 437, 722, 966, 213, 811, 940, 198,  42, 130, 961, 759, 162, 539,\n",
      "                                 59, 935, 984, 279, 670, 988, 413, 405,  55, 813, 941, 291, 105, 290,\n",
      "                                147, 532, 569, 565, 350, 802, 233, 986, 516, 572, 732,  91, 863, 476,\n",
      "                                570, 992, 904, 551, 479, 900, 601, 544, 604, 332, 394, 775, 176, 912,\n",
      "                                712, 375, 505, 543, 605, 502, 206, 963, 692, 494, 143, 510, 111, 304,\n",
      "                                974, 259, 227, 622, 886, 658, 716, 475,  68, 814, 869, 765, 704, 771,\n",
      "                                346, 651, 322, 965, 364, 167, 737, 568, 779, 357, 967, 806, 668,  90,\n",
      "                                561, 528, 899, 845, 861, 163, 709,  73, 280, 517, 343, 724, 293,  38,\n",
      "                                944, 656, 443, 945,  57, 950, 707,  12,  84, 335, 916, 573,  56, 590,\n",
      "                                420, 730, 491, 823, 178, 952, 459, 359, 148, 862,  20, 100, 159, 427,\n",
      "                                844, 352, 404, 891, 931, 255, 509, 361, 215, 579, 803, 486,  50, 858,\n",
      "                                 92, 384, 261, 376, 868, 750, 113, 654, 888, 260,   6, 557, 239, 337,\n",
      "                                582, 498, 307, 928, 650, 194, 641, 905,  93, 381, 127, 639, 142, 690,\n",
      "                                338, 901, 756, 939, 412, 637, 778, 274, 318, 348, 661, 303, 535, 514,\n",
      "                                630, 558, 648, 129, 278, 577, 185, 951,  66, 220, 796,   5, 155, 140,\n",
      "                                609,  95, 893, 345, 608, 632, 519, 501, 761,  19, 166, 832, 764, 503,\n",
      "                                807, 798, 302, 706,   7, 377, 415, 836, 817, 124, 457, 575, 726, 195,\n",
      "                                431, 808, 805, 323, 487, 410, 754, 107, 727, 308, 848, 738, 550, 794,\n",
      "                                116, 810, 875, 512, 927, 179, 600, 599, 422,  36, 428, 371,  58,  15,\n",
      "                                170, 287, 320,   4, 982, 839, 533,  29, 914, 395, 837, 454, 746, 825,\n",
      "                                918, 295, 444, 247, 717, 947, 999, 742, 128, 273, 373, 177, 867, 411,\n",
      "                                994, 313, 298, 374, 835, 529, 545, 812, 954, 392, 666, 559, 115,  10,\n",
      "                                380, 190, 432,  22, 822, 452, 309, 266, 101, 972, 369, 701, 782, 181,\n",
      "                                  2, 110, 688, 205, 191, 979, 160,  83, 663, 729, 249, 657, 578, 154,\n",
      "                                917,  17, 819, 484, 612, 326, 120, 226, 456, 372, 106, 997, 871, 232,\n",
      "                                542,  78, 521, 117, 631, 269, 615, 102, 581, 441, 652, 370, 934, 251,\n",
      "                                 31, 399, 225, 586, 358, 653, 135, 721,  80, 680, 164, 786, 678,  64,\n",
      "                                 27, 301, 574, 221, 772, 930, 571, 585, 876,  61, 311, 366, 430, 785,\n",
      "                                995, 340, 681,  28, 199, 792, 378, 288, 465, 877, 809, 121, 275, 314,\n",
      "                                108,  48, 958, 499, 223, 417, 472, 834, 879, 889, 203, 492, 473, 258,\n",
      "                                691, 576, 874, 414, 748, 647, 362, 922, 833,  37, 640, 774, 736, 401,\n",
      "                                864, 386, 461, 434, 485, 146, 626, 989, 460, 138, 562, 212, 294, 216,\n",
      "                                446, 188, 458, 489, 896, 703, 766, 365, 353, 739, 490, 508, 734, 842,\n",
      "                                136, 145, 758, 667, 207, 504, 636,  69, 660, 217, 438, 463, 506, 700,\n",
      "                                920, 391, 271,  76, 824, 387, 448, 312, 985, 556, 949, 230, 114, 787,\n",
      "                                 45, 538, 634, 144, 859, 134, 993,  98, 482, 910, 534, 123, 210, 591,\n",
      "                                526, 182, 234, 878, 980,  34, 390, 356, 849, 541, 969, 970, 462, 926,\n",
      "                                 41,  88, 209,  18, 800, 285, 241, 180, 530, 363, 784, 676, 416, 547,\n",
      "                                549, 892, 856, 511,  35, 679, 522,  67, 253, 450, 406, 560, 731, 852,\n",
      "                                214, 169, 695, 540, 686,   1, 584, 981, 296, 527, 211, 903, 804,   0,\n",
      "                                267, 546, 523, 524,  23, 433, 355,  47, 470, 219, 500, 897],\n",
      "                               dtype=torch.int32), tensor([2, 9, 7, 2, 5, 2, 6, 7, 5, 2, 7, 3, 0, 7, 7, 2, 3, 6, 0, 9, 2, 9, 1, 5,\n",
      "                                7, 1, 0, 0, 6, 0, 6, 0, 6, 9, 6, 9, 6, 8, 1, 8, 1, 8, 7, 0, 0, 2, 6, 5,\n",
      "                                5, 0, 8, 0, 7, 4, 1, 6, 1, 4, 4, 3, 7, 3, 4, 2, 4, 8, 7, 7, 6, 9, 7, 2,\n",
      "                                1, 2, 4, 5, 5, 7, 3, 9, 4, 8, 9, 2, 5, 7, 9, 0, 9, 5, 2, 6, 5, 9, 2, 4,\n",
      "                                0, 0, 2, 9, 4, 0, 8, 7, 9, 2, 9, 4, 7, 1, 8, 4, 6, 1, 0, 6, 8, 8, 5, 7,\n",
      "                                3, 3, 5, 0, 2, 4, 7, 9, 4, 9, 7, 1, 0, 8, 4, 8, 7, 1, 4, 5, 9, 5, 2, 9,\n",
      "                                4, 3, 5, 4, 0, 3, 5, 3, 8, 1, 5, 4, 1, 8, 4, 5, 3, 4, 9, 1, 7, 8, 0, 7,\n",
      "                                7, 1, 7, 1, 6, 1, 0, 9, 3, 9, 2, 3, 5, 9, 9, 1, 8, 8, 6, 7, 7, 6, 7, 5,\n",
      "                                6, 5, 7, 3, 4, 3, 1, 8, 6, 5, 5, 3, 9, 9, 3, 5, 4, 5, 4, 2, 6, 6, 8, 9,\n",
      "                                6, 1, 8, 9, 3, 6, 0, 8, 7, 3, 8, 7, 5, 6, 9, 1, 5, 9, 8, 9, 4, 5, 2, 8,\n",
      "                                5, 0, 4, 2, 4, 4, 0, 7, 3, 7, 3, 2, 1, 7, 9, 3, 4, 3, 4, 5, 4, 0, 8, 0,\n",
      "                                3, 5, 3, 0, 4, 2, 7, 7, 6, 9, 3, 8, 4, 3, 7, 1, 4, 0, 5, 2, 9, 5, 2, 2,\n",
      "                                7, 9, 4, 6, 3, 4, 6, 3, 9, 9, 8, 3, 6, 6, 2, 7, 7, 3, 0, 6, 3, 1, 0, 8,\n",
      "                                8, 3, 8, 0, 9, 9, 7, 7, 5, 6, 0, 9, 4, 8, 2, 9, 7, 4, 1, 9, 3, 8, 3, 1,\n",
      "                                5, 4, 2, 1, 0, 8, 7, 6, 2, 7, 4, 3, 4, 5, 1, 5, 0, 5, 2, 6, 5, 0, 9, 0,\n",
      "                                5, 1, 6, 4, 8, 5, 3, 5, 6, 7, 7, 2, 1, 6, 5, 2, 4, 4, 5, 4, 6, 9, 7, 1,\n",
      "                                0, 2, 7, 5, 5, 6, 4, 6, 1, 2, 2, 3, 4, 6, 7, 7, 3, 9, 5, 1, 2, 0, 1, 4,\n",
      "                                0, 0, 9, 3, 4, 3, 6, 0, 6, 1, 5, 8, 9, 3, 7, 7, 7, 1, 3, 9, 0, 9, 1, 5,\n",
      "                                2, 9, 1, 5, 2, 6, 9, 4, 0, 0, 1, 6, 4, 5, 6, 5, 1, 8, 8, 1, 6, 7, 0, 1,\n",
      "                                7, 0, 4, 1, 7, 0, 6, 3, 9, 7, 5, 5, 1, 1, 5, 8, 2, 8, 6, 2, 5, 1, 6, 4,\n",
      "                                0, 9, 4, 5, 0, 1, 0, 5, 3, 0, 1, 7, 9, 8, 8, 1, 2, 3, 7, 9, 5, 1, 7, 1,\n",
      "                                6, 9, 1, 4, 9, 2, 2, 9, 7, 6, 1, 5, 4, 9, 0, 4, 2, 3, 7, 7, 4, 6, 8, 9,\n",
      "                                0, 7, 4, 6, 3, 1, 2, 0, 8, 4, 5, 3, 3, 2, 6, 3, 7, 5, 4, 8, 8, 3, 8, 2,\n",
      "                                5, 4, 0, 5, 0, 5, 0, 0, 1, 2, 8, 5, 0, 4, 3, 0, 5, 1, 1, 1, 1, 7, 4, 8,\n",
      "                                7, 8, 5, 8, 5, 5, 0, 6, 0, 3, 8, 7, 3, 9, 8, 6, 7, 0, 1, 8, 5, 4, 4, 7])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([421, 331,  96, 145, 851, 474, 108, 838, 617, 637, 873, 201, 657, 522,\n",
      "                                684, 994, 901, 520, 280, 808, 971, 493, 318, 584, 222, 343, 475, 486,\n",
      "                                456, 174,  30, 365, 587, 958, 628, 240, 192,  64, 664, 992, 581, 635,\n",
      "                                481, 786, 245, 915, 847, 810, 312, 932, 128, 338, 554, 643, 358, 134,\n",
      "                                 58,  54, 676, 415,  55,  83, 211, 371, 179, 303,  94, 154, 720, 968,\n",
      "                                683, 609, 509, 112, 100, 680, 320,  92,   0, 970, 775, 180,  99, 942,\n",
      "                                644, 397, 203, 246, 985, 527, 546, 693, 762, 889, 275, 770, 580, 648,\n",
      "                                394, 780,   3, 419, 466,  22, 806, 335, 257, 364, 764, 763, 603, 530,\n",
      "                                374, 247, 422, 533, 963, 840, 399, 575, 271, 487, 828, 175, 867, 242,\n",
      "                                740, 989, 367, 444, 146, 773, 305, 983, 516, 557,  46, 356, 239, 636,\n",
      "                                723, 898, 384, 123, 283, 248, 199, 754, 383, 567, 340, 388, 548, 449,\n",
      "                                829, 625, 726, 349, 977,  48, 789, 217, 454, 223, 278,  18, 519,   7,\n",
      "                                140, 695, 417, 446, 437,  82, 300, 301, 212, 485, 555, 731, 758, 319,\n",
      "                                133, 987, 707, 725,  77, 261, 869, 448, 213, 841, 336, 874, 218, 316,\n",
      "                                 90,  25, 964, 886, 719, 830, 428, 122, 959, 585, 521, 710, 504, 465,\n",
      "                                164, 903, 993, 750, 323, 531,  73, 982, 756, 193, 916,  47, 749, 443,\n",
      "                                479, 166, 697,   8, 330, 183, 675,   2, 328, 497, 638, 809, 662, 767,\n",
      "                                 63, 860, 114, 344, 743, 694, 268, 525, 980, 260, 129, 709, 566, 285,\n",
      "                                788, 431, 256, 293, 850, 998,  67, 176, 593, 771, 339, 125, 865, 462,\n",
      "                                631, 287, 907, 665, 118, 232, 408, 295, 273, 508, 106,   4, 735, 902,\n",
      "                                500,  17, 877, 348, 378, 579, 765,  50, 511,  29, 863, 138, 184, 253,\n",
      "                                 68, 679,  56, 577, 457, 973, 267, 606, 423, 127, 646, 745, 512,  60,\n",
      "                                812,  37, 769, 975, 645, 933, 501,  35, 324, 496, 717,  41,  65, 674,\n",
      "                                359, 390, 934, 406,  93, 109, 513, 586, 852, 597, 558, 755, 629, 162,\n",
      "                                914, 181, 342,  91, 230, 804, 602, 578, 929, 920, 891, 102, 288, 768,\n",
      "                                435, 304,  69,  53, 452, 315, 152, 791, 103, 400, 659, 144, 596, 472,\n",
      "                                716, 562, 326, 391, 187, 194, 673, 885, 705, 157, 447, 615, 610, 654,\n",
      "                                937, 210, 101, 798, 414, 276, 468, 131, 434, 868, 269, 535, 931, 633,\n",
      "                                817,  34, 647, 226, 264, 369, 718, 897,  14, 262, 892, 499, 407, 150,\n",
      "                                238, 835, 473, 565, 117, 700, 861, 277, 961, 308, 539, 738, 859, 453,\n",
      "                                 52, 298, 713, 724, 216,  86, 119,  24, 888,  33, 460, 836, 362, 160,\n",
      "                                772, 569, 708, 614, 266, 737, 136, 760, 255, 418, 215, 714,  76, 607,\n",
      "                                450, 427, 505, 881,   1, 302, 640, 882, 200, 736, 682, 536, 477, 921,\n",
      "                                424, 884, 542, 547, 594, 819,  40, 905, 111, 834, 385, 236, 405, 821,\n",
      "                                844, 503, 928, 956, 691, 325, 639, 802, 766, 420, 189,  27, 926,   5,\n",
      "                                832, 346,  42, 950, 649, 653, 576, 279, 148, 721, 205, 887, 670, 313,\n",
      "                                843, 996, 483, 258, 284, 730, 377, 489, 311, 207, 484, 574, 696, 911,\n",
      "                                234, 393, 158, 945, 191, 366, 815, 795, 177, 953, 561, 776, 650, 917,\n",
      "                                744, 281, 608, 476,  84, 979, 924, 822, 214, 906, 116, 957, 467, 999,\n",
      "                                943, 469, 186, 296, 537,  39, 827, 105, 549, 544, 722, 327, 491, 510,\n",
      "                                671, 620,  28, 197, 411, 833, 353, 742, 621,  43, 265, 910, 259, 404,\n",
      "                                940, 458, 233, 893, 292, 272, 526, 113, 660, 818, 517,  95,  20, 143,\n",
      "                                 21, 220, 655, 879, 663, 611, 219, 595, 170, 981, 952, 787],\n",
      "                               dtype=torch.int32), tensor([9, 6, 2, 5, 9, 1, 7, 3, 9, 3, 7, 2, 6, 7, 3, 6, 9, 7, 3, 4, 8, 7, 0, 3,\n",
      "                                5, 7, 1, 2, 4, 5, 9, 0, 3, 4, 1, 0, 5, 8, 0, 1, 0, 2, 0, 0, 2, 2, 1, 6,\n",
      "                                7, 3, 8, 6, 7, 6, 9, 1, 0, 3, 0, 9, 3, 7, 1, 7, 5, 1, 4, 6, 6, 8, 2, 0,\n",
      "                                4, 4, 7, 5, 8, 4, 2, 5, 4, 4, 6, 6, 4, 4, 8, 6, 9, 3, 1, 7, 5, 3, 6, 5,\n",
      "                                9, 6, 4, 9, 8, 1, 5, 1, 8, 7, 3, 7, 4, 5, 4, 2, 7, 7, 6, 8, 0, 9, 8, 0,\n",
      "                                2, 5, 2, 4, 5, 5, 8, 7, 8, 4, 2, 6, 5, 4, 6, 3, 3, 4, 1, 1, 2, 0, 4, 9,\n",
      "                                1, 3, 3, 5, 2, 2, 0, 4, 8, 0, 3, 8, 2, 2, 2, 3, 6, 0, 1, 4, 9, 6, 1, 3,\n",
      "                                4, 3, 0, 7, 1, 3, 6, 4, 5, 5, 2, 9, 1, 6, 7, 2, 8, 1, 4, 7, 3, 1, 8, 2,\n",
      "                                9, 0, 8, 7, 1, 7, 4, 6, 0, 7, 7, 7, 6, 3, 1, 4, 3, 0, 4, 8, 8, 2, 8, 7,\n",
      "                                5, 3, 9, 8, 3, 8, 6, 2, 7, 6, 9, 3, 3, 6, 4, 1, 6, 3, 6, 4, 2, 7, 4, 2,\n",
      "                                7, 6, 0, 0, 2, 1, 7, 5, 8, 9, 4, 1, 1, 5, 3, 4, 6, 6, 8, 2, 4, 5, 9, 1,\n",
      "                                1, 5, 6, 7, 0, 2, 1, 6, 2, 1, 8, 4, 0, 0, 0, 0, 2, 5, 4, 8, 4, 7, 1, 3,\n",
      "                                3, 2, 9, 6, 1, 6, 6, 2, 7, 7, 6, 3, 3, 6, 8, 1, 3, 2, 2, 5, 2, 4, 5, 1,\n",
      "                                5, 1, 4, 9, 0, 6, 4, 1, 1, 1, 8, 5, 1, 7, 6, 3, 2, 6, 1, 4, 4, 8, 3, 0,\n",
      "                                7, 4, 3, 6, 5, 2, 1, 4, 4, 0, 0, 1, 2, 2, 2, 7, 8, 1, 2, 6, 9, 4, 9, 3,\n",
      "                                1, 0, 2, 4, 4, 9, 5, 9, 3, 8, 8, 0, 6, 2, 1, 0, 1, 3, 0, 2, 4, 3, 7, 6,\n",
      "                                0, 4, 8, 4, 7, 3, 0, 2, 7, 3, 3, 7, 0, 1, 1, 1, 6, 2, 0, 8, 2, 7, 2, 8,\n",
      "                                4, 1, 3, 1, 8, 6, 1, 7, 0, 6, 5, 2, 8, 3, 2, 3, 7, 7, 1, 9, 4, 8, 3, 9,\n",
      "                                4, 7, 7, 4, 2, 0, 8, 4, 1, 9, 1, 5, 8, 9, 5, 5, 0, 9, 6, 1, 9, 7, 0, 8,\n",
      "                                8, 1, 4, 1, 3, 1, 0, 6, 6, 9, 4, 1, 8, 5, 7, 3, 9, 5, 4, 0, 1, 6, 2, 5,\n",
      "                                3, 4, 3, 0, 4, 6, 4, 7, 8, 2, 3, 1, 8, 0, 9, 5, 9, 5, 9, 7, 0, 4, 8, 0,\n",
      "                                2, 3, 3, 2, 0, 0, 3, 1, 5, 2, 9, 9, 6, 4, 3, 5, 6, 4, 1, 5, 0, 8, 3, 5,\n",
      "                                6, 7, 6, 0, 8, 9, 5, 3, 5, 0, 1, 6, 5, 7, 9, 2, 9, 0, 9, 3, 9, 4, 3, 1,\n",
      "                                1, 7, 2, 9, 0, 2, 5, 1, 7, 4, 0, 0, 7, 8, 0, 3, 1, 8, 1, 2, 6, 0, 5, 7,\n",
      "                                5, 1, 1, 6, 2, 2, 3, 4, 4, 9, 3, 0, 9, 2, 1, 4, 7, 7, 2, 2, 3, 2, 6, 1])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           test_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([240, 628,  39,  86, 644, 451, 327, 828, 783, 627, 776, 552, 747, 400,\n",
      "                                 94, 246, 398, 218, 689, 383, 629, 193,  54, 360, 555, 598, 173, 938,\n",
      "                                141, 959, 257, 349, 884, 635, 435, 202,  99, 646, 767, 773, 708, 662,\n",
      "                                389, 317, 687, 493,  74, 789, 276,  82, 553, 204, 425, 478, 299,  87,\n",
      "                                310,  13, 243, 328, 752, 453, 642, 745, 603, 281, 471, 316,  46,  33,\n",
      "                                625, 840, 855, 186, 968, 964, 937, 948, 409, 643, 711, 915, 483, 872,\n",
      "                                580, 403, 250, 763, 197, 393, 157, 139, 677, 725, 978, 496, 705, 587,\n",
      "                                426, 925, 235, 103, 818, 873, 272, 252, 827, 983, 419,  26, 885, 228,\n",
      "                                 77, 131, 421, 592,  60, 655, 955, 723, 119, 610,  85, 165, 385, 942,\n",
      "                                442, 675, 781, 525, 137, 671, 402, 518, 237, 894, 829, 133, 740, 468,\n",
      "                                741,  81, 513, 684, 477, 341,  40,  97, 621, 265, 718, 175,  79, 753,\n",
      "                                284, 846, 262, 429, 841, 749,  62, 693, 728, 865, 263,  24, 564, 152,\n",
      "                                638, 990, 911, 906, 520, 236, 495, 132, 224,  71, 282, 751, 645, 455,\n",
      "                                659, 305, 683, 977, 780, 151, 769, 537, 238, 244, 126,   8,  96, 319,\n",
      "                                768, 297, 850, 174], dtype=torch.int32), tensor([2, 3, 7, 0, 0, 9, 2, 1, 8, 4, 0, 0, 6, 5, 2, 3, 9, 9, 7, 4, 8, 0, 2, 7,\n",
      "                                8, 2, 4, 8, 9, 2, 8, 3, 5, 4, 2, 2, 5, 2, 9, 8, 1, 2, 8, 8, 3, 2, 6, 9,\n",
      "                                9, 6, 9, 9, 7, 8, 6, 9, 5, 2, 7, 1, 2, 5, 6, 7, 1, 7, 1, 0, 2, 0, 5, 2,\n",
      "                                0, 4, 7, 4, 2, 6, 3, 6, 1, 6, 6, 3, 9, 1, 3, 4, 4, 0, 7, 8, 0, 1, 5, 2,\n",
      "                                2, 3, 9, 3, 9, 4, 8, 1, 9, 2, 7, 0, 3, 9, 7, 5, 5, 5, 0, 9, 2, 2, 4, 8,\n",
      "                                9, 7, 5, 6, 8, 3, 5, 4, 0, 1, 1, 4, 0, 6, 7, 3, 1, 1, 6, 1, 2, 2, 3, 5,\n",
      "                                6, 4, 1, 6, 7, 7, 9, 0, 0, 3, 5, 4, 9, 3, 4, 9, 5, 3, 8, 4, 7, 2, 4, 6,\n",
      "                                6, 6, 6, 1, 7, 4, 1, 0, 6, 8, 3, 0, 6, 6, 0, 7, 6, 6, 8, 1, 0, 5, 1, 9,\n",
      "                                1, 3, 7, 1, 2, 9, 2, 9])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([225, 688, 357, 582, 701, 297, 632, 518, 528, 442, 413, 120, 515, 732,\n",
      "                                 51, 972, 227, 672, 559, 630, 899, 811, 147, 598, 969, 661,  97, 135,\n",
      "                                355, 286, 563, 375, 250, 807, 823, 235, 658, 813, 782, 748, 471, 728,\n",
      "                                440, 556, 204, 173, 894, 494, 409, 337, 599, 545, 858, 169, 941, 206,\n",
      "                                839, 687, 480, 825, 966, 995, 883, 387, 185,  98, 426,  87,  31, 368,\n",
      "                                551, 991, 927, 845, 604, 438, 379, 986, 960, 796,  70, 600, 988, 685,\n",
      "                                571, 492, 538,  62, 752, 570,  13, 534, 498, 965, 132, 759, 195, 352,\n",
      "                                221, 141, 461, 853, 900, 433, 746, 783, 237, 376, 373, 733, 455, 923,\n",
      "                                967, 243, 307, 244, 182, 347, 925, 502, 704, 591, 354,  71, 978, 142,\n",
      "                                601, 488, 801, 178, 321, 171, 110,  79, 229, 560, 360, 540, 777, 848,\n",
      "                                190,  66, 439,  38, 712, 478, 955, 386, 529, 572, 912, 403, 949, 842,\n",
      "                                156,  16, 837, 702, 689, 939, 153,  10, 490, 627, 855, 163, 753, 616,\n",
      "                                309, 541,  59, 793, 306, 416, 866, 605,  75, 350, 890, 797,  12,  85,\n",
      "                                392, 849, 332, 734, 372, 715, 864, 436, 568, 289, 291, 946, 612, 552,\n",
      "                                172, 922, 623, 459], dtype=torch.int32), tensor([7, 7, 8, 4, 0, 0, 0, 7, 6, 1, 4, 2, 6, 7, 5, 2, 8, 7, 8, 5, 9, 7, 0, 9,\n",
      "                                9, 4, 0, 1, 7, 5, 0, 5, 6, 1, 3, 8, 0, 3, 8, 7, 6, 4, 2, 9, 2, 7, 2, 2,\n",
      "                                1, 5, 9, 5, 7, 0, 8, 2, 3, 2, 6, 3, 1, 2, 2, 5, 8, 5, 8, 7, 2, 3, 0, 4,\n",
      "                                8, 0, 6, 1, 1, 5, 1, 3, 2, 8, 9, 4, 9, 8, 3, 2, 7, 1, 1, 1, 9, 5, 2, 2,\n",
      "                                6, 5, 4, 4, 6, 0, 7, 7, 6, 6, 5, 8, 5, 1, 3, 2, 4, 5, 8, 2, 9, 8, 7, 0,\n",
      "                                4, 2, 7, 6, 8, 4, 2, 6, 1, 8, 4, 7, 8, 4, 0, 4, 9, 8, 0, 9, 0, 6, 7, 1,\n",
      "                                2, 6, 9, 4, 3, 8, 5, 0, 9, 8, 6, 2, 4, 7, 8, 2, 5, 7, 9, 4, 6, 3, 1, 3,\n",
      "                                7, 9, 5, 6, 1, 0, 2, 1, 7, 2, 7, 2, 4, 4, 9, 5, 9, 3, 1, 7, 2, 6, 4, 4,\n",
      "                                0, 8, 4, 8, 6, 5, 8, 0])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           metadata={'name': 'node_classification', 'num_classes': 10},)\n",
      "\n",
      "Loaded link prediction task: OnDiskTask(validation_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[ 78, 352],\n",
      "                                [514, 127],\n",
      "                                [164, 780],\n",
      "                                ...,\n",
      "                                [400, 661],\n",
      "                                [400, 615],\n",
      "                                [400, 871]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[901, 957],\n",
      "                                [227,  52],\n",
      "                                [769, 857],\n",
      "                                ...,\n",
      "                                [583, 109],\n",
      "                                [583, 116],\n",
      "                                [583,   6]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels', 'indexes'),\n",
      "           ),\n",
      "           train_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[461, 584],\n",
      "                                [410, 546],\n",
      "                                [972, 165],\n",
      "                                ...,\n",
      "                                [733, 956],\n",
      "                                [340, 550],\n",
      "                                [935, 749]], dtype=torch.int32),),\n",
      "                            names=('seeds',),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[367,  77],\n",
      "                                [169, 971],\n",
      "                                [529, 825],\n",
      "                                ...,\n",
      "                                [ 74, 539],\n",
      "                                [285, 869],\n",
      "                                [  1, 988]], dtype=torch.int32),),\n",
      "                            names=('seeds',),\n",
      "                        )},\n",
      "               names=('seeds',),\n",
      "           ),\n",
      "           test_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[833, 430],\n",
      "                                [924, 894],\n",
      "                                [998, 574],\n",
      "                                ...,\n",
      "                                [280, 310],\n",
      "                                [280, 825],\n",
      "                                [280, 902]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[353, 847],\n",
      "                                [552, 260],\n",
      "                                [ 40, 759],\n",
      "                                ...,\n",
      "                                [873,  52],\n",
      "                                [873, 794],\n",
      "                                [873,  19]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels', 'indexes'),\n",
      "           ),\n",
      "           metadata={'name': 'link_prediction', 'num_classes': 10},)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dgl/python/dgl/graphbolt/impl/ondisk_dataset.py:463: GBWarning: Edge feature is stored, but edge IDs are not saved.\n",
      "  gb_warning(\"Edge feature is stored, but edge IDs are not saved.\")\n"
     ]
    }
   ],
   "source": [
    "dataset = gb.OnDiskDataset(base_dir).load()\n",
    "graph = dataset.graph\n",
    "print(f\"Loaded graph: {graph}\\n\")\n",
    "\n",
    "feature = dataset.feature\n",
    "print(f\"Loaded feature store: {feature}\\n\")\n",
    "\n",
    "tasks = dataset.tasks\n",
    "nc_task = tasks[0]\n",
    "print(f\"Loaded node classification task: {nc_task}\\n\")\n",
    "lp_task = tasks[1]\n",
    "print(f\"Loaded link prediction task: {lp_task}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
