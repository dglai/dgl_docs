{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnFhPMaAfLtJ"
   },
   "source": [
    "# OnDiskDataset for Heterogeneous Graph\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/stochastic_training/ondisk_dataset_heterograph.ipynb) [![GitHub](https://img.shields.io/badge/-View%20on%20GitHub-181717?logo=github&logoColor=ffffff)](https://github.com/dmlc/dgl/blob/master/notebooks/stochastic_training/ondisk_dataset_heterograph.ipynb)\n",
    "\n",
    "This tutorial shows how to create `OnDiskDataset` for heterogeneous graph that could be used in **GraphBolt** framework. The major difference from creating dataset for homogeneous graph is that we need to specify node/edge types for edges, feature data, training/validation/test sets.\n",
    "\n",
    "By the end of this tutorial, you will be able to\n",
    "\n",
    "- organize graph structure data.\n",
    "- organize feature data.\n",
    "- organize training/validation/test set for specific tasks.\n",
    "\n",
    "To create an ``OnDiskDataset`` object, you need to organize all the data including graph structure, feature data and tasks into a directory. The directory should contain a ``metadata.yaml`` file that describes the metadata of the dataset.\n",
    "\n",
    "Now let's generate various data step by step and organize them together to instantiate `OnDiskDataset` finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlb19DtWgtzq"
   },
   "source": [
    "## Install DGL package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T07:27:17.027716Z",
     "iopub.status.busy": "2024-09-19T07:27:17.027372Z",
     "iopub.status.idle": "2024-09-19T07:27:20.054527Z",
     "shell.execute_reply": "2024-09-19T07:27:20.053515Z"
    },
    "id": "UojlT9ZGgyr9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels-test/repo.html\r\n",
      "Requirement already satisfied: dgl in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (2.2a240410)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (1.14.1)\r\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (3.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (4.66.5)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (6.0.0)\r\n",
      "Requirement already satisfied: torchdata>=0.5.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (0.8.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (2.2.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2.2.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2024.8.30)\r\n",
      "Requirement already satisfied: torch>=2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torchdata>=0.5.0->dgl) (2.1.0+cpu)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.9.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGL installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "os.environ['DGLBACKEND'] = \"pytorch\"\n",
    "\n",
    "# Install the CPU version.\n",
    "device = torch.device(\"cpu\")\n",
    "!pip install --pre dgl -f https://data.dgl.ai/wheels-test/repo.html\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    import dgl.graphbolt as gb\n",
    "    installed = True\n",
    "except ImportError as error:\n",
    "    installed = False\n",
    "    print(error)\n",
    "print(\"DGL installed!\" if installed else \"DGL not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2R7WnSbjsfbr"
   },
   "source": [
    "## Data preparation\n",
    "In order to demonstrate how to organize various data, let's create a base directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T07:27:20.057433Z",
     "iopub.status.busy": "2024-09-19T07:27:20.056906Z",
     "iopub.status.idle": "2024-09-19T07:27:20.062272Z",
     "shell.execute_reply": "2024-09-19T07:27:20.061403Z"
    },
    "id": "SZipbzyltLfO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created base directory: ./ondisk_dataset_heterograph\n"
     ]
    }
   ],
   "source": [
    "base_dir = './ondisk_dataset_heterograph'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "print(f\"Created base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhNtIn_xhlnl"
   },
   "source": [
    "### Generate graph structure data\n",
    "For heterogeneous graph, we need to save different edge edges(namely seeds) into separate **Numpy** or **CSV** files.\n",
    "\n",
    "Note:\n",
    "- when saving to **Numpy**, the array requires to be in shape of `(2, N)`. This format is recommended as constructing graph from it is much faster than **CSV** file.\n",
    "- when saving to **CSV** file, do not save index and header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T07:27:20.064477Z",
     "iopub.status.busy": "2024-09-19T07:27:20.064093Z",
     "iopub.status.idle": "2024-09-19T07:27:20.089671Z",
     "shell.execute_reply": "2024-09-19T07:27:20.088739Z"
    },
    "id": "HcBt4G5BmSjr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of [user:like:item] edges: [[530 906]\n",
      " [170 811]\n",
      " [431  23]\n",
      " [ 90 401]\n",
      " [548 140]]\n",
      "\n",
      "[user:like:item] edges are saved into ./ondisk_dataset_heterograph/like-edges.csv\n",
      "\n",
      "Part of [user:follow:user] edges: [[795  32]\n",
      " [321 815]\n",
      " [234 831]\n",
      " [997 684]\n",
      " [ 82  87]]\n",
      "\n",
      "[user:follow:user] edges are saved into ./ondisk_dataset_heterograph/follow-edges.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For simplicity, we create a heterogeneous graph with\n",
    "# 2 node types: `user`, `item`\n",
    "# 2 edge types: `user:like:item`, `user:follow:user`\n",
    "# And each node/edge type has the same number of nodes/edges.\n",
    "num_nodes = 1000\n",
    "num_edges = 10 * num_nodes\n",
    "\n",
    "# Edge type: \"user:like:item\"\n",
    "like_edges_path = os.path.join(base_dir, \"like-edges.csv\")\n",
    "like_edges = np.random.randint(0, num_nodes, size=(num_edges, 2))\n",
    "print(f\"Part of [user:like:item] edges: {like_edges[:5, :]}\\n\")\n",
    "\n",
    "df = pd.DataFrame(like_edges)\n",
    "df.to_csv(like_edges_path, index=False, header=False)\n",
    "print(f\"[user:like:item] edges are saved into {like_edges_path}\\n\")\n",
    "\n",
    "# Edge type: \"user:follow:user\"\n",
    "follow_edges_path = os.path.join(base_dir, \"follow-edges.csv\")\n",
    "follow_edges = np.random.randint(0, num_nodes, size=(num_edges, 2))\n",
    "print(f\"Part of [user:follow:user] edges: {follow_edges[:5, :]}\\n\")\n",
    "\n",
    "df = pd.DataFrame(follow_edges)\n",
    "df.to_csv(follow_edges_path, index=False, header=False)\n",
    "print(f\"[user:follow:user] edges are saved into {follow_edges_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh-4cPtzpcaH"
   },
   "source": [
    "### Generate feature data for graph\n",
    "For feature data, numpy arrays and torch tensors are supported for now. Let's generate feature data for each node/edge type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T07:27:20.092198Z",
     "iopub.status.busy": "2024-09-19T07:27:20.091995Z",
     "iopub.status.idle": "2024-09-19T07:27:20.114754Z",
     "shell.execute_reply": "2024-09-19T07:27:20.113887Z"
    },
    "id": "_PVu1u5brBhF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of node[user] feature [feat_0]: [[0.32360411 0.0561361  0.26397998 0.24043763 0.80448772]\n",
      " [0.81858033 0.24203719 0.80044174 0.83939496 0.1943281 ]\n",
      " [0.6199952  0.48460553 0.58947008 0.19963965 0.86860922]]\n",
      "Node[user] feature [feat_0] is saved to ./ondisk_dataset_heterograph/node-user-feat-0.npy\n",
      "\n",
      "Part of node[user] feature [feat_1]: tensor([[0.6393, 0.2935, 0.7817, 0.5309, 0.8513],\n",
      "        [0.3523, 0.5367, 0.0106, 0.2039, 0.9900],\n",
      "        [0.3833, 0.6105, 0.5327, 0.4471, 0.9685]])\n",
      "Node[user] feature [feat_1] is saved to ./ondisk_dataset_heterograph/node-user-feat-1.pt\n",
      "\n",
      "Part of node[item] feature [feat_0]: [[0.86192001 0.05067332 0.33874822 0.23789461 0.5989163 ]\n",
      " [0.4884144  0.65026805 0.09424973 0.75971783 0.06156225]\n",
      " [0.44024315 0.42225996 0.01302141 0.45946902 0.32924122]]\n",
      "Node[item] feature [feat_0] is saved to ./ondisk_dataset_heterograph/node-item-feat-0.npy\n",
      "\n",
      "Part of node[item] feature [feat_1]: tensor([[0.9045, 0.9076, 0.9799, 0.9640, 0.6546],\n",
      "        [0.5709, 0.2063, 0.0226, 0.0567, 0.5681],\n",
      "        [0.7979, 0.0573, 0.9011, 0.9951, 0.3689]])\n",
      "Node[item] feature [feat_1] is saved to ./ondisk_dataset_heterograph/node-item-feat-1.pt\n",
      "\n",
      "Part of edge[user:like:item] feature [feat_0]: [[0.64613077 0.93039138 0.94300171 0.25507203 0.21245229]\n",
      " [0.15490514 0.1477348  0.53056139 0.35704461 0.48779542]\n",
      " [0.10094831 0.79691679 0.11919351 0.29004019 0.99272341]]\n",
      "Edge[user:like:item] feature [feat_0] is saved to ./ondisk_dataset_heterograph/edge-like-feat-0.npy\n",
      "\n",
      "Part of edge[user:like:item] feature [feat_1]: tensor([[0.5407, 0.4052, 0.4030, 0.6136, 0.9814],\n",
      "        [0.5502, 0.5136, 0.0462, 0.2717, 0.0953],\n",
      "        [0.2576, 0.8391, 0.3014, 0.7970, 0.8010]])\n",
      "Edge[user:like:item] feature [feat_1] is saved to ./ondisk_dataset_heterograph/edge-like-feat-1.pt\n",
      "\n",
      "Part of edge[user:follow:user] feature [feat_0]: [[0.99319295 0.9463934  0.53166443 0.94183936 0.51002707]\n",
      " [0.42411933 0.30033281 0.10596687 0.27489065 0.14744356]\n",
      " [0.79537836 0.98174527 0.09718806 0.26474628 0.57895256]]\n",
      "Edge[user:follow:user] feature [feat_0] is saved to ./ondisk_dataset_heterograph/edge-follow-feat-0.npy\n",
      "\n",
      "Part of edge[user:follow:user] feature [feat_1]: tensor([[0.2728, 0.6300, 0.0400, 0.7417, 0.0648],\n",
      "        [0.6277, 0.0462, 0.5728, 0.8218, 0.3954],\n",
      "        [0.2750, 0.3536, 0.1480, 0.3750, 0.0895]])\n",
      "Edge[user:follow:user] feature [feat_1] is saved to ./ondisk_dataset_heterograph/edge-follow-feat-1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate node[user] feature in numpy array.\n",
    "node_user_feat_0_path = os.path.join(base_dir, \"node-user-feat-0.npy\")\n",
    "node_user_feat_0 = np.random.rand(num_nodes, 5)\n",
    "print(f\"Part of node[user] feature [feat_0]: {node_user_feat_0[:3, :]}\")\n",
    "np.save(node_user_feat_0_path, node_user_feat_0)\n",
    "print(f\"Node[user] feature [feat_0] is saved to {node_user_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another node[user] feature in torch tensor\n",
    "node_user_feat_1_path = os.path.join(base_dir, \"node-user-feat-1.pt\")\n",
    "node_user_feat_1 = torch.rand(num_nodes, 5)\n",
    "print(f\"Part of node[user] feature [feat_1]: {node_user_feat_1[:3, :]}\")\n",
    "torch.save(node_user_feat_1, node_user_feat_1_path)\n",
    "print(f\"Node[user] feature [feat_1] is saved to {node_user_feat_1_path}\\n\")\n",
    "\n",
    "# Generate node[item] feature in numpy array.\n",
    "node_item_feat_0_path = os.path.join(base_dir, \"node-item-feat-0.npy\")\n",
    "node_item_feat_0 = np.random.rand(num_nodes, 5)\n",
    "print(f\"Part of node[item] feature [feat_0]: {node_item_feat_0[:3, :]}\")\n",
    "np.save(node_item_feat_0_path, node_item_feat_0)\n",
    "print(f\"Node[item] feature [feat_0] is saved to {node_item_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another node[item] feature in torch tensor\n",
    "node_item_feat_1_path = os.path.join(base_dir, \"node-item-feat-1.pt\")\n",
    "node_item_feat_1 = torch.rand(num_nodes, 5)\n",
    "print(f\"Part of node[item] feature [feat_1]: {node_item_feat_1[:3, :]}\")\n",
    "torch.save(node_item_feat_1, node_item_feat_1_path)\n",
    "print(f\"Node[item] feature [feat_1] is saved to {node_item_feat_1_path}\\n\")\n",
    "\n",
    "# Generate edge[user:like:item] feature in numpy array.\n",
    "edge_like_feat_0_path = os.path.join(base_dir, \"edge-like-feat-0.npy\")\n",
    "edge_like_feat_0 = np.random.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:like:item] feature [feat_0]: {edge_like_feat_0[:3, :]}\")\n",
    "np.save(edge_like_feat_0_path, edge_like_feat_0)\n",
    "print(f\"Edge[user:like:item] feature [feat_0] is saved to {edge_like_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another edge[user:like:item] feature in torch tensor\n",
    "edge_like_feat_1_path = os.path.join(base_dir, \"edge-like-feat-1.pt\")\n",
    "edge_like_feat_1 = torch.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:like:item] feature [feat_1]: {edge_like_feat_1[:3, :]}\")\n",
    "torch.save(edge_like_feat_1, edge_like_feat_1_path)\n",
    "print(f\"Edge[user:like:item] feature [feat_1] is saved to {edge_like_feat_1_path}\\n\")\n",
    "\n",
    "# Generate edge[user:follow:user] feature in numpy array.\n",
    "edge_follow_feat_0_path = os.path.join(base_dir, \"edge-follow-feat-0.npy\")\n",
    "edge_follow_feat_0 = np.random.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:follow:user] feature [feat_0]: {edge_follow_feat_0[:3, :]}\")\n",
    "np.save(edge_follow_feat_0_path, edge_follow_feat_0)\n",
    "print(f\"Edge[user:follow:user] feature [feat_0] is saved to {edge_follow_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another edge[user:follow:user] feature in torch tensor\n",
    "edge_follow_feat_1_path = os.path.join(base_dir, \"edge-follow-feat-1.pt\")\n",
    "edge_follow_feat_1 = torch.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:follow:user] feature [feat_1]: {edge_follow_feat_1[:3, :]}\")\n",
    "torch.save(edge_follow_feat_1, edge_follow_feat_1_path)\n",
    "print(f\"Edge[user:follow:user] feature [feat_1] is saved to {edge_follow_feat_1_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyqgOtsIwzh_"
   },
   "source": [
    "### Generate tasks\n",
    "`OnDiskDataset` supports multiple tasks. For each task, we need to prepare training/validation/test sets respectively. Such sets usually vary among different tasks. In this tutorial, let's create a **Node Classification** task and **Link Prediction** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVxHaDIfzCkr"
   },
   "source": [
    "#### Node Classification Task\n",
    "For node classification task, we need **node IDs** and corresponding **labels** for each training/validation/test set. Like feature data, numpy arrays and torch tensors are supported for these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T07:27:20.117144Z",
     "iopub.status.busy": "2024-09-19T07:27:20.116753Z",
     "iopub.status.idle": "2024-09-19T07:27:20.136583Z",
     "shell.execute_reply": "2024-09-19T07:27:20.135684Z"
    },
    "id": "S5-fyBbHzTCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of train ids[user] for node classification: [859 916 628]\n",
      "NC train ids[user] are saved to ./ondisk_dataset_heterograph/nc-train-user-ids.npy\n",
      "\n",
      "Part of train labels[user] for node classification: tensor([8, 6, 5])\n",
      "NC train labels[user] are saved to ./ondisk_dataset_heterograph/nc-train-user-labels.pt\n",
      "\n",
      "Part of train ids[item] for node classification: [645 862 976]\n",
      "NC train ids[item] are saved to ./ondisk_dataset_heterograph/nc-train-item-ids.npy\n",
      "\n",
      "Part of train labels[item] for node classification: tensor([7, 2, 3])\n",
      "NC train labels[item] are saved to ./ondisk_dataset_heterograph/nc-train-item-labels.pt\n",
      "\n",
      "Part of val ids[user] for node classification: [796 542 969]\n",
      "NC val ids[user] are saved to ./ondisk_dataset_heterograph/nc-val-user-ids.npy\n",
      "\n",
      "Part of val labels[user] for node classification: tensor([4, 9, 5])\n",
      "NC val labels[user] are saved to ./ondisk_dataset_heterograph/nc-val-user-labels.pt\n",
      "\n",
      "Part of val ids[item] for node classification: [725 836 168]\n",
      "NC val ids[item] are saved to ./ondisk_dataset_heterograph/nc-val-item-ids.npy\n",
      "\n",
      "Part of val labels[item] for node classification: tensor([9, 6, 5])\n",
      "NC val labels[item] are saved to ./ondisk_dataset_heterograph/nc-val-item-labels.pt\n",
      "\n",
      "Part of test ids[user] for node classification: [857 282  48]\n",
      "NC test ids[user] are saved to ./ondisk_dataset_heterograph/nc-test-user-ids.npy\n",
      "\n",
      "Part of test labels[user] for node classification: tensor([0, 5, 6])\n",
      "NC test labels[user] are saved to ./ondisk_dataset_heterograph/nc-test-user-labels.pt\n",
      "\n",
      "Part of test ids[item] for node classification: [751 273 708]\n",
      "NC test ids[item] are saved to ./ondisk_dataset_heterograph/nc-test-item-ids.npy\n",
      "\n",
      "Part of test labels[item] for node classification: tensor([3, 7, 2])\n",
      "NC test labels[item] are saved to ./ondisk_dataset_heterograph/nc-test-item-labels.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration, let's generate item sets for each node type.\n",
    "num_trains = int(num_nodes * 0.6)\n",
    "num_vals = int(num_nodes * 0.2)\n",
    "num_tests = num_nodes - num_trains - num_vals\n",
    "\n",
    "user_ids = np.arange(num_nodes)\n",
    "np.random.shuffle(user_ids)\n",
    "\n",
    "item_ids = np.arange(num_nodes)\n",
    "np.random.shuffle(item_ids)\n",
    "\n",
    "# Train IDs for user.\n",
    "nc_train_user_ids_path = os.path.join(base_dir, \"nc-train-user-ids.npy\")\n",
    "nc_train_user_ids = user_ids[:num_trains]\n",
    "print(f\"Part of train ids[user] for node classification: {nc_train_user_ids[:3]}\")\n",
    "np.save(nc_train_user_ids_path, nc_train_user_ids)\n",
    "print(f\"NC train ids[user] are saved to {nc_train_user_ids_path}\\n\")\n",
    "\n",
    "# Train labels for user.\n",
    "nc_train_user_labels_path = os.path.join(base_dir, \"nc-train-user-labels.pt\")\n",
    "nc_train_user_labels = torch.randint(0, 10, (num_trains,))\n",
    "print(f\"Part of train labels[user] for node classification: {nc_train_user_labels[:3]}\")\n",
    "torch.save(nc_train_user_labels, nc_train_user_labels_path)\n",
    "print(f\"NC train labels[user] are saved to {nc_train_user_labels_path}\\n\")\n",
    "\n",
    "# Train IDs for item.\n",
    "nc_train_item_ids_path = os.path.join(base_dir, \"nc-train-item-ids.npy\")\n",
    "nc_train_item_ids = item_ids[:num_trains]\n",
    "print(f\"Part of train ids[item] for node classification: {nc_train_item_ids[:3]}\")\n",
    "np.save(nc_train_item_ids_path, nc_train_item_ids)\n",
    "print(f\"NC train ids[item] are saved to {nc_train_item_ids_path}\\n\")\n",
    "\n",
    "# Train labels for item.\n",
    "nc_train_item_labels_path = os.path.join(base_dir, \"nc-train-item-labels.pt\")\n",
    "nc_train_item_labels = torch.randint(0, 10, (num_trains,))\n",
    "print(f\"Part of train labels[item] for node classification: {nc_train_item_labels[:3]}\")\n",
    "torch.save(nc_train_item_labels, nc_train_item_labels_path)\n",
    "print(f\"NC train labels[item] are saved to {nc_train_item_labels_path}\\n\")\n",
    "\n",
    "# Val IDs for user.\n",
    "nc_val_user_ids_path = os.path.join(base_dir, \"nc-val-user-ids.npy\")\n",
    "nc_val_user_ids = user_ids[num_trains:num_trains+num_vals]\n",
    "print(f\"Part of val ids[user] for node classification: {nc_val_user_ids[:3]}\")\n",
    "np.save(nc_val_user_ids_path, nc_val_user_ids)\n",
    "print(f\"NC val ids[user] are saved to {nc_val_user_ids_path}\\n\")\n",
    "\n",
    "# Val labels for user.\n",
    "nc_val_user_labels_path = os.path.join(base_dir, \"nc-val-user-labels.pt\")\n",
    "nc_val_user_labels = torch.randint(0, 10, (num_vals,))\n",
    "print(f\"Part of val labels[user] for node classification: {nc_val_user_labels[:3]}\")\n",
    "torch.save(nc_val_user_labels, nc_val_user_labels_path)\n",
    "print(f\"NC val labels[user] are saved to {nc_val_user_labels_path}\\n\")\n",
    "\n",
    "# Val IDs for item.\n",
    "nc_val_item_ids_path = os.path.join(base_dir, \"nc-val-item-ids.npy\")\n",
    "nc_val_item_ids = item_ids[num_trains:num_trains+num_vals]\n",
    "print(f\"Part of val ids[item] for node classification: {nc_val_item_ids[:3]}\")\n",
    "np.save(nc_val_item_ids_path, nc_val_item_ids)\n",
    "print(f\"NC val ids[item] are saved to {nc_val_item_ids_path}\\n\")\n",
    "\n",
    "# Val labels for item.\n",
    "nc_val_item_labels_path = os.path.join(base_dir, \"nc-val-item-labels.pt\")\n",
    "nc_val_item_labels = torch.randint(0, 10, (num_vals,))\n",
    "print(f\"Part of val labels[item] for node classification: {nc_val_item_labels[:3]}\")\n",
    "torch.save(nc_val_item_labels, nc_val_item_labels_path)\n",
    "print(f\"NC val labels[item] are saved to {nc_val_item_labels_path}\\n\")\n",
    "\n",
    "# Test IDs for user.\n",
    "nc_test_user_ids_path = os.path.join(base_dir, \"nc-test-user-ids.npy\")\n",
    "nc_test_user_ids = user_ids[-num_tests:]\n",
    "print(f\"Part of test ids[user] for node classification: {nc_test_user_ids[:3]}\")\n",
    "np.save(nc_test_user_ids_path, nc_test_user_ids)\n",
    "print(f\"NC test ids[user] are saved to {nc_test_user_ids_path}\\n\")\n",
    "\n",
    "# Test labels for user.\n",
    "nc_test_user_labels_path = os.path.join(base_dir, \"nc-test-user-labels.pt\")\n",
    "nc_test_user_labels = torch.randint(0, 10, (num_tests,))\n",
    "print(f\"Part of test labels[user] for node classification: {nc_test_user_labels[:3]}\")\n",
    "torch.save(nc_test_user_labels, nc_test_user_labels_path)\n",
    "print(f\"NC test labels[user] are saved to {nc_test_user_labels_path}\\n\")\n",
    "\n",
    "# Test IDs for item.\n",
    "nc_test_item_ids_path = os.path.join(base_dir, \"nc-test-item-ids.npy\")\n",
    "nc_test_item_ids = item_ids[-num_tests:]\n",
    "print(f\"Part of test ids[item] for node classification: {nc_test_item_ids[:3]}\")\n",
    "np.save(nc_test_item_ids_path, nc_test_item_ids)\n",
    "print(f\"NC test ids[item] are saved to {nc_test_item_ids_path}\\n\")\n",
    "\n",
    "# Test labels for item.\n",
    "nc_test_item_labels_path = os.path.join(base_dir, \"nc-test-item-labels.pt\")\n",
    "nc_test_item_labels = torch.randint(0, 10, (num_tests,))\n",
    "print(f\"Part of test labels[item] for node classification: {nc_test_item_labels[:3]}\")\n",
    "torch.save(nc_test_item_labels, nc_test_item_labels_path)\n",
    "print(f\"NC test labels[item] are saved to {nc_test_item_labels_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhAcDCHQ_KJ0"
   },
   "source": [
    "#### Link Prediction Task\n",
    "For link prediction task, we need **seeds** or **corresponding labels and indexes** which representing the pos/neg property and group of the seeds for each training/validation/test set. Like feature data, numpy arrays and torch tensors are supported for these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T07:27:20.139102Z",
     "iopub.status.busy": "2024-09-19T07:27:20.138727Z",
     "iopub.status.idle": "2024-09-19T07:27:20.170865Z",
     "shell.execute_reply": "2024-09-19T07:27:20.169276Z"
    },
    "id": "u0jCnXIcAQy4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of train seeds[user:like:item] for link prediction: [[530 906]\n",
      " [170 811]\n",
      " [431  23]]\n",
      "LP train seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-train-like-seeds.npy\n",
      "\n",
      "Part of train seeds[user:follow:user] for link prediction: [[795  32]\n",
      " [321 815]\n",
      " [234 831]]\n",
      "LP train seeds[user:follow:user] are saved to ./ondisk_dataset_heterograph/lp-train-follow-seeds.npy\n",
      "\n",
      "Part of val seeds[user:like:item] for link prediction: [[872 565]\n",
      " [670 356]\n",
      " [790 976]]\n",
      "LP val seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-seeds.npy\n",
      "\n",
      "Part of val labels[user:like:item] for link prediction: [1. 1. 1.]\n",
      "LP val labels[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-labels.npy\n",
      "\n",
      "Part of val indexes[user:like:item] for link prediction: [0 1 2]\n",
      "LP val indexes[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-indexes.npy\n",
      "\n",
      "Part of val seeds[user:follow:item] for link prediction: [[602 109]\n",
      " [562 230]\n",
      " [192  14]]\n",
      "LP val seeds[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-seeds.npy\n",
      "\n",
      "Part of val labels[user:follow:item] for link prediction: [1. 1. 1.]\n",
      "LP val labels[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-labels.npy\n",
      "\n",
      "Part of val indexes[user:follow:item] for link prediction: [0 1 2]\n",
      "LP val indexes[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-indexes.npy\n",
      "\n",
      "Part of test seeds[user:like:item] for link prediction: [[894  35]\n",
      " [645 860]\n",
      " [215 708]]\n",
      "LP test seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-seeds.npy\n",
      "\n",
      "Part of test labels[user:like:item] for link prediction: [1. 1. 1.]\n",
      "LP test labels[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-labels.npy\n",
      "\n",
      "Part of test indexes[user:like:item] for link prediction: [0 1 2]\n",
      "LP test indexes[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-indexes.npy\n",
      "\n",
      "Part of test seeds[user:follow:item] for link prediction: [[744 813]\n",
      " [633 913]\n",
      " [888 109]]\n",
      "LP test seeds[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-seeds.npy\n",
      "\n",
      "Part of test labels[user:follow:item] for link prediction: [1. 1. 1.]\n",
      "LP test labels[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-labels.npy\n",
      "\n",
      "Part of test indexes[user:follow:item] for link prediction: [0 1 2]\n",
      "LP test indexes[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-indexes.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration, let's generate item sets for each edge type.\n",
    "num_trains = int(num_edges * 0.6)\n",
    "num_vals = int(num_edges * 0.2)\n",
    "num_tests = num_edges - num_trains - num_vals\n",
    "\n",
    "# Train seeds for user:like:item.\n",
    "lp_train_like_seeds_path = os.path.join(base_dir, \"lp-train-like-seeds.npy\")\n",
    "lp_train_like_seeds = like_edges[:num_trains, :]\n",
    "print(f\"Part of train seeds[user:like:item] for link prediction: {lp_train_like_seeds[:3]}\")\n",
    "np.save(lp_train_like_seeds_path, lp_train_like_seeds)\n",
    "print(f\"LP train seeds[user:like:item] are saved to {lp_train_like_seeds_path}\\n\")\n",
    "\n",
    "# Train seeds for user:follow:user.\n",
    "lp_train_follow_seeds_path = os.path.join(base_dir, \"lp-train-follow-seeds.npy\")\n",
    "lp_train_follow_seeds = follow_edges[:num_trains, :]\n",
    "print(f\"Part of train seeds[user:follow:user] for link prediction: {lp_train_follow_seeds[:3]}\")\n",
    "np.save(lp_train_follow_seeds_path, lp_train_follow_seeds)\n",
    "print(f\"LP train seeds[user:follow:user] are saved to {lp_train_follow_seeds_path}\\n\")\n",
    "\n",
    "# Val seeds for user:like:item.\n",
    "lp_val_like_seeds_path = os.path.join(base_dir, \"lp-val-like-seeds.npy\")\n",
    "lp_val_like_seeds = like_edges[num_trains:num_trains+num_vals, :]\n",
    "lp_val_like_neg_dsts = np.random.randint(0, num_nodes, (num_vals, 10)).reshape(-1)\n",
    "lp_val_like_neg_srcs = np.repeat(lp_val_like_seeds[:,0], 10)\n",
    "lp_val_like_neg_seeds = np.concatenate((lp_val_like_neg_srcs, lp_val_like_neg_dsts)).reshape(2,-1).T\n",
    "lp_val_like_seeds = np.concatenate((lp_val_like_seeds, lp_val_like_neg_seeds))\n",
    "print(f\"Part of val seeds[user:like:item] for link prediction: {lp_val_like_seeds[:3]}\")\n",
    "np.save(lp_val_like_seeds_path, lp_val_like_seeds)\n",
    "print(f\"LP val seeds[user:like:item] are saved to {lp_val_like_seeds_path}\\n\")\n",
    "\n",
    "# Val labels for user:like:item.\n",
    "lp_val_like_labels_path = os.path.join(base_dir, \"lp-val-like-labels.npy\")\n",
    "lp_val_like_labels = np.empty(num_vals * (10 + 1))\n",
    "lp_val_like_labels[:num_vals] = 1\n",
    "lp_val_like_labels[num_vals:] = 0\n",
    "print(f\"Part of val labels[user:like:item] for link prediction: {lp_val_like_labels[:3]}\")\n",
    "np.save(lp_val_like_labels_path, lp_val_like_labels)\n",
    "print(f\"LP val labels[user:like:item] are saved to {lp_val_like_labels_path}\\n\")\n",
    "\n",
    "# Val indexes for user:like:item.\n",
    "lp_val_like_indexes_path = os.path.join(base_dir, \"lp-val-like-indexes.npy\")\n",
    "lp_val_like_indexes = np.arange(0, num_vals)\n",
    "lp_val_like_neg_indexes = np.repeat(lp_val_like_indexes, 10)\n",
    "lp_val_like_indexes = np.concatenate([lp_val_like_indexes, lp_val_like_neg_indexes])\n",
    "print(f\"Part of val indexes[user:like:item] for link prediction: {lp_val_like_indexes[:3]}\")\n",
    "np.save(lp_val_like_indexes_path, lp_val_like_indexes)\n",
    "print(f\"LP val indexes[user:like:item] are saved to {lp_val_like_indexes_path}\\n\")\n",
    "\n",
    "# Val seeds for user:follow:item.\n",
    "lp_val_follow_seeds_path = os.path.join(base_dir, \"lp-val-follow-seeds.npy\")\n",
    "lp_val_follow_seeds = follow_edges[num_trains:num_trains+num_vals, :]\n",
    "lp_val_follow_neg_dsts = np.random.randint(0, num_nodes, (num_vals, 10)).reshape(-1)\n",
    "lp_val_follow_neg_srcs = np.repeat(lp_val_follow_seeds[:,0], 10)\n",
    "lp_val_follow_neg_seeds = np.concatenate((lp_val_follow_neg_srcs, lp_val_follow_neg_dsts)).reshape(2,-1).T\n",
    "lp_val_follow_seeds = np.concatenate((lp_val_follow_seeds, lp_val_follow_neg_seeds))\n",
    "print(f\"Part of val seeds[user:follow:item] for link prediction: {lp_val_follow_seeds[:3]}\")\n",
    "np.save(lp_val_follow_seeds_path, lp_val_follow_seeds)\n",
    "print(f\"LP val seeds[user:follow:item] are saved to {lp_val_follow_seeds_path}\\n\")\n",
    "\n",
    "# Val labels for user:follow:item.\n",
    "lp_val_follow_labels_path = os.path.join(base_dir, \"lp-val-follow-labels.npy\")\n",
    "lp_val_follow_labels = np.empty(num_vals * (10 + 1))\n",
    "lp_val_follow_labels[:num_vals] = 1\n",
    "lp_val_follow_labels[num_vals:] = 0\n",
    "print(f\"Part of val labels[user:follow:item] for link prediction: {lp_val_follow_labels[:3]}\")\n",
    "np.save(lp_val_follow_labels_path, lp_val_follow_labels)\n",
    "print(f\"LP val labels[user:follow:item] are saved to {lp_val_follow_labels_path}\\n\")\n",
    "\n",
    "# Val indexes for user:follow:item.\n",
    "lp_val_follow_indexes_path = os.path.join(base_dir, \"lp-val-follow-indexes.npy\")\n",
    "lp_val_follow_indexes = np.arange(0, num_vals)\n",
    "lp_val_follow_neg_indexes = np.repeat(lp_val_follow_indexes, 10)\n",
    "lp_val_follow_indexes = np.concatenate([lp_val_follow_indexes, lp_val_follow_neg_indexes])\n",
    "print(f\"Part of val indexes[user:follow:item] for link prediction: {lp_val_follow_indexes[:3]}\")\n",
    "np.save(lp_val_follow_indexes_path, lp_val_follow_indexes)\n",
    "print(f\"LP val indexes[user:follow:item] are saved to {lp_val_follow_indexes_path}\\n\")\n",
    "\n",
    "# Test seeds for user:like:item.\n",
    "lp_test_like_seeds_path = os.path.join(base_dir, \"lp-test-like-seeds.npy\")\n",
    "lp_test_like_seeds = like_edges[-num_tests:, :]\n",
    "lp_test_like_neg_dsts = np.random.randint(0, num_nodes, (num_tests, 10)).reshape(-1)\n",
    "lp_test_like_neg_srcs = np.repeat(lp_test_like_seeds[:,0], 10)\n",
    "lp_test_like_neg_seeds = np.concatenate((lp_test_like_neg_srcs, lp_test_like_neg_dsts)).reshape(2,-1).T\n",
    "lp_test_like_seeds = np.concatenate((lp_test_like_seeds, lp_test_like_neg_seeds))\n",
    "print(f\"Part of test seeds[user:like:item] for link prediction: {lp_test_like_seeds[:3]}\")\n",
    "np.save(lp_test_like_seeds_path, lp_test_like_seeds)\n",
    "print(f\"LP test seeds[user:like:item] are saved to {lp_test_like_seeds_path}\\n\")\n",
    "\n",
    "# Test labels for user:like:item.\n",
    "lp_test_like_labels_path = os.path.join(base_dir, \"lp-test-like-labels.npy\")\n",
    "lp_test_like_labels = np.empty(num_tests * (10 + 1))\n",
    "lp_test_like_labels[:num_tests] = 1\n",
    "lp_test_like_labels[num_tests:] = 0\n",
    "print(f\"Part of test labels[user:like:item] for link prediction: {lp_test_like_labels[:3]}\")\n",
    "np.save(lp_test_like_labels_path, lp_test_like_labels)\n",
    "print(f\"LP test labels[user:like:item] are saved to {lp_test_like_labels_path}\\n\")\n",
    "\n",
    "# Test indexes for user:like:item.\n",
    "lp_test_like_indexes_path = os.path.join(base_dir, \"lp-test-like-indexes.npy\")\n",
    "lp_test_like_indexes = np.arange(0, num_tests)\n",
    "lp_test_like_neg_indexes = np.repeat(lp_test_like_indexes, 10)\n",
    "lp_test_like_indexes = np.concatenate([lp_test_like_indexes, lp_test_like_neg_indexes])\n",
    "print(f\"Part of test indexes[user:like:item] for link prediction: {lp_test_like_indexes[:3]}\")\n",
    "np.save(lp_test_like_indexes_path, lp_test_like_indexes)\n",
    "print(f\"LP test indexes[user:like:item] are saved to {lp_test_like_indexes_path}\\n\")\n",
    "\n",
    "# Test seeds for user:follow:item.\n",
    "lp_test_follow_seeds_path = os.path.join(base_dir, \"lp-test-follow-seeds.npy\")\n",
    "lp_test_follow_seeds = follow_edges[-num_tests:, :]\n",
    "lp_test_follow_neg_dsts = np.random.randint(0, num_nodes, (num_tests, 10)).reshape(-1)\n",
    "lp_test_follow_neg_srcs = np.repeat(lp_test_follow_seeds[:,0], 10)\n",
    "lp_test_follow_neg_seeds = np.concatenate((lp_test_follow_neg_srcs, lp_test_follow_neg_dsts)).reshape(2,-1).T\n",
    "lp_test_follow_seeds = np.concatenate((lp_test_follow_seeds, lp_test_follow_neg_seeds))\n",
    "print(f\"Part of test seeds[user:follow:item] for link prediction: {lp_test_follow_seeds[:3]}\")\n",
    "np.save(lp_test_follow_seeds_path, lp_test_follow_seeds)\n",
    "print(f\"LP test seeds[user:follow:item] are saved to {lp_test_follow_seeds_path}\\n\")\n",
    "\n",
    "# Test labels for user:follow:item.\n",
    "lp_test_follow_labels_path = os.path.join(base_dir, \"lp-test-follow-labels.npy\")\n",
    "lp_test_follow_labels = np.empty(num_tests * (10 + 1))\n",
    "lp_test_follow_labels[:num_tests] = 1\n",
    "lp_test_follow_labels[num_tests:] = 0\n",
    "print(f\"Part of test labels[user:follow:item] for link prediction: {lp_test_follow_labels[:3]}\")\n",
    "np.save(lp_test_follow_labels_path, lp_test_follow_labels)\n",
    "print(f\"LP test labels[user:follow:item] are saved to {lp_test_follow_labels_path}\\n\")\n",
    "\n",
    "# Test indexes for user:follow:item.\n",
    "lp_test_follow_indexes_path = os.path.join(base_dir, \"lp-test-follow-indexes.npy\")\n",
    "lp_test_follow_indexes = np.arange(0, num_tests)\n",
    "lp_test_follow_neg_indexes = np.repeat(lp_test_follow_indexes, 10)\n",
    "lp_test_follow_indexes = np.concatenate([lp_test_follow_indexes, lp_test_follow_neg_indexes])\n",
    "print(f\"Part of test indexes[user:follow:item] for link prediction: {lp_test_follow_indexes[:3]}\")\n",
    "np.save(lp_test_follow_indexes_path, lp_test_follow_indexes)\n",
    "print(f\"LP test indexes[user:follow:item] are saved to {lp_test_follow_indexes_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbk6-wxRK-6S"
   },
   "source": [
    "## Organize Data into YAML File\n",
    "Now we need to create a `metadata.yaml` file which contains the paths, dadta types of graph structure, feature data, training/validation/test sets. Please note that all path should be relative to `metadata.yaml`.\n",
    "\n",
    "For heterogeneous graph, we need to specify the node/edge type in **type** fields. For edge type, canonical etype is required which is a string that's concatenated by source node type, etype, and destination node type together with `:`.\n",
    "\n",
    "Notes:\n",
    "- all path should be relative to `metadata.yaml`.\n",
    "- Below fields are optional and not specified in below example.\n",
    "  - `in_memory`: indicates whether to load dada into memory or `mmap`. Default is `True`.\n",
    "\n",
    "Please refer to [YAML specification](https://github.com/dmlc/dgl/blob/master/docs/source/stochastic_training/ondisk-dataset-specification.rst) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T07:27:20.172988Z",
     "iopub.status.busy": "2024-09-19T07:27:20.172694Z",
     "iopub.status.idle": "2024-09-19T07:27:20.180461Z",
     "shell.execute_reply": "2024-09-19T07:27:20.179922Z"
    },
    "id": "ddGTWW61Lpwp"
   },
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "    dataset_name: heterogeneous_graph_nc_lp\n",
    "    graph:\n",
    "      nodes:\n",
    "        - type: user\n",
    "          num: {num_nodes}\n",
    "        - type: item\n",
    "          num: {num_nodes}\n",
    "      edges:\n",
    "        - type: \"user:like:item\"\n",
    "          format: csv\n",
    "          path: {os.path.basename(like_edges_path)}\n",
    "        - type: \"user:follow:user\"\n",
    "          format: csv\n",
    "          path: {os.path.basename(follow_edges_path)}\n",
    "    feature_data:\n",
    "      - domain: node\n",
    "        type: user\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(node_user_feat_0_path)}\n",
    "      - domain: node\n",
    "        type: user\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(node_user_feat_1_path)}\n",
    "      - domain: node\n",
    "        type: item\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(node_item_feat_0_path)}\n",
    "      - domain: node\n",
    "        type: item\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(node_item_feat_1_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:like:item\"\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(edge_like_feat_0_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:like:item\"\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(edge_like_feat_1_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:follow:user\"\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(edge_follow_feat_0_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:follow:user\"\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(edge_follow_feat_1_path)}\n",
    "    tasks:\n",
    "      - name: node_classification\n",
    "        num_classes: 10\n",
    "        train_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_train_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_train_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_train_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_train_item_labels_path)}\n",
    "        validation_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_val_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_val_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_val_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_val_item_labels_path)}\n",
    "        test_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_test_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_test_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_test_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_test_item_labels_path)}\n",
    "      - name: link_prediction\n",
    "        num_classes: 10\n",
    "        train_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_train_like_seeds_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_train_follow_seeds_path)}\n",
    "        validation_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_indexes_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_indexes_path)}\n",
    "        test_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_indexes_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_indexes_path)}\n",
    "\"\"\"\n",
    "metadata_path = os.path.join(base_dir, \"metadata.yaml\")\n",
    "with open(metadata_path, \"w\") as f:\n",
    "  f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEfybHGhOW7O"
   },
   "source": [
    "## Instantiate `OnDiskDataset`\n",
    "Now we're ready to load dataset via `dgl.graphbolt.OnDiskDataset`. When instantiating, we just pass in the base directory where `metadata.yaml` file lies.\n",
    "\n",
    "During first instantiation, GraphBolt preprocesses the raw data such as constructing `FusedCSCSamplingGraph` from edges. All data including graph, feature data, training/validation/test sets are put into `preprocessed` directory after preprocessing. Any following dataset loading will skip the preprocess stage.\n",
    "\n",
    "After preprocessing, `load()` is required to be called explicitly in order to load graph, feature data and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T07:27:20.182243Z",
     "iopub.status.busy": "2024-09-19T07:27:20.181952Z",
     "iopub.status.idle": "2024-09-19T07:27:20.297470Z",
     "shell.execute_reply": "2024-09-19T07:27:20.296721Z"
    },
    "id": "W58CZoSzOiyo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to preprocess the on-disk dataset.\n",
      "Finish preprocessing the on-disk dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph: FusedCSCSamplingGraph(csc_indptr=tensor([    0,    11,    22,  ..., 19986, 19994, 20000], dtype=torch.int32),\n",
      "                      indices=tensor([1356, 1137, 1830,  ..., 1158, 1905, 1089], dtype=torch.int32),\n",
      "                      total_num_nodes=2000, num_edges={'user:follow:user': 10000, 'user:like:item': 10000},\n",
      "                      node_type_offset=tensor([   0, 1000, 2000], dtype=torch.int32),\n",
      "                      type_per_edge=tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.uint8),\n",
      "                      node_type_to_id={'item': 0, 'user': 1},\n",
      "                      edge_type_to_id={'user:follow:user': 0, 'user:like:item': 1},)\n",
      "\n",
      "Loaded feature store: TorchBasedFeatureStore(\n",
      "    {(<OnDiskFeatureDataDomain.NODE: 'node'>, 'user', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.3236, 0.0561, 0.2640, 0.2404, 0.8045],\n",
      "                        [0.8186, 0.2420, 0.8004, 0.8394, 0.1943],\n",
      "                        [0.6200, 0.4846, 0.5895, 0.1996, 0.8686],\n",
      "                        ...,\n",
      "                        [0.3297, 0.6982, 0.6987, 0.4993, 0.8864],\n",
      "                        [0.7753, 0.5101, 0.3836, 0.8304, 0.3071],\n",
      "                        [0.1668, 0.5764, 0.6507, 0.5483, 0.5107]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'user', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.6393, 0.2935, 0.7817, 0.5309, 0.8513],\n",
      "                        [0.3523, 0.5367, 0.0106, 0.2039, 0.9900],\n",
      "                        [0.3833, 0.6105, 0.5327, 0.4471, 0.9685],\n",
      "                        ...,\n",
      "                        [0.0330, 0.6352, 0.1997, 0.2262, 0.7156],\n",
      "                        [0.4925, 0.2611, 0.8147, 0.4416, 0.9332],\n",
      "                        [0.7774, 0.3144, 0.1833, 0.6757, 0.2252]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'item', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.8619, 0.0507, 0.3387, 0.2379, 0.5989],\n",
      "                        [0.4884, 0.6503, 0.0942, 0.7597, 0.0616],\n",
      "                        [0.4402, 0.4223, 0.0130, 0.4595, 0.3292],\n",
      "                        ...,\n",
      "                        [0.0829, 0.3431, 0.2289, 0.1476, 0.8983],\n",
      "                        [0.8712, 0.7179, 0.1777, 0.9661, 0.3139],\n",
      "                        [0.3360, 0.1619, 0.2104, 0.3877, 0.2766]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'item', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.9045, 0.9076, 0.9799, 0.9640, 0.6546],\n",
      "                        [0.5709, 0.2063, 0.0226, 0.0567, 0.5681],\n",
      "                        [0.7979, 0.0573, 0.9011, 0.9951, 0.3689],\n",
      "                        ...,\n",
      "                        [0.6491, 0.2993, 0.2206, 0.3068, 0.8981],\n",
      "                        [0.8792, 0.5996, 0.0042, 0.1445, 0.6354],\n",
      "                        [0.6899, 0.7106, 0.1138, 0.7694, 0.3017]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:like:item', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.6461, 0.9304, 0.9430, 0.2551, 0.2125],\n",
      "                        [0.1549, 0.1477, 0.5306, 0.3570, 0.4878],\n",
      "                        [0.1009, 0.7969, 0.1192, 0.2900, 0.9927],\n",
      "                        ...,\n",
      "                        [0.1874, 0.0635, 0.9738, 0.2024, 0.7063],\n",
      "                        [0.7126, 0.5691, 0.7745, 0.1108, 0.1631],\n",
      "                        [0.5836, 0.0769, 0.2437, 0.3562, 0.7614]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:like:item', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.5407, 0.4052, 0.4030, 0.6136, 0.9814],\n",
      "                        [0.5502, 0.5136, 0.0462, 0.2717, 0.0953],\n",
      "                        [0.2576, 0.8391, 0.3014, 0.7970, 0.8010],\n",
      "                        ...,\n",
      "                        [0.3711, 0.0317, 0.9801, 0.9050, 0.8651],\n",
      "                        [0.9737, 0.9631, 0.3508, 0.2226, 0.2762],\n",
      "                        [0.4833, 0.9166, 0.2450, 0.2580, 0.2245]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:follow:user', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.9932, 0.9464, 0.5317, 0.9418, 0.5100],\n",
      "                        [0.4241, 0.3003, 0.1060, 0.2749, 0.1474],\n",
      "                        [0.7954, 0.9817, 0.0972, 0.2647, 0.5790],\n",
      "                        ...,\n",
      "                        [0.6275, 0.8007, 0.4111, 0.4215, 0.4808],\n",
      "                        [0.1117, 0.6800, 0.6369, 0.7934, 0.3033],\n",
      "                        [0.5999, 0.4349, 0.1407, 0.5477, 0.3746]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:follow:user', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.2728, 0.6300, 0.0400, 0.7417, 0.0648],\n",
      "                        [0.6277, 0.0462, 0.5728, 0.8218, 0.3954],\n",
      "                        [0.2750, 0.3536, 0.1480, 0.3750, 0.0895],\n",
      "                        ...,\n",
      "                        [0.9502, 0.2407, 0.3350, 0.8940, 0.7721],\n",
      "                        [0.8667, 0.0798, 0.4495, 0.9612, 0.3544],\n",
      "                        [0.6603, 0.1980, 0.8548, 0.3670, 0.0434]]),\n",
      "        metadata={},\n",
      "    )}\n",
      ")\n",
      "\n",
      "Loaded node classification task: OnDiskTask(validation_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([796, 542, 969, 365, 159, 441, 493, 415, 149, 530, 880, 461, 708, 556,\n",
      "                                 72, 866, 669, 668, 846, 672, 579, 884, 809, 607,  42, 814, 725, 114,\n",
      "                                550, 419, 585,  59,  50,   3, 373, 401,  38, 382, 557, 979, 905, 871,\n",
      "                                500, 952, 724, 148, 683, 193, 587, 357, 485, 863,  74, 111, 255, 791,\n",
      "                                667, 558, 286, 512, 689, 746,  84, 739, 570, 417, 423, 950, 699, 536,\n",
      "                                263, 100,  23, 862, 563, 782, 326, 406,  30,  71, 118,  26, 943, 386,\n",
      "                                682, 158, 716, 945, 989, 873,  37, 743, 673, 117, 964, 686, 935, 411,\n",
      "                                497, 626, 328, 653, 502, 645, 549, 555,   8, 295, 953,  97, 812, 843,\n",
      "                                398, 487, 571, 981, 233, 624, 157, 302, 894, 397, 612, 914, 450, 755,\n",
      "                                619, 469, 874, 709, 718, 934,  22, 381, 353, 641, 845, 291, 749, 937,\n",
      "                                 19,  36, 861, 799,  76,  75, 854, 345, 577,  52, 231, 471, 393, 652,\n",
      "                                936, 137, 183, 643, 474,  99, 480, 907, 972, 997, 760, 531, 705, 252,\n",
      "                                 82,  69, 747, 721, 697, 269, 878,   4, 630, 275, 259, 685, 242, 711,\n",
      "                                 90, 232, 877,  17, 771, 429, 383, 199, 351, 360, 449, 658, 370,  40,\n",
      "                                963, 687, 352, 659], dtype=torch.int32), tensor([4, 9, 5, 2, 5, 6, 6, 7, 7, 9, 5, 4, 4, 9, 9, 8, 2, 5, 7, 9, 8, 3, 3, 2,\n",
      "                                5, 5, 5, 3, 5, 6, 8, 3, 4, 9, 8, 3, 9, 6, 5, 1, 6, 3, 5, 7, 3, 8, 6, 0,\n",
      "                                7, 0, 9, 0, 2, 8, 0, 4, 1, 0, 5, 5, 5, 8, 8, 4, 6, 3, 7, 2, 4, 3, 8, 8,\n",
      "                                5, 6, 9, 2, 2, 7, 5, 3, 2, 7, 3, 3, 2, 7, 8, 4, 1, 2, 9, 9, 2, 0, 6, 1,\n",
      "                                3, 9, 6, 5, 4, 0, 7, 8, 4, 8, 9, 8, 4, 9, 4, 4, 4, 5, 4, 3, 3, 9, 9, 3,\n",
      "                                1, 6, 0, 3, 4, 5, 8, 3, 5, 5, 6, 2, 6, 6, 8, 9, 7, 6, 4, 1, 6, 2, 4, 2,\n",
      "                                2, 5, 4, 3, 0, 1, 4, 4, 4, 1, 3, 2, 9, 1, 7, 8, 0, 5, 5, 2, 1, 5, 1, 8,\n",
      "                                6, 0, 7, 8, 8, 1, 7, 4, 0, 0, 2, 2, 0, 8, 0, 0, 0, 6, 9, 9, 7, 1, 3, 4,\n",
      "                                6, 5, 2, 8, 0, 1, 8, 8])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([725, 836, 168, 735, 298, 878, 898, 529, 159, 150, 801, 340, 305, 756,\n",
      "                                443, 363, 537, 746, 910, 118, 934, 861, 711, 792, 367, 356, 723,  92,\n",
      "                                331, 121, 926, 378, 243, 824, 278, 454, 781, 981, 913, 487, 223, 888,\n",
      "                                510, 502,  99, 871, 107, 461, 389, 111, 214, 202, 405, 822, 989, 543,\n",
      "                                776,  55, 700, 332, 768, 946, 803,  44, 555, 500, 547, 432,  38, 949,\n",
      "                                582, 337, 207, 528, 777, 471, 293,  23, 577, 274, 437, 699, 540, 164,\n",
      "                                570, 800, 235, 417, 958, 252,   8, 999, 854, 176, 275, 692, 515, 813,\n",
      "                                448, 714, 560, 839, 377, 128, 233, 589, 239, 660, 971, 682,  53, 290,\n",
      "                                338, 123,   2,  64, 716, 952, 993, 965, 655,  37, 407, 154, 163, 397,\n",
      "                                 56, 449, 610, 834, 843, 640, 400, 940, 276, 345, 380,  81, 957,  32,\n",
      "                                848, 605, 909, 103, 206, 986, 470,  15, 224, 720, 826, 733,  19, 249,\n",
      "                                748, 379, 587, 484,  59,   0, 874, 486, 920, 998, 404, 503,  40, 551,\n",
      "                                418, 104,   3, 632, 919, 376, 598, 659, 818, 122, 766,  95, 267, 391,\n",
      "                                687, 779, 703, 747, 835, 649, 778, 180, 621, 179, 618, 516,  85,  21,\n",
      "                                109, 974, 672, 152], dtype=torch.int32), tensor([9, 6, 5, 3, 8, 8, 6, 6, 8, 3, 8, 9, 9, 2, 2, 1, 6, 9, 5, 3, 3, 8, 8, 5,\n",
      "                                8, 5, 3, 7, 6, 0, 9, 1, 9, 9, 7, 0, 6, 5, 0, 5, 3, 6, 5, 6, 9, 5, 6, 5,\n",
      "                                3, 8, 1, 4, 9, 5, 7, 5, 5, 0, 8, 5, 1, 4, 7, 7, 8, 9, 3, 6, 6, 5, 3, 8,\n",
      "                                5, 3, 6, 7, 2, 0, 5, 7, 4, 0, 1, 8, 6, 8, 4, 0, 2, 2, 8, 8, 3, 7, 5, 1,\n",
      "                                0, 0, 1, 7, 9, 3, 6, 5, 4, 0, 1, 2, 4, 6, 0, 2, 5, 0, 8, 2, 5, 5, 6, 8,\n",
      "                                9, 4, 0, 2, 1, 4, 5, 0, 9, 9, 7, 9, 9, 9, 9, 5, 0, 6, 9, 5, 1, 6, 4, 1,\n",
      "                                7, 5, 5, 8, 5, 7, 3, 0, 0, 6, 1, 9, 1, 7, 6, 7, 1, 4, 8, 0, 6, 9, 0, 3,\n",
      "                                8, 7, 3, 2, 1, 8, 4, 4, 4, 5, 1, 6, 4, 3, 2, 3, 8, 2, 8, 9, 6, 8, 7, 2,\n",
      "                                3, 3, 2, 2, 0, 8, 0, 0])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           train_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([859, 916, 628, 241,  46, 606, 513, 377, 662,  77, 738, 306, 372, 629,\n",
      "                                594,  87, 852, 297,  27, 574, 694, 970, 767, 616, 318, 961, 560, 499,\n",
      "                                127,  13, 876, 510, 414, 798, 677, 817, 664, 251, 591, 520, 163, 569,\n",
      "                                203, 620, 296,  49, 276, 396, 853, 713, 151, 292, 186, 190, 478, 196,\n",
      "                                321, 548,  58, 488, 422, 362, 438, 847, 614, 348, 816, 820, 123, 889,\n",
      "                                539, 217, 828, 968, 752, 763, 517, 593, 769, 319, 339, 407, 947, 813,\n",
      "                                405, 691, 267,  44, 766,  51, 192, 106, 482, 960, 982, 967, 559, 578,\n",
      "                                309, 592, 245, 204, 261, 128, 933, 340, 544, 279, 973, 778, 971, 526,\n",
      "                                883, 748, 473, 803, 367, 138, 776, 439, 288, 891, 385, 784, 317, 910,\n",
      "                                464,  86, 590,  64, 388, 274, 304,  57,  18, 177, 608, 632, 224,  39,\n",
      "                                665, 807, 432, 313, 977, 600, 834, 225, 431, 136, 695, 840, 491, 838,\n",
      "                                 93, 552, 928, 646, 787, 248, 540, 202, 509, 336, 307, 333, 855, 350,\n",
      "                                704, 647, 958, 456, 408, 575, 152, 733, 974, 226, 475, 995, 736, 254,\n",
      "                                675, 582, 395, 200, 215, 344, 545,   0, 205, 460, 529, 993, 112,  47,\n",
      "                                277, 220, 707, 872, 712,  70, 929, 337, 801, 568, 143, 882, 639, 634,\n",
      "                                 34,  95, 404, 129,  61, 917, 257, 425, 179, 427, 463, 294, 120, 289,\n",
      "                                654, 376, 793, 851,  29, 830, 996, 625, 494, 428, 490, 167, 228,  41,\n",
      "                                284, 361, 572, 369, 627, 153, 523, 206, 618, 521, 692, 124, 486,  20,\n",
      "                                130, 783, 790, 514, 403, 940, 770, 990, 214, 920, 416,  88, 201, 553,\n",
      "                                886, 147, 141, 583, 666, 932, 759, 911,   7, 524, 906, 976, 316, 447,\n",
      "                                371, 992,  83, 327, 835, 533, 765, 364, 312, 280, 496, 273, 734, 104,\n",
      "                                941,  35, 888, 308, 849, 283, 868, 703, 921, 253, 445, 679, 946, 821,\n",
      "                                735, 644, 602, 985, 604, 421, 366, 506, 898, 271, 693, 825, 636, 239,\n",
      "                                476, 804, 243, 330, 465, 409, 902, 516, 171, 125, 430, 170, 656, 671,\n",
      "                                156,  14, 221, 810, 655, 547, 897, 150, 753, 185, 827, 311, 565, 919,\n",
      "                                839, 433, 865, 528, 265, 184, 589, 660, 468, 823, 234, 379, 795, 842,\n",
      "                                182, 426, 459, 741, 786, 768, 535,  54, 443, 756, 278, 300, 732, 637,\n",
      "                                470, 815,  63, 532, 881, 180, 424, 250, 236, 258, 730, 264, 551,  24,\n",
      "                                238, 710, 301, 462, 554, 856, 832, 818, 116, 347, 858,  68, 648, 580,\n",
      "                                864, 661, 598, 546, 375, 534, 391, 831, 119, 412, 727, 714, 505, 684,\n",
      "                                246, 811, 107, 681, 794, 293, 162, 142, 413, 323, 956, 611, 957, 631,\n",
      "                                 15, 197, 978, 299, 939, 999, 324,  43, 700,  96, 962, 781, 651,  32,\n",
      "                                 91, 762, 244,  81, 893, 808, 678, 720, 603, 844, 931, 519, 418, 775,\n",
      "                                588, 913, 581, 638, 314, 477, 195, 342, 322, 744,   1, 249, 213, 210,\n",
      "                                621, 446, 633, 584, 454, 904, 188, 161, 757, 731, 194, 181, 869,  60,\n",
      "                                538, 209, 389, 132, 745, 511, 688, 955, 980, 103, 285, 826, 444, 663,\n",
      "                                102, 537, 562, 146, 145, 613, 341, 601, 750, 363, 908, 706, 789, 649,\n",
      "                                457, 623, 110, 334, 737, 467, 903,  33, 325,  53, 268, 983, 155, 187,\n",
      "                                355, 779,  80, 139, 515, 833, 218, 717, 527, 922, 229,  62, 121, 674,\n",
      "                                605, 189, 723, 466, 802, 690, 927,  85, 247, 256, 230, 266,  65, 841,\n",
      "                                237, 399, 829, 715, 287,  66, 522, 359, 165, 951,   9, 495, 925, 991,\n",
      "                                698, 298, 498, 303, 131, 696, 650,  94, 642,  25, 792, 178,  98, 610,\n",
      "                                525, 173, 895, 507,  11, 207, 800,  12, 380, 455,  79, 472],\n",
      "                               dtype=torch.int32), tensor([8, 6, 5, 4, 9, 2, 2, 6, 0, 7, 7, 2, 5, 4, 3, 7, 5, 7, 7, 0, 2, 0, 5, 1,\n",
      "                                2, 1, 4, 4, 9, 3, 2, 8, 4, 2, 4, 8, 1, 7, 0, 8, 5, 3, 9, 5, 8, 7, 8, 5,\n",
      "                                3, 0, 3, 0, 2, 6, 5, 4, 3, 0, 4, 5, 6, 8, 6, 8, 9, 4, 3, 0, 3, 3, 3, 6,\n",
      "                                6, 0, 0, 3, 1, 6, 5, 2, 3, 2, 4, 0, 7, 7, 1, 7, 9, 7, 8, 7, 4, 2, 5, 0,\n",
      "                                3, 6, 5, 9, 7, 9, 1, 1, 0, 8, 8, 1, 4, 8, 2, 8, 7, 6, 5, 0, 6, 6, 5, 0,\n",
      "                                0, 5, 1, 4, 9, 1, 5, 2, 0, 6, 3, 7, 1, 3, 8, 1, 4, 8, 3, 4, 3, 3, 0, 4,\n",
      "                                8, 9, 0, 4, 4, 3, 1, 6, 1, 9, 8, 9, 7, 2, 1, 7, 0, 2, 6, 0, 8, 4, 3, 7,\n",
      "                                3, 1, 5, 9, 3, 6, 4, 0, 5, 9, 1, 6, 7, 6, 3, 1, 1, 0, 0, 3, 3, 2, 8, 4,\n",
      "                                2, 3, 3, 6, 9, 8, 6, 5, 8, 1, 3, 7, 3, 5, 1, 7, 2, 1, 4, 8, 0, 7, 2, 6,\n",
      "                                2, 6, 5, 0, 2, 1, 1, 7, 2, 4, 4, 9, 6, 2, 1, 1, 9, 4, 3, 2, 4, 0, 9, 6,\n",
      "                                3, 5, 2, 9, 2, 7, 7, 8, 1, 9, 3, 3, 0, 5, 2, 7, 7, 8, 4, 0, 9, 4, 8, 5,\n",
      "                                8, 7, 0, 2, 8, 8, 4, 5, 3, 5, 9, 9, 5, 2, 8, 0, 8, 0, 8, 9, 9, 9, 8, 7,\n",
      "                                7, 0, 8, 0, 9, 6, 1, 7, 5, 7, 3, 8, 8, 4, 1, 8, 8, 8, 2, 6, 0, 0, 4, 3,\n",
      "                                9, 2, 0, 2, 6, 6, 3, 5, 4, 5, 1, 8, 2, 5, 6, 4, 3, 8, 6, 8, 2, 0, 3, 4,\n",
      "                                1, 2, 0, 7, 7, 0, 2, 6, 9, 4, 2, 8, 5, 0, 2, 3, 5, 1, 1, 0, 7, 9, 0, 2,\n",
      "                                6, 7, 9, 3, 4, 8, 0, 5, 4, 5, 2, 5, 2, 8, 4, 0, 9, 3, 9, 5, 1, 9, 0, 9,\n",
      "                                6, 9, 1, 6, 1, 8, 0, 8, 6, 0, 7, 2, 9, 1, 1, 8, 2, 4, 8, 5, 3, 5, 0, 7,\n",
      "                                8, 9, 5, 4, 5, 6, 1, 8, 3, 2, 8, 2, 6, 9, 7, 8, 4, 6, 2, 4, 6, 1, 4, 8,\n",
      "                                0, 0, 1, 1, 9, 5, 3, 6, 1, 2, 6, 1, 1, 2, 8, 9, 3, 0, 4, 8, 3, 8, 5, 2,\n",
      "                                8, 5, 3, 5, 2, 0, 0, 0, 8, 8, 4, 1, 2, 5, 1, 1, 5, 8, 3, 6, 5, 7, 2, 0,\n",
      "                                1, 6, 6, 9, 4, 1, 6, 3, 5, 9, 9, 8, 6, 6, 5, 1, 6, 5, 5, 3, 5, 8, 1, 1,\n",
      "                                2, 1, 9, 8, 6, 4, 1, 7, 7, 8, 8, 4, 8, 0, 5, 2, 9, 7, 9, 6, 6, 5, 7, 5,\n",
      "                                9, 9, 9, 4, 0, 9, 1, 2, 8, 9, 7, 0, 0, 0, 3, 9, 1, 2, 5, 3, 5, 8, 3, 0,\n",
      "                                0, 3, 7, 0, 6, 3, 7, 4, 7, 8, 5, 8, 5, 1, 9, 3, 6, 8, 0, 2, 3, 8, 4, 5,\n",
      "                                0, 0, 6, 0, 2, 9, 6, 2, 1, 7, 8, 3, 7, 8, 2, 7, 6, 9, 0, 8, 4, 0, 4, 7])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([645, 862, 976, 880, 602,  94, 254, 629,  67, 799, 761,  62, 319, 369,\n",
      "                                631, 961, 937, 133, 728, 100,  86, 908, 316, 349, 317, 365, 856, 701,\n",
      "                                175, 810, 562,  68, 994, 424,  70, 157, 907, 110, 395,  39, 445,  54,\n",
      "                                534, 825, 603, 842, 284, 576, 807, 281, 326, 925,   4, 791, 641, 886,\n",
      "                                177, 513, 571, 416, 867, 269, 705, 996, 771, 978, 335, 967, 702, 475,\n",
      "                                729, 426, 220, 309,  79,  13, 514, 295,  17, 260, 245, 709, 132, 301,\n",
      "                                 34, 775, 257, 146, 707, 422, 647, 114, 401, 359, 879, 366, 183, 204,\n",
      "                                857, 644, 156, 438, 230, 600, 959, 373, 218,  60, 827,  27,  22, 876,\n",
      "                                194, 609, 147, 464, 737, 944, 439, 794, 694, 755, 409, 188, 588, 619,\n",
      "                                458, 592, 642,  35, 533, 627, 554, 948, 280, 237, 763, 182, 266, 760,\n",
      "                                 24, 657, 139, 238, 678, 466, 706, 313, 750,  18, 774, 144, 321, 790,\n",
      "                                  7, 988, 808, 310, 696, 505, 883,  71, 754, 403, 221, 255, 544,  69,\n",
      "                                838, 601, 140, 594, 770, 225, 413, 821,  77,  65, 738, 304, 658, 217,\n",
      "                                 20, 935, 677, 762, 261, 459, 798, 135, 538, 483, 530,  28, 722, 219,\n",
      "                                914, 186, 126, 565, 347, 595, 299, 258, 465, 211, 795, 170,   1, 793,\n",
      "                                666, 893, 357,  16, 256, 911, 215, 250, 668,  75,  25,  29,  72, 361,\n",
      "                                904, 815, 344, 353, 951, 240,  36, 314, 171,  47, 542, 472,  88, 210,\n",
      "                                285, 901, 286, 368, 921, 637, 802, 497, 199, 853, 550, 421, 520, 358,\n",
      "                                864, 789,  97, 950, 388, 525, 324, 167, 662, 462, 626, 596, 201, 559,\n",
      "                                865, 468, 882, 654, 279, 259, 480, 680, 442, 341, 902, 873, 923, 408,\n",
      "                                929, 759, 566, 812, 868, 485, 191,  74, 558, 890, 527, 653, 434, 992,\n",
      "                                 90, 968, 444, 953, 963, 561, 749, 212, 866, 131, 584, 425, 667, 636,\n",
      "                                 12, 546, 918,  78, 585, 859, 742, 112, 362, 178, 327, 134, 899, 381,\n",
      "                                 49, 975, 688, 764, 591, 969, 797, 745, 830, 241, 683, 197, 850, 900,\n",
      "                                622,  89, 370, 430, 656, 820, 858, 435, 387, 102, 263,  80, 847, 496,\n",
      "                                938, 806, 712, 329,  30, 928, 997, 814, 523, 889, 406, 987, 717, 532,\n",
      "                                849, 788, 982, 303, 765, 578, 973, 773, 941,  43, 287,  31, 568, 277,\n",
      "                                583, 455, 731, 420, 412, 877,  93, 924, 423, 213, 433, 744, 531, 884,\n",
      "                                137, 574, 752, 979, 216, 187, 482, 663, 386, 264, 643, 272, 460, 829,\n",
      "                                597, 689, 758, 939, 490, 832, 512, 402, 892, 652, 912, 840, 549, 323,\n",
      "                                569, 960, 628, 669, 494, 757, 896, 614, 541, 242, 517, 620, 124, 498,\n",
      "                                964, 945, 851,  82, 234, 895, 398, 991, 811, 719, 507, 302, 195, 488,\n",
      "                                125, 105, 915, 552, 136, 697,  91, 633, 492, 457, 942, 966, 355, 673,\n",
      "                                769, 931, 450, 617, 336, 248, 322, 375,  48, 817, 823, 897, 451, 453,\n",
      "                                493, 977, 557, 564, 670, 289, 936,  41, 650, 476,  61, 172, 463, 648,\n",
      "                                 83, 222, 885, 553, 917, 870, 236, 787, 165, 142, 127,  51, 526, 247,\n",
      "                                715, 297, 608,  57, 671, 767, 732,   5, 308, 990, 452, 351, 491, 674,\n",
      "                                906,  58, 162, 228, 599, 208, 573, 117, 639, 106, 616, 841, 203, 780,\n",
      "                                 52, 296, 352, 265, 130, 753, 995, 427, 374, 501, 522, 972, 784, 469,\n",
      "                                474, 724, 383, 809, 478,  96, 384, 508, 151, 881, 691, 661, 350, 141,\n",
      "                                734, 863, 318, 860, 315, 419, 676, 819, 431, 611, 563, 606, 411, 415,\n",
      "                                 45, 372, 930, 325, 392, 306, 174, 844, 664, 693,  98, 153, 869, 772,\n",
      "                                262, 436, 364, 685, 467, 390, 339, 894,  10, 481, 518, 984],\n",
      "                               dtype=torch.int32), tensor([7, 2, 3, 4, 7, 2, 3, 0, 6, 2, 1, 0, 8, 4, 5, 7, 8, 1, 5, 4, 0, 6, 6, 8,\n",
      "                                9, 9, 3, 4, 0, 4, 2, 3, 6, 9, 1, 8, 7, 2, 2, 0, 3, 6, 0, 1, 5, 9, 8, 3,\n",
      "                                7, 0, 4, 8, 1, 7, 1, 6, 5, 3, 0, 7, 7, 4, 2, 9, 0, 3, 9, 4, 9, 2, 5, 6,\n",
      "                                5, 4, 9, 0, 1, 5, 9, 8, 6, 0, 0, 8, 9, 3, 2, 0, 8, 0, 2, 7, 6, 6, 6, 9,\n",
      "                                0, 8, 4, 0, 9, 0, 6, 8, 7, 5, 6, 6, 1, 6, 9, 8, 8, 0, 5, 9, 1, 7, 5, 9,\n",
      "                                0, 9, 5, 7, 9, 8, 5, 1, 5, 7, 8, 9, 2, 3, 5, 1, 7, 3, 1, 5, 2, 7, 9, 0,\n",
      "                                0, 9, 4, 4, 7, 1, 0, 1, 8, 6, 1, 7, 6, 7, 1, 0, 6, 3, 8, 1, 0, 1, 9, 1,\n",
      "                                3, 2, 6, 9, 7, 4, 8, 8, 3, 9, 1, 7, 7, 4, 4, 3, 9, 2, 7, 0, 0, 2, 6, 8,\n",
      "                                6, 9, 3, 2, 3, 3, 8, 7, 6, 8, 6, 8, 8, 0, 5, 6, 3, 9, 1, 9, 3, 2, 0, 9,\n",
      "                                6, 3, 7, 5, 5, 6, 5, 6, 8, 4, 2, 9, 1, 5, 9, 7, 7, 7, 6, 1, 8, 4, 1, 9,\n",
      "                                9, 3, 3, 7, 2, 1, 8, 7, 2, 6, 3, 9, 2, 0, 6, 2, 9, 7, 7, 1, 9, 9, 1, 8,\n",
      "                                5, 6, 5, 0, 7, 6, 0, 4, 8, 2, 9, 4, 3, 8, 8, 1, 0, 2, 6, 7, 7, 6, 2, 7,\n",
      "                                0, 1, 1, 2, 9, 9, 0, 2, 4, 2, 8, 3, 3, 0, 1, 6, 3, 7, 0, 5, 8, 5, 8, 2,\n",
      "                                2, 4, 7, 3, 2, 2, 2, 2, 4, 0, 0, 6, 2, 3, 2, 4, 9, 2, 2, 1, 8, 5, 4, 6,\n",
      "                                2, 9, 6, 0, 7, 5, 5, 3, 5, 5, 1, 0, 6, 9, 7, 3, 1, 7, 2, 5, 6, 2, 3, 7,\n",
      "                                0, 2, 8, 7, 9, 5, 8, 6, 0, 5, 4, 6, 1, 0, 3, 7, 7, 3, 2, 2, 6, 0, 0, 0,\n",
      "                                8, 7, 9, 7, 4, 4, 1, 4, 3, 2, 0, 7, 7, 8, 3, 7, 2, 1, 8, 5, 1, 0, 4, 2,\n",
      "                                0, 0, 9, 2, 5, 0, 7, 2, 1, 3, 6, 4, 2, 4, 7, 9, 6, 8, 1, 0, 0, 0, 0, 6,\n",
      "                                9, 6, 0, 3, 4, 5, 8, 2, 7, 2, 2, 3, 2, 8, 5, 6, 0, 8, 9, 8, 9, 3, 2, 8,\n",
      "                                6, 5, 3, 2, 6, 7, 5, 7, 1, 5, 5, 7, 8, 9, 4, 9, 7, 0, 2, 4, 1, 5, 2, 1,\n",
      "                                6, 1, 5, 2, 3, 4, 2, 2, 0, 2, 1, 7, 9, 5, 0, 9, 5, 7, 0, 2, 4, 1, 2, 6,\n",
      "                                1, 2, 2, 4, 5, 3, 7, 2, 6, 9, 9, 0, 0, 4, 0, 6, 2, 1, 8, 9, 2, 7, 6, 4,\n",
      "                                6, 1, 9, 3, 6, 9, 3, 1, 1, 9, 4, 8, 9, 9, 2, 1, 3, 8, 2, 5, 4, 8, 9, 4,\n",
      "                                7, 9, 0, 2, 2, 3, 9, 2, 8, 2, 0, 3, 9, 1, 1, 5, 7, 8, 3, 3, 6, 3, 7, 9,\n",
      "                                6, 1, 9, 6, 8, 9, 6, 4, 8, 3, 1, 6, 7, 4, 8, 9, 6, 2, 6, 7, 1, 1, 1, 4])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           test_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([857, 282,  48, 378, 290, 670, 160, 676, 948, 896, 994, 483, 567, 954,\n",
      "                                212, 222, 975, 503, 387, 680, 227, 101, 489, 617, 315, 774, 609, 942,\n",
      "                                930, 870, 599, 331, 966,  16, 850, 806, 984, 508, 805, 384, 402, 909,\n",
      "                                133, 140, 726, 437, 492, 400, 504, 518, 822, 702, 923, 758, 722, 719,\n",
      "                                564, 349, 777, 988, 890,  55, 773, 965, 320, 346, 343, 260, 452, 824,\n",
      "                                332, 356, 368, 657, 836, 338, 622, 938, 440, 561, 174,  89, 390, 751,\n",
      "                                134, 998, 270, 597, 191, 576, 819, 329, 754, 912, 837, 109, 176, 262,\n",
      "                                635, 541, 879, 875, 742, 987, 986, 797, 211, 435, 479, 458, 108, 944,\n",
      "                                310, 453, 899, 729,  21, 501, 543, 481, 848, 394, 780, 887, 335, 169,\n",
      "                                566, 586, 216, 436, 198, 281, 144,  78, 701, 223, 926, 175, 885, 434,\n",
      "                                924, 785, 959, 442, 135, 772, 451, 949, 788, 918, 305, 420,  45,  31,\n",
      "                                105, 728, 596, 172, 240,  56, 113, 892, 901, 900, 235, 595, 860, 740,\n",
      "                                915, 867,   5, 484,  73, 272,  92, 573, 448,  10, 115,   6, 154, 374,\n",
      "                                  2, 354, 164, 122, 764,  28, 126, 166, 392,  67, 761, 410, 640, 358,\n",
      "                                219, 168, 615, 208], dtype=torch.int32), tensor([0, 5, 6, 8, 2, 9, 4, 5, 1, 8, 3, 7, 4, 6, 0, 0, 1, 9, 8, 2, 2, 2, 4, 2,\n",
      "                                4, 3, 8, 3, 9, 8, 0, 3, 4, 1, 6, 7, 9, 6, 2, 4, 1, 7, 8, 1, 6, 5, 9, 0,\n",
      "                                3, 2, 1, 1, 4, 3, 9, 5, 8, 7, 7, 7, 6, 3, 1, 6, 7, 1, 1, 5, 8, 9, 0, 5,\n",
      "                                8, 8, 6, 2, 0, 8, 2, 6, 0, 8, 7, 1, 7, 9, 4, 6, 2, 5, 2, 9, 8, 9, 6, 2,\n",
      "                                3, 7, 5, 2, 4, 3, 3, 4, 9, 1, 2, 2, 7, 0, 4, 1, 4, 8, 8, 4, 9, 6, 1, 0,\n",
      "                                4, 6, 9, 3, 4, 9, 2, 2, 0, 3, 8, 8, 2, 0, 2, 9, 5, 4, 2, 9, 6, 8, 6, 8,\n",
      "                                5, 5, 0, 0, 0, 0, 7, 1, 7, 4, 6, 5, 1, 3, 2, 1, 7, 2, 5, 5, 0, 2, 5, 5,\n",
      "                                5, 3, 6, 6, 0, 1, 2, 8, 0, 4, 4, 3, 4, 5, 0, 4, 2, 6, 6, 7, 7, 9, 6, 1,\n",
      "                                1, 5, 6, 7, 9, 1, 9, 2])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([751, 273, 708, 922, 446,  33, 567, 593, 169, 586, 246, 158, 613, 548,\n",
      "                                 26, 410, 690, 625, 947, 956, 804, 875, 253, 268, 271, 590,  42, 342,\n",
      "                                300, 354, 189, 665, 251, 209,  14, 504, 108, 604, 903, 312, 846, 473,\n",
      "                                456, 509, 521, 429,   6, 428, 741, 166, 229, 580,  87, 382, 138, 414,\n",
      "                                232, 916, 736, 985, 983, 396, 624, 519, 511, 394, 330, 891, 185, 440,\n",
      "                                727, 740, 607, 726, 292,  63, 226, 955, 887, 710, 704, 980, 828, 845,\n",
      "                                556, 143, 294, 244, 785, 385, 545, 681, 198, 227, 782, 739, 805, 684,\n",
      "                                572, 933, 499, 348, 718, 115,  66, 101, 943, 371, 129, 231, 184, 675,\n",
      "                                630, 161, 120, 447, 623, 145, 282,  73, 393, 679, 730, 796, 615, 399,\n",
      "                                200, 489, 743, 837, 575, 149, 320, 927, 192, 612, 333, 328, 307, 962,\n",
      "                                786, 783, 334,  84, 196, 148,  76, 291,  46, 579,   9, 119, 970, 479,\n",
      "                                536, 932, 360, 524, 721,  50, 638, 539, 833, 343, 116, 205, 311, 581,\n",
      "                                816, 270, 535, 634, 190, 477, 855, 686, 288, 346, 441, 283, 635, 155,\n",
      "                                 11, 954, 160, 651, 852, 646, 698, 695, 193, 113, 905, 831, 872, 181,\n",
      "                                495, 173, 713, 506], dtype=torch.int32), tensor([3, 7, 2, 2, 4, 9, 5, 5, 5, 8, 0, 7, 6, 1, 5, 2, 2, 9, 7, 3, 8, 0, 4, 7,\n",
      "                                0, 6, 3, 7, 5, 6, 1, 5, 0, 8, 0, 7, 6, 2, 8, 6, 5, 2, 3, 9, 1, 7, 9, 8,\n",
      "                                2, 3, 7, 6, 6, 6, 7, 7, 4, 9, 3, 7, 4, 3, 5, 2, 9, 9, 8, 9, 7, 1, 4, 8,\n",
      "                                6, 6, 8, 1, 4, 2, 2, 4, 2, 9, 6, 4, 5, 4, 7, 3, 6, 6, 8, 0, 5, 4, 7, 1,\n",
      "                                1, 9, 0, 8, 4, 3, 4, 4, 4, 7, 3, 1, 1, 3, 1, 2, 1, 8, 4, 5, 1, 5, 4, 2,\n",
      "                                7, 6, 2, 2, 5, 7, 5, 0, 3, 7, 1, 8, 1, 3, 6, 5, 9, 7, 9, 1, 7, 6, 9, 5,\n",
      "                                8, 3, 4, 6, 8, 6, 4, 3, 5, 4, 7, 7, 6, 3, 5, 6, 6, 1, 6, 9, 2, 1, 7, 3,\n",
      "                                0, 3, 3, 5, 6, 9, 4, 1, 6, 6, 1, 1, 0, 6, 7, 3, 6, 9, 9, 9, 4, 4, 2, 2,\n",
      "                                8, 6, 3, 3, 4, 4, 6, 8])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           metadata={'name': 'node_classification', 'num_classes': 10},)\n",
      "\n",
      "Loaded link prediction task: OnDiskTask(validation_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[872, 565],\n",
      "                                [670, 356],\n",
      "                                [790, 976],\n",
      "                                ...,\n",
      "                                [101, 664],\n",
      "                                [101, 782],\n",
      "                                [101, 695]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[602, 109],\n",
      "                                [562, 230],\n",
      "                                [192,  14],\n",
      "                                ...,\n",
      "                                [648, 488],\n",
      "                                [648, 300],\n",
      "                                [648, 441]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels', 'indexes'),\n",
      "           ),\n",
      "           train_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[530, 906],\n",
      "                                [170, 811],\n",
      "                                [431,  23],\n",
      "                                ...,\n",
      "                                [788, 352],\n",
      "                                [388,  30],\n",
      "                                [456,   7]], dtype=torch.int32),),\n",
      "                            names=('seeds',),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[795,  32],\n",
      "                                [321, 815],\n",
      "                                [234, 831],\n",
      "                                ...,\n",
      "                                [607,   4],\n",
      "                                [124, 510],\n",
      "                                [ 33, 876]], dtype=torch.int32),),\n",
      "                            names=('seeds',),\n",
      "                        )},\n",
      "               names=('seeds',),\n",
      "           ),\n",
      "           test_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[894,  35],\n",
      "                                [645, 860],\n",
      "                                [215, 708],\n",
      "                                ...,\n",
      "                                [545, 770],\n",
      "                                [545,  42],\n",
      "                                [545, 213]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[744, 813],\n",
      "                                [633, 913],\n",
      "                                [888, 109],\n",
      "                                ...,\n",
      "                                [507, 252],\n",
      "                                [507, 289],\n",
      "                                [507, 962]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels', 'indexes'),\n",
      "           ),\n",
      "           metadata={'name': 'link_prediction', 'num_classes': 10},)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dgl/python/dgl/graphbolt/impl/ondisk_dataset.py:463: GBWarning: Edge feature is stored, but edge IDs are not saved.\n",
      "  gb_warning(\"Edge feature is stored, but edge IDs are not saved.\")\n"
     ]
    }
   ],
   "source": [
    "dataset = gb.OnDiskDataset(base_dir).load()\n",
    "graph = dataset.graph\n",
    "print(f\"Loaded graph: {graph}\\n\")\n",
    "\n",
    "feature = dataset.feature\n",
    "print(f\"Loaded feature store: {feature}\\n\")\n",
    "\n",
    "tasks = dataset.tasks\n",
    "nc_task = tasks[0]\n",
    "print(f\"Loaded node classification task: {nc_task}\\n\")\n",
    "lp_task = tasks[1]\n",
    "print(f\"Loaded link prediction task: {lp_task}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
