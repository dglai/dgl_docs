{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnFhPMaAfLtJ"
   },
   "source": [
    "# OnDiskDataset for Heterogeneous Graph\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/stochastic_training/ondisk_dataset_heterograph.ipynb) [![GitHub](https://img.shields.io/badge/-View%20on%20GitHub-181717?logo=github&logoColor=ffffff)](https://github.com/dmlc/dgl/blob/master/notebooks/stochastic_training/ondisk_dataset_heterograph.ipynb)\n",
    "\n",
    "This tutorial shows how to create `OnDiskDataset` for heterogeneous graph that could be used in **GraphBolt** framework. The major difference from creating dataset for homogeneous graph is that we need to specify node/edge types for edges, feature data, training/validation/test sets.\n",
    "\n",
    "By the end of this tutorial, you will be able to\n",
    "\n",
    "- organize graph structure data.\n",
    "- organize feature data.\n",
    "- organize training/validation/test set for specific tasks.\n",
    "\n",
    "To create an ``OnDiskDataset`` object, you need to organize all the data including graph structure, feature data and tasks into a directory. The directory should contain a ``metadata.yaml`` file that describes the metadata of the dataset.\n",
    "\n",
    "Now let's generate various data step by step and organize them together to instantiate `OnDiskDataset` finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlb19DtWgtzq"
   },
   "source": [
    "## Install DGL package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T13:16:01.767129Z",
     "iopub.status.busy": "2024-10-08T13:16:01.766559Z",
     "iopub.status.idle": "2024-10-08T13:16:04.796292Z",
     "shell.execute_reply": "2024-10-08T13:16:04.795166Z"
    },
    "id": "UojlT9ZGgyr9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels-test/repo.html\r\n",
      "Requirement already satisfied: dgl in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (2.2a240410)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (1.14.1)\r\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (3.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (4.66.5)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (6.0.0)\r\n",
      "Requirement already satisfied: torchdata>=0.5.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (0.8.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (2.2.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2024.8.30)\r\n",
      "Requirement already satisfied: torch>=2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torchdata>=0.5.0->dgl) (2.1.0+cpu)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.9.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (3.0.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGL installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "os.environ['DGLBACKEND'] = \"pytorch\"\n",
    "\n",
    "# Install the CPU version.\n",
    "device = torch.device(\"cpu\")\n",
    "!pip install --pre dgl -f https://data.dgl.ai/wheels-test/repo.html\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    import dgl.graphbolt as gb\n",
    "    installed = True\n",
    "except ImportError as error:\n",
    "    installed = False\n",
    "    print(error)\n",
    "print(\"DGL installed!\" if installed else \"DGL not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2R7WnSbjsfbr"
   },
   "source": [
    "## Data preparation\n",
    "In order to demonstrate how to organize various data, let's create a base directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T13:16:04.799206Z",
     "iopub.status.busy": "2024-10-08T13:16:04.798387Z",
     "iopub.status.idle": "2024-10-08T13:16:04.803481Z",
     "shell.execute_reply": "2024-10-08T13:16:04.802617Z"
    },
    "id": "SZipbzyltLfO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created base directory: ./ondisk_dataset_heterograph\n"
     ]
    }
   ],
   "source": [
    "base_dir = './ondisk_dataset_heterograph'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "print(f\"Created base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhNtIn_xhlnl"
   },
   "source": [
    "### Generate graph structure data\n",
    "For heterogeneous graph, we need to save different edge edges(namely seeds) into separate **Numpy** or **CSV** files.\n",
    "\n",
    "Note:\n",
    "- when saving to **Numpy**, the array requires to be in shape of `(2, N)`. This format is recommended as constructing graph from it is much faster than **CSV** file.\n",
    "- when saving to **CSV** file, do not save index and header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T13:16:04.805728Z",
     "iopub.status.busy": "2024-10-08T13:16:04.805435Z",
     "iopub.status.idle": "2024-10-08T13:16:04.826979Z",
     "shell.execute_reply": "2024-10-08T13:16:04.825975Z"
    },
    "id": "HcBt4G5BmSjr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of [user:like:item] edges: [[ 91  50]\n",
      " [535 818]\n",
      " [936 684]\n",
      " [941 350]\n",
      " [ 65 726]]\n",
      "\n",
      "[user:like:item] edges are saved into ./ondisk_dataset_heterograph/like-edges.csv\n",
      "\n",
      "Part of [user:follow:user] edges: [[821 387]\n",
      " [259 712]\n",
      " [ 99 380]\n",
      " [510 717]\n",
      " [138 980]]\n",
      "\n",
      "[user:follow:user] edges are saved into ./ondisk_dataset_heterograph/follow-edges.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For simplicity, we create a heterogeneous graph with\n",
    "# 2 node types: `user`, `item`\n",
    "# 2 edge types: `user:like:item`, `user:follow:user`\n",
    "# And each node/edge type has the same number of nodes/edges.\n",
    "num_nodes = 1000\n",
    "num_edges = 10 * num_nodes\n",
    "\n",
    "# Edge type: \"user:like:item\"\n",
    "like_edges_path = os.path.join(base_dir, \"like-edges.csv\")\n",
    "like_edges = np.random.randint(0, num_nodes, size=(num_edges, 2))\n",
    "print(f\"Part of [user:like:item] edges: {like_edges[:5, :]}\\n\")\n",
    "\n",
    "df = pd.DataFrame(like_edges)\n",
    "df.to_csv(like_edges_path, index=False, header=False)\n",
    "print(f\"[user:like:item] edges are saved into {like_edges_path}\\n\")\n",
    "\n",
    "# Edge type: \"user:follow:user\"\n",
    "follow_edges_path = os.path.join(base_dir, \"follow-edges.csv\")\n",
    "follow_edges = np.random.randint(0, num_nodes, size=(num_edges, 2))\n",
    "print(f\"Part of [user:follow:user] edges: {follow_edges[:5, :]}\\n\")\n",
    "\n",
    "df = pd.DataFrame(follow_edges)\n",
    "df.to_csv(follow_edges_path, index=False, header=False)\n",
    "print(f\"[user:follow:user] edges are saved into {follow_edges_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh-4cPtzpcaH"
   },
   "source": [
    "### Generate feature data for graph\n",
    "For feature data, numpy arrays and torch tensors are supported for now. Let's generate feature data for each node/edge type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T13:16:04.829362Z",
     "iopub.status.busy": "2024-10-08T13:16:04.828945Z",
     "iopub.status.idle": "2024-10-08T13:16:04.856220Z",
     "shell.execute_reply": "2024-10-08T13:16:04.855364Z"
    },
    "id": "_PVu1u5brBhF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of node[user] feature [feat_0]: [[0.03884392 0.18089688 0.669433   0.23429238 0.61734171]\n",
      " [0.10406934 0.90966658 0.04910352 0.07506954 0.93137157]\n",
      " [0.50522244 0.52598641 0.8381592  0.24523537 0.23411243]]\n",
      "Node[user] feature [feat_0] is saved to ./ondisk_dataset_heterograph/node-user-feat-0.npy\n",
      "\n",
      "Part of node[user] feature [feat_1]: tensor([[0.3889, 0.7271, 0.6841, 0.0787, 0.1833],\n",
      "        [0.7456, 0.5380, 0.1171, 0.0586, 0.9332],\n",
      "        [0.7691, 0.1819, 0.2899, 0.1092, 0.9930]])\n",
      "Node[user] feature [feat_1] is saved to ./ondisk_dataset_heterograph/node-user-feat-1.pt\n",
      "\n",
      "Part of node[item] feature [feat_0]: [[0.66460526 0.66121368 0.44104573 0.27252067 0.77962481]\n",
      " [0.20535868 0.65459509 0.12486955 0.11850859 0.28994011]\n",
      " [0.51450492 0.4042643  0.65495489 0.65297862 0.51353316]]\n",
      "Node[item] feature [feat_0] is saved to ./ondisk_dataset_heterograph/node-item-feat-0.npy\n",
      "\n",
      "Part of node[item] feature [feat_1]: tensor([[0.7049, 0.8159, 0.0270, 0.8561, 0.1643],\n",
      "        [0.4519, 0.0489, 0.8217, 0.1029, 0.2037],\n",
      "        [0.4429, 0.5648, 0.8133, 0.7929, 0.5278]])\n",
      "Node[item] feature [feat_1] is saved to ./ondisk_dataset_heterograph/node-item-feat-1.pt\n",
      "\n",
      "Part of edge[user:like:item] feature [feat_0]: [[0.6834038  0.49364496 0.91489178 0.21486139 0.48835498]\n",
      " [0.0523541  0.94322926 0.44475279 0.66771833 0.42404606]\n",
      " [0.51671263 0.75903733 0.38879249 0.57428073 0.70164897]]\n",
      "Edge[user:like:item] feature [feat_0] is saved to ./ondisk_dataset_heterograph/edge-like-feat-0.npy\n",
      "\n",
      "Part of edge[user:like:item] feature [feat_1]: tensor([[0.1656, 0.0723, 0.0628, 0.5738, 0.7722],\n",
      "        [0.1290, 0.0576, 0.0119, 0.0860, 0.8274],\n",
      "        [0.4964, 0.0884, 0.2959, 0.3679, 0.7033]])\n",
      "Edge[user:like:item] feature [feat_1] is saved to ./ondisk_dataset_heterograph/edge-like-feat-1.pt\n",
      "\n",
      "Part of edge[user:follow:user] feature [feat_0]: [[0.82167324 0.55361004 0.29381855 0.18365526 0.93236786]\n",
      " [0.88420989 0.1045981  0.78409535 0.40008032 0.26765459]\n",
      " [0.33225152 0.21105033 0.19992145 0.82920922 0.02210918]]\n",
      "Edge[user:follow:user] feature [feat_0] is saved to ./ondisk_dataset_heterograph/edge-follow-feat-0.npy\n",
      "\n",
      "Part of edge[user:follow:user] feature [feat_1]: tensor([[0.0631, 0.8269, 0.3711, 0.8949, 0.7186],\n",
      "        [0.3398, 0.0094, 0.0960, 0.9435, 0.4578],\n",
      "        [0.9937, 0.2810, 0.9538, 0.4726, 0.6980]])\n",
      "Edge[user:follow:user] feature [feat_1] is saved to ./ondisk_dataset_heterograph/edge-follow-feat-1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate node[user] feature in numpy array.\n",
    "node_user_feat_0_path = os.path.join(base_dir, \"node-user-feat-0.npy\")\n",
    "node_user_feat_0 = np.random.rand(num_nodes, 5)\n",
    "print(f\"Part of node[user] feature [feat_0]: {node_user_feat_0[:3, :]}\")\n",
    "np.save(node_user_feat_0_path, node_user_feat_0)\n",
    "print(f\"Node[user] feature [feat_0] is saved to {node_user_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another node[user] feature in torch tensor\n",
    "node_user_feat_1_path = os.path.join(base_dir, \"node-user-feat-1.pt\")\n",
    "node_user_feat_1 = torch.rand(num_nodes, 5)\n",
    "print(f\"Part of node[user] feature [feat_1]: {node_user_feat_1[:3, :]}\")\n",
    "torch.save(node_user_feat_1, node_user_feat_1_path)\n",
    "print(f\"Node[user] feature [feat_1] is saved to {node_user_feat_1_path}\\n\")\n",
    "\n",
    "# Generate node[item] feature in numpy array.\n",
    "node_item_feat_0_path = os.path.join(base_dir, \"node-item-feat-0.npy\")\n",
    "node_item_feat_0 = np.random.rand(num_nodes, 5)\n",
    "print(f\"Part of node[item] feature [feat_0]: {node_item_feat_0[:3, :]}\")\n",
    "np.save(node_item_feat_0_path, node_item_feat_0)\n",
    "print(f\"Node[item] feature [feat_0] is saved to {node_item_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another node[item] feature in torch tensor\n",
    "node_item_feat_1_path = os.path.join(base_dir, \"node-item-feat-1.pt\")\n",
    "node_item_feat_1 = torch.rand(num_nodes, 5)\n",
    "print(f\"Part of node[item] feature [feat_1]: {node_item_feat_1[:3, :]}\")\n",
    "torch.save(node_item_feat_1, node_item_feat_1_path)\n",
    "print(f\"Node[item] feature [feat_1] is saved to {node_item_feat_1_path}\\n\")\n",
    "\n",
    "# Generate edge[user:like:item] feature in numpy array.\n",
    "edge_like_feat_0_path = os.path.join(base_dir, \"edge-like-feat-0.npy\")\n",
    "edge_like_feat_0 = np.random.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:like:item] feature [feat_0]: {edge_like_feat_0[:3, :]}\")\n",
    "np.save(edge_like_feat_0_path, edge_like_feat_0)\n",
    "print(f\"Edge[user:like:item] feature [feat_0] is saved to {edge_like_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another edge[user:like:item] feature in torch tensor\n",
    "edge_like_feat_1_path = os.path.join(base_dir, \"edge-like-feat-1.pt\")\n",
    "edge_like_feat_1 = torch.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:like:item] feature [feat_1]: {edge_like_feat_1[:3, :]}\")\n",
    "torch.save(edge_like_feat_1, edge_like_feat_1_path)\n",
    "print(f\"Edge[user:like:item] feature [feat_1] is saved to {edge_like_feat_1_path}\\n\")\n",
    "\n",
    "# Generate edge[user:follow:user] feature in numpy array.\n",
    "edge_follow_feat_0_path = os.path.join(base_dir, \"edge-follow-feat-0.npy\")\n",
    "edge_follow_feat_0 = np.random.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:follow:user] feature [feat_0]: {edge_follow_feat_0[:3, :]}\")\n",
    "np.save(edge_follow_feat_0_path, edge_follow_feat_0)\n",
    "print(f\"Edge[user:follow:user] feature [feat_0] is saved to {edge_follow_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another edge[user:follow:user] feature in torch tensor\n",
    "edge_follow_feat_1_path = os.path.join(base_dir, \"edge-follow-feat-1.pt\")\n",
    "edge_follow_feat_1 = torch.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:follow:user] feature [feat_1]: {edge_follow_feat_1[:3, :]}\")\n",
    "torch.save(edge_follow_feat_1, edge_follow_feat_1_path)\n",
    "print(f\"Edge[user:follow:user] feature [feat_1] is saved to {edge_follow_feat_1_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyqgOtsIwzh_"
   },
   "source": [
    "### Generate tasks\n",
    "`OnDiskDataset` supports multiple tasks. For each task, we need to prepare training/validation/test sets respectively. Such sets usually vary among different tasks. In this tutorial, let's create a **Node Classification** task and **Link Prediction** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVxHaDIfzCkr"
   },
   "source": [
    "#### Node Classification Task\n",
    "For node classification task, we need **node IDs** and corresponding **labels** for each training/validation/test set. Like feature data, numpy arrays and torch tensors are supported for these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T13:16:04.858632Z",
     "iopub.status.busy": "2024-10-08T13:16:04.858214Z",
     "iopub.status.idle": "2024-10-08T13:16:04.877692Z",
     "shell.execute_reply": "2024-10-08T13:16:04.876869Z"
    },
    "id": "S5-fyBbHzTCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of train ids[user] for node classification: [843 592 167]\n",
      "NC train ids[user] are saved to ./ondisk_dataset_heterograph/nc-train-user-ids.npy\n",
      "\n",
      "Part of train labels[user] for node classification: tensor([8, 9, 3])\n",
      "NC train labels[user] are saved to ./ondisk_dataset_heterograph/nc-train-user-labels.pt\n",
      "\n",
      "Part of train ids[item] for node classification: [234 817 484]\n",
      "NC train ids[item] are saved to ./ondisk_dataset_heterograph/nc-train-item-ids.npy\n",
      "\n",
      "Part of train labels[item] for node classification: tensor([8, 1, 8])\n",
      "NC train labels[item] are saved to ./ondisk_dataset_heterograph/nc-train-item-labels.pt\n",
      "\n",
      "Part of val ids[user] for node classification: [797  93 657]\n",
      "NC val ids[user] are saved to ./ondisk_dataset_heterograph/nc-val-user-ids.npy\n",
      "\n",
      "Part of val labels[user] for node classification: tensor([4, 3, 3])\n",
      "NC val labels[user] are saved to ./ondisk_dataset_heterograph/nc-val-user-labels.pt\n",
      "\n",
      "Part of val ids[item] for node classification: [584 924 156]\n",
      "NC val ids[item] are saved to ./ondisk_dataset_heterograph/nc-val-item-ids.npy\n",
      "\n",
      "Part of val labels[item] for node classification: tensor([9, 5, 8])\n",
      "NC val labels[item] are saved to ./ondisk_dataset_heterograph/nc-val-item-labels.pt\n",
      "\n",
      "Part of test ids[user] for node classification: [921 226 743]\n",
      "NC test ids[user] are saved to ./ondisk_dataset_heterograph/nc-test-user-ids.npy\n",
      "\n",
      "Part of test labels[user] for node classification: tensor([8, 4, 8])\n",
      "NC test labels[user] are saved to ./ondisk_dataset_heterograph/nc-test-user-labels.pt\n",
      "\n",
      "Part of test ids[item] for node classification: [682 627 546]\n",
      "NC test ids[item] are saved to ./ondisk_dataset_heterograph/nc-test-item-ids.npy\n",
      "\n",
      "Part of test labels[item] for node classification: tensor([2, 8, 0])\n",
      "NC test labels[item] are saved to ./ondisk_dataset_heterograph/nc-test-item-labels.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration, let's generate item sets for each node type.\n",
    "num_trains = int(num_nodes * 0.6)\n",
    "num_vals = int(num_nodes * 0.2)\n",
    "num_tests = num_nodes - num_trains - num_vals\n",
    "\n",
    "user_ids = np.arange(num_nodes)\n",
    "np.random.shuffle(user_ids)\n",
    "\n",
    "item_ids = np.arange(num_nodes)\n",
    "np.random.shuffle(item_ids)\n",
    "\n",
    "# Train IDs for user.\n",
    "nc_train_user_ids_path = os.path.join(base_dir, \"nc-train-user-ids.npy\")\n",
    "nc_train_user_ids = user_ids[:num_trains]\n",
    "print(f\"Part of train ids[user] for node classification: {nc_train_user_ids[:3]}\")\n",
    "np.save(nc_train_user_ids_path, nc_train_user_ids)\n",
    "print(f\"NC train ids[user] are saved to {nc_train_user_ids_path}\\n\")\n",
    "\n",
    "# Train labels for user.\n",
    "nc_train_user_labels_path = os.path.join(base_dir, \"nc-train-user-labels.pt\")\n",
    "nc_train_user_labels = torch.randint(0, 10, (num_trains,))\n",
    "print(f\"Part of train labels[user] for node classification: {nc_train_user_labels[:3]}\")\n",
    "torch.save(nc_train_user_labels, nc_train_user_labels_path)\n",
    "print(f\"NC train labels[user] are saved to {nc_train_user_labels_path}\\n\")\n",
    "\n",
    "# Train IDs for item.\n",
    "nc_train_item_ids_path = os.path.join(base_dir, \"nc-train-item-ids.npy\")\n",
    "nc_train_item_ids = item_ids[:num_trains]\n",
    "print(f\"Part of train ids[item] for node classification: {nc_train_item_ids[:3]}\")\n",
    "np.save(nc_train_item_ids_path, nc_train_item_ids)\n",
    "print(f\"NC train ids[item] are saved to {nc_train_item_ids_path}\\n\")\n",
    "\n",
    "# Train labels for item.\n",
    "nc_train_item_labels_path = os.path.join(base_dir, \"nc-train-item-labels.pt\")\n",
    "nc_train_item_labels = torch.randint(0, 10, (num_trains,))\n",
    "print(f\"Part of train labels[item] for node classification: {nc_train_item_labels[:3]}\")\n",
    "torch.save(nc_train_item_labels, nc_train_item_labels_path)\n",
    "print(f\"NC train labels[item] are saved to {nc_train_item_labels_path}\\n\")\n",
    "\n",
    "# Val IDs for user.\n",
    "nc_val_user_ids_path = os.path.join(base_dir, \"nc-val-user-ids.npy\")\n",
    "nc_val_user_ids = user_ids[num_trains:num_trains+num_vals]\n",
    "print(f\"Part of val ids[user] for node classification: {nc_val_user_ids[:3]}\")\n",
    "np.save(nc_val_user_ids_path, nc_val_user_ids)\n",
    "print(f\"NC val ids[user] are saved to {nc_val_user_ids_path}\\n\")\n",
    "\n",
    "# Val labels for user.\n",
    "nc_val_user_labels_path = os.path.join(base_dir, \"nc-val-user-labels.pt\")\n",
    "nc_val_user_labels = torch.randint(0, 10, (num_vals,))\n",
    "print(f\"Part of val labels[user] for node classification: {nc_val_user_labels[:3]}\")\n",
    "torch.save(nc_val_user_labels, nc_val_user_labels_path)\n",
    "print(f\"NC val labels[user] are saved to {nc_val_user_labels_path}\\n\")\n",
    "\n",
    "# Val IDs for item.\n",
    "nc_val_item_ids_path = os.path.join(base_dir, \"nc-val-item-ids.npy\")\n",
    "nc_val_item_ids = item_ids[num_trains:num_trains+num_vals]\n",
    "print(f\"Part of val ids[item] for node classification: {nc_val_item_ids[:3]}\")\n",
    "np.save(nc_val_item_ids_path, nc_val_item_ids)\n",
    "print(f\"NC val ids[item] are saved to {nc_val_item_ids_path}\\n\")\n",
    "\n",
    "# Val labels for item.\n",
    "nc_val_item_labels_path = os.path.join(base_dir, \"nc-val-item-labels.pt\")\n",
    "nc_val_item_labels = torch.randint(0, 10, (num_vals,))\n",
    "print(f\"Part of val labels[item] for node classification: {nc_val_item_labels[:3]}\")\n",
    "torch.save(nc_val_item_labels, nc_val_item_labels_path)\n",
    "print(f\"NC val labels[item] are saved to {nc_val_item_labels_path}\\n\")\n",
    "\n",
    "# Test IDs for user.\n",
    "nc_test_user_ids_path = os.path.join(base_dir, \"nc-test-user-ids.npy\")\n",
    "nc_test_user_ids = user_ids[-num_tests:]\n",
    "print(f\"Part of test ids[user] for node classification: {nc_test_user_ids[:3]}\")\n",
    "np.save(nc_test_user_ids_path, nc_test_user_ids)\n",
    "print(f\"NC test ids[user] are saved to {nc_test_user_ids_path}\\n\")\n",
    "\n",
    "# Test labels for user.\n",
    "nc_test_user_labels_path = os.path.join(base_dir, \"nc-test-user-labels.pt\")\n",
    "nc_test_user_labels = torch.randint(0, 10, (num_tests,))\n",
    "print(f\"Part of test labels[user] for node classification: {nc_test_user_labels[:3]}\")\n",
    "torch.save(nc_test_user_labels, nc_test_user_labels_path)\n",
    "print(f\"NC test labels[user] are saved to {nc_test_user_labels_path}\\n\")\n",
    "\n",
    "# Test IDs for item.\n",
    "nc_test_item_ids_path = os.path.join(base_dir, \"nc-test-item-ids.npy\")\n",
    "nc_test_item_ids = item_ids[-num_tests:]\n",
    "print(f\"Part of test ids[item] for node classification: {nc_test_item_ids[:3]}\")\n",
    "np.save(nc_test_item_ids_path, nc_test_item_ids)\n",
    "print(f\"NC test ids[item] are saved to {nc_test_item_ids_path}\\n\")\n",
    "\n",
    "# Test labels for item.\n",
    "nc_test_item_labels_path = os.path.join(base_dir, \"nc-test-item-labels.pt\")\n",
    "nc_test_item_labels = torch.randint(0, 10, (num_tests,))\n",
    "print(f\"Part of test labels[item] for node classification: {nc_test_item_labels[:3]}\")\n",
    "torch.save(nc_test_item_labels, nc_test_item_labels_path)\n",
    "print(f\"NC test labels[item] are saved to {nc_test_item_labels_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhAcDCHQ_KJ0"
   },
   "source": [
    "#### Link Prediction Task\n",
    "For link prediction task, we need **seeds** or **corresponding labels and indexes** which representing the pos/neg property and group of the seeds for each training/validation/test set. Like feature data, numpy arrays and torch tensors are supported for these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T13:16:04.880128Z",
     "iopub.status.busy": "2024-10-08T13:16:04.879718Z",
     "iopub.status.idle": "2024-10-08T13:16:04.911504Z",
     "shell.execute_reply": "2024-10-08T13:16:04.910610Z"
    },
    "id": "u0jCnXIcAQy4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of train seeds[user:like:item] for link prediction: [[ 91  50]\n",
      " [535 818]\n",
      " [936 684]]\n",
      "LP train seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-train-like-seeds.npy\n",
      "\n",
      "Part of train seeds[user:follow:user] for link prediction: [[821 387]\n",
      " [259 712]\n",
      " [ 99 380]]\n",
      "LP train seeds[user:follow:user] are saved to ./ondisk_dataset_heterograph/lp-train-follow-seeds.npy\n",
      "\n",
      "Part of val seeds[user:like:item] for link prediction: [[827 150]\n",
      " [739  86]\n",
      " [ 21 811]]\n",
      "LP val seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-seeds.npy\n",
      "\n",
      "Part of val labels[user:like:item] for link prediction: [1. 1. 1.]\n",
      "LP val labels[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-labels.npy\n",
      "\n",
      "Part of val indexes[user:like:item] for link prediction: [0 1 2]\n",
      "LP val indexes[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-indexes.npy\n",
      "\n",
      "Part of val seeds[user:follow:item] for link prediction: [[106 414]\n",
      " [108 276]\n",
      " [ 84 764]]\n",
      "LP val seeds[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-seeds.npy\n",
      "\n",
      "Part of val labels[user:follow:item] for link prediction: [1. 1. 1.]\n",
      "LP val labels[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-labels.npy\n",
      "\n",
      "Part of val indexes[user:follow:item] for link prediction: [0 1 2]\n",
      "LP val indexes[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-val-follow-indexes.npy\n",
      "\n",
      "Part of test seeds[user:like:item] for link prediction: [[873 464]\n",
      " [521  46]\n",
      " [147 280]]\n",
      "LP test seeds[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-seeds.npy\n",
      "\n",
      "Part of test labels[user:like:item] for link prediction: [1. 1. 1.]\n",
      "LP test labels[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-labels.npy\n",
      "\n",
      "Part of test indexes[user:like:item] for link prediction: [0 1 2]\n",
      "LP test indexes[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-indexes.npy\n",
      "\n",
      "Part of test seeds[user:follow:item] for link prediction: [[350 862]\n",
      " [912 711]\n",
      " [894 220]]\n",
      "LP test seeds[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-seeds.npy\n",
      "\n",
      "Part of test labels[user:follow:item] for link prediction: [1. 1. 1.]\n",
      "LP test labels[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-labels.npy\n",
      "\n",
      "Part of test indexes[user:follow:item] for link prediction: [0 1 2]\n",
      "LP test indexes[user:follow:item] are saved to ./ondisk_dataset_heterograph/lp-test-follow-indexes.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration, let's generate item sets for each edge type.\n",
    "num_trains = int(num_edges * 0.6)\n",
    "num_vals = int(num_edges * 0.2)\n",
    "num_tests = num_edges - num_trains - num_vals\n",
    "\n",
    "# Train seeds for user:like:item.\n",
    "lp_train_like_seeds_path = os.path.join(base_dir, \"lp-train-like-seeds.npy\")\n",
    "lp_train_like_seeds = like_edges[:num_trains, :]\n",
    "print(f\"Part of train seeds[user:like:item] for link prediction: {lp_train_like_seeds[:3]}\")\n",
    "np.save(lp_train_like_seeds_path, lp_train_like_seeds)\n",
    "print(f\"LP train seeds[user:like:item] are saved to {lp_train_like_seeds_path}\\n\")\n",
    "\n",
    "# Train seeds for user:follow:user.\n",
    "lp_train_follow_seeds_path = os.path.join(base_dir, \"lp-train-follow-seeds.npy\")\n",
    "lp_train_follow_seeds = follow_edges[:num_trains, :]\n",
    "print(f\"Part of train seeds[user:follow:user] for link prediction: {lp_train_follow_seeds[:3]}\")\n",
    "np.save(lp_train_follow_seeds_path, lp_train_follow_seeds)\n",
    "print(f\"LP train seeds[user:follow:user] are saved to {lp_train_follow_seeds_path}\\n\")\n",
    "\n",
    "# Val seeds for user:like:item.\n",
    "lp_val_like_seeds_path = os.path.join(base_dir, \"lp-val-like-seeds.npy\")\n",
    "lp_val_like_seeds = like_edges[num_trains:num_trains+num_vals, :]\n",
    "lp_val_like_neg_dsts = np.random.randint(0, num_nodes, (num_vals, 10)).reshape(-1)\n",
    "lp_val_like_neg_srcs = np.repeat(lp_val_like_seeds[:,0], 10)\n",
    "lp_val_like_neg_seeds = np.concatenate((lp_val_like_neg_srcs, lp_val_like_neg_dsts)).reshape(2,-1).T\n",
    "lp_val_like_seeds = np.concatenate((lp_val_like_seeds, lp_val_like_neg_seeds))\n",
    "print(f\"Part of val seeds[user:like:item] for link prediction: {lp_val_like_seeds[:3]}\")\n",
    "np.save(lp_val_like_seeds_path, lp_val_like_seeds)\n",
    "print(f\"LP val seeds[user:like:item] are saved to {lp_val_like_seeds_path}\\n\")\n",
    "\n",
    "# Val labels for user:like:item.\n",
    "lp_val_like_labels_path = os.path.join(base_dir, \"lp-val-like-labels.npy\")\n",
    "lp_val_like_labels = np.empty(num_vals * (10 + 1))\n",
    "lp_val_like_labels[:num_vals] = 1\n",
    "lp_val_like_labels[num_vals:] = 0\n",
    "print(f\"Part of val labels[user:like:item] for link prediction: {lp_val_like_labels[:3]}\")\n",
    "np.save(lp_val_like_labels_path, lp_val_like_labels)\n",
    "print(f\"LP val labels[user:like:item] are saved to {lp_val_like_labels_path}\\n\")\n",
    "\n",
    "# Val indexes for user:like:item.\n",
    "lp_val_like_indexes_path = os.path.join(base_dir, \"lp-val-like-indexes.npy\")\n",
    "lp_val_like_indexes = np.arange(0, num_vals)\n",
    "lp_val_like_neg_indexes = np.repeat(lp_val_like_indexes, 10)\n",
    "lp_val_like_indexes = np.concatenate([lp_val_like_indexes, lp_val_like_neg_indexes])\n",
    "print(f\"Part of val indexes[user:like:item] for link prediction: {lp_val_like_indexes[:3]}\")\n",
    "np.save(lp_val_like_indexes_path, lp_val_like_indexes)\n",
    "print(f\"LP val indexes[user:like:item] are saved to {lp_val_like_indexes_path}\\n\")\n",
    "\n",
    "# Val seeds for user:follow:item.\n",
    "lp_val_follow_seeds_path = os.path.join(base_dir, \"lp-val-follow-seeds.npy\")\n",
    "lp_val_follow_seeds = follow_edges[num_trains:num_trains+num_vals, :]\n",
    "lp_val_follow_neg_dsts = np.random.randint(0, num_nodes, (num_vals, 10)).reshape(-1)\n",
    "lp_val_follow_neg_srcs = np.repeat(lp_val_follow_seeds[:,0], 10)\n",
    "lp_val_follow_neg_seeds = np.concatenate((lp_val_follow_neg_srcs, lp_val_follow_neg_dsts)).reshape(2,-1).T\n",
    "lp_val_follow_seeds = np.concatenate((lp_val_follow_seeds, lp_val_follow_neg_seeds))\n",
    "print(f\"Part of val seeds[user:follow:item] for link prediction: {lp_val_follow_seeds[:3]}\")\n",
    "np.save(lp_val_follow_seeds_path, lp_val_follow_seeds)\n",
    "print(f\"LP val seeds[user:follow:item] are saved to {lp_val_follow_seeds_path}\\n\")\n",
    "\n",
    "# Val labels for user:follow:item.\n",
    "lp_val_follow_labels_path = os.path.join(base_dir, \"lp-val-follow-labels.npy\")\n",
    "lp_val_follow_labels = np.empty(num_vals * (10 + 1))\n",
    "lp_val_follow_labels[:num_vals] = 1\n",
    "lp_val_follow_labels[num_vals:] = 0\n",
    "print(f\"Part of val labels[user:follow:item] for link prediction: {lp_val_follow_labels[:3]}\")\n",
    "np.save(lp_val_follow_labels_path, lp_val_follow_labels)\n",
    "print(f\"LP val labels[user:follow:item] are saved to {lp_val_follow_labels_path}\\n\")\n",
    "\n",
    "# Val indexes for user:follow:item.\n",
    "lp_val_follow_indexes_path = os.path.join(base_dir, \"lp-val-follow-indexes.npy\")\n",
    "lp_val_follow_indexes = np.arange(0, num_vals)\n",
    "lp_val_follow_neg_indexes = np.repeat(lp_val_follow_indexes, 10)\n",
    "lp_val_follow_indexes = np.concatenate([lp_val_follow_indexes, lp_val_follow_neg_indexes])\n",
    "print(f\"Part of val indexes[user:follow:item] for link prediction: {lp_val_follow_indexes[:3]}\")\n",
    "np.save(lp_val_follow_indexes_path, lp_val_follow_indexes)\n",
    "print(f\"LP val indexes[user:follow:item] are saved to {lp_val_follow_indexes_path}\\n\")\n",
    "\n",
    "# Test seeds for user:like:item.\n",
    "lp_test_like_seeds_path = os.path.join(base_dir, \"lp-test-like-seeds.npy\")\n",
    "lp_test_like_seeds = like_edges[-num_tests:, :]\n",
    "lp_test_like_neg_dsts = np.random.randint(0, num_nodes, (num_tests, 10)).reshape(-1)\n",
    "lp_test_like_neg_srcs = np.repeat(lp_test_like_seeds[:,0], 10)\n",
    "lp_test_like_neg_seeds = np.concatenate((lp_test_like_neg_srcs, lp_test_like_neg_dsts)).reshape(2,-1).T\n",
    "lp_test_like_seeds = np.concatenate((lp_test_like_seeds, lp_test_like_neg_seeds))\n",
    "print(f\"Part of test seeds[user:like:item] for link prediction: {lp_test_like_seeds[:3]}\")\n",
    "np.save(lp_test_like_seeds_path, lp_test_like_seeds)\n",
    "print(f\"LP test seeds[user:like:item] are saved to {lp_test_like_seeds_path}\\n\")\n",
    "\n",
    "# Test labels for user:like:item.\n",
    "lp_test_like_labels_path = os.path.join(base_dir, \"lp-test-like-labels.npy\")\n",
    "lp_test_like_labels = np.empty(num_tests * (10 + 1))\n",
    "lp_test_like_labels[:num_tests] = 1\n",
    "lp_test_like_labels[num_tests:] = 0\n",
    "print(f\"Part of test labels[user:like:item] for link prediction: {lp_test_like_labels[:3]}\")\n",
    "np.save(lp_test_like_labels_path, lp_test_like_labels)\n",
    "print(f\"LP test labels[user:like:item] are saved to {lp_test_like_labels_path}\\n\")\n",
    "\n",
    "# Test indexes for user:like:item.\n",
    "lp_test_like_indexes_path = os.path.join(base_dir, \"lp-test-like-indexes.npy\")\n",
    "lp_test_like_indexes = np.arange(0, num_tests)\n",
    "lp_test_like_neg_indexes = np.repeat(lp_test_like_indexes, 10)\n",
    "lp_test_like_indexes = np.concatenate([lp_test_like_indexes, lp_test_like_neg_indexes])\n",
    "print(f\"Part of test indexes[user:like:item] for link prediction: {lp_test_like_indexes[:3]}\")\n",
    "np.save(lp_test_like_indexes_path, lp_test_like_indexes)\n",
    "print(f\"LP test indexes[user:like:item] are saved to {lp_test_like_indexes_path}\\n\")\n",
    "\n",
    "# Test seeds for user:follow:item.\n",
    "lp_test_follow_seeds_path = os.path.join(base_dir, \"lp-test-follow-seeds.npy\")\n",
    "lp_test_follow_seeds = follow_edges[-num_tests:, :]\n",
    "lp_test_follow_neg_dsts = np.random.randint(0, num_nodes, (num_tests, 10)).reshape(-1)\n",
    "lp_test_follow_neg_srcs = np.repeat(lp_test_follow_seeds[:,0], 10)\n",
    "lp_test_follow_neg_seeds = np.concatenate((lp_test_follow_neg_srcs, lp_test_follow_neg_dsts)).reshape(2,-1).T\n",
    "lp_test_follow_seeds = np.concatenate((lp_test_follow_seeds, lp_test_follow_neg_seeds))\n",
    "print(f\"Part of test seeds[user:follow:item] for link prediction: {lp_test_follow_seeds[:3]}\")\n",
    "np.save(lp_test_follow_seeds_path, lp_test_follow_seeds)\n",
    "print(f\"LP test seeds[user:follow:item] are saved to {lp_test_follow_seeds_path}\\n\")\n",
    "\n",
    "# Test labels for user:follow:item.\n",
    "lp_test_follow_labels_path = os.path.join(base_dir, \"lp-test-follow-labels.npy\")\n",
    "lp_test_follow_labels = np.empty(num_tests * (10 + 1))\n",
    "lp_test_follow_labels[:num_tests] = 1\n",
    "lp_test_follow_labels[num_tests:] = 0\n",
    "print(f\"Part of test labels[user:follow:item] for link prediction: {lp_test_follow_labels[:3]}\")\n",
    "np.save(lp_test_follow_labels_path, lp_test_follow_labels)\n",
    "print(f\"LP test labels[user:follow:item] are saved to {lp_test_follow_labels_path}\\n\")\n",
    "\n",
    "# Test indexes for user:follow:item.\n",
    "lp_test_follow_indexes_path = os.path.join(base_dir, \"lp-test-follow-indexes.npy\")\n",
    "lp_test_follow_indexes = np.arange(0, num_tests)\n",
    "lp_test_follow_neg_indexes = np.repeat(lp_test_follow_indexes, 10)\n",
    "lp_test_follow_indexes = np.concatenate([lp_test_follow_indexes, lp_test_follow_neg_indexes])\n",
    "print(f\"Part of test indexes[user:follow:item] for link prediction: {lp_test_follow_indexes[:3]}\")\n",
    "np.save(lp_test_follow_indexes_path, lp_test_follow_indexes)\n",
    "print(f\"LP test indexes[user:follow:item] are saved to {lp_test_follow_indexes_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbk6-wxRK-6S"
   },
   "source": [
    "## Organize Data into YAML File\n",
    "Now we need to create a `metadata.yaml` file which contains the paths, dadta types of graph structure, feature data, training/validation/test sets. Please note that all path should be relative to `metadata.yaml`.\n",
    "\n",
    "For heterogeneous graph, we need to specify the node/edge type in **type** fields. For edge type, canonical etype is required which is a string that's concatenated by source node type, etype, and destination node type together with `:`.\n",
    "\n",
    "Notes:\n",
    "- all path should be relative to `metadata.yaml`.\n",
    "- Below fields are optional and not specified in below example.\n",
    "  - `in_memory`: indicates whether to load dada into memory or `mmap`. Default is `True`.\n",
    "\n",
    "Please refer to [YAML specification](https://github.com/dmlc/dgl/blob/master/docs/source/stochastic_training/ondisk-dataset-specification.rst) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T13:16:04.913812Z",
     "iopub.status.busy": "2024-10-08T13:16:04.913403Z",
     "iopub.status.idle": "2024-10-08T13:16:04.921994Z",
     "shell.execute_reply": "2024-10-08T13:16:04.921178Z"
    },
    "id": "ddGTWW61Lpwp"
   },
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "    dataset_name: heterogeneous_graph_nc_lp\n",
    "    graph:\n",
    "      nodes:\n",
    "        - type: user\n",
    "          num: {num_nodes}\n",
    "        - type: item\n",
    "          num: {num_nodes}\n",
    "      edges:\n",
    "        - type: \"user:like:item\"\n",
    "          format: csv\n",
    "          path: {os.path.basename(like_edges_path)}\n",
    "        - type: \"user:follow:user\"\n",
    "          format: csv\n",
    "          path: {os.path.basename(follow_edges_path)}\n",
    "    feature_data:\n",
    "      - domain: node\n",
    "        type: user\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(node_user_feat_0_path)}\n",
    "      - domain: node\n",
    "        type: user\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(node_user_feat_1_path)}\n",
    "      - domain: node\n",
    "        type: item\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(node_item_feat_0_path)}\n",
    "      - domain: node\n",
    "        type: item\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(node_item_feat_1_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:like:item\"\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(edge_like_feat_0_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:like:item\"\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(edge_like_feat_1_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:follow:user\"\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(edge_follow_feat_0_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:follow:user\"\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(edge_follow_feat_1_path)}\n",
    "    tasks:\n",
    "      - name: node_classification\n",
    "        num_classes: 10\n",
    "        train_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_train_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_train_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_train_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_train_item_labels_path)}\n",
    "        validation_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_val_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_val_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_val_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_val_item_labels_path)}\n",
    "        test_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_test_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_test_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_test_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_test_item_labels_path)}\n",
    "      - name: link_prediction\n",
    "        num_classes: 10\n",
    "        train_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_train_like_seeds_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_train_follow_seeds_path)}\n",
    "        validation_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_indexes_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_indexes_path)}\n",
    "        test_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_indexes_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: seeds\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_seeds_path)}\n",
    "              - name: labels\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_labels_path)}\n",
    "              - name: indexes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_indexes_path)}\n",
    "\"\"\"\n",
    "metadata_path = os.path.join(base_dir, \"metadata.yaml\")\n",
    "with open(metadata_path, \"w\") as f:\n",
    "  f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEfybHGhOW7O"
   },
   "source": [
    "## Instantiate `OnDiskDataset`\n",
    "Now we're ready to load dataset via `dgl.graphbolt.OnDiskDataset`. When instantiating, we just pass in the base directory where `metadata.yaml` file lies.\n",
    "\n",
    "During first instantiation, GraphBolt preprocesses the raw data such as constructing `FusedCSCSamplingGraph` from edges. All data including graph, feature data, training/validation/test sets are put into `preprocessed` directory after preprocessing. Any following dataset loading will skip the preprocess stage.\n",
    "\n",
    "After preprocessing, `load()` is required to be called explicitly in order to load graph, feature data and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T13:16:04.924372Z",
     "iopub.status.busy": "2024-10-08T13:16:04.924076Z",
     "iopub.status.idle": "2024-10-08T13:16:05.042775Z",
     "shell.execute_reply": "2024-10-08T13:16:05.042180Z"
    },
    "id": "W58CZoSzOiyo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to preprocess the on-disk dataset.\n",
      "Finish preprocessing the on-disk dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph: FusedCSCSamplingGraph(csc_indptr=tensor([    0,     7,    20,  ..., 19981, 19987, 20000], dtype=torch.int32),\n",
      "                      indices=tensor([1061, 1347, 1060,  ..., 1853, 1057, 1462], dtype=torch.int32),\n",
      "                      total_num_nodes=2000, num_edges={'user:follow:user': 10000, 'user:like:item': 10000},\n",
      "                      node_type_offset=tensor([   0, 1000, 2000], dtype=torch.int32),\n",
      "                      type_per_edge=tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.uint8),\n",
      "                      node_type_to_id={'item': 0, 'user': 1},\n",
      "                      edge_type_to_id={'user:follow:user': 0, 'user:like:item': 1},)\n",
      "\n",
      "Loaded feature store: TorchBasedFeatureStore(\n",
      "    {(<OnDiskFeatureDataDomain.NODE: 'node'>, 'user', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.0388, 0.1809, 0.6694, 0.2343, 0.6173],\n",
      "                        [0.1041, 0.9097, 0.0491, 0.0751, 0.9314],\n",
      "                        [0.5052, 0.5260, 0.8382, 0.2452, 0.2341],\n",
      "                        ...,\n",
      "                        [0.6549, 0.4805, 0.8269, 0.0585, 0.4942],\n",
      "                        [0.6037, 0.3739, 0.3181, 0.2126, 0.6285],\n",
      "                        [0.5971, 0.8303, 0.2417, 0.5225, 0.0564]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'user', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.3889, 0.7271, 0.6841, 0.0787, 0.1833],\n",
      "                        [0.7456, 0.5380, 0.1171, 0.0586, 0.9332],\n",
      "                        [0.7691, 0.1819, 0.2899, 0.1092, 0.9930],\n",
      "                        ...,\n",
      "                        [0.5594, 0.5223, 0.1745, 0.6396, 0.1781],\n",
      "                        [0.9603, 0.9440, 0.1808, 0.4081, 0.5882],\n",
      "                        [0.1679, 0.5123, 0.7888, 0.7486, 0.6827]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'item', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.6646, 0.6612, 0.4410, 0.2725, 0.7796],\n",
      "                        [0.2054, 0.6546, 0.1249, 0.1185, 0.2899],\n",
      "                        [0.5145, 0.4043, 0.6550, 0.6530, 0.5135],\n",
      "                        ...,\n",
      "                        [0.7201, 0.3856, 0.1242, 0.8938, 0.6054],\n",
      "                        [0.8616, 0.0438, 0.5382, 0.8913, 0.3149],\n",
      "                        [0.8120, 0.5806, 0.3884, 0.2068, 0.4609]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'item', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.7049, 0.8159, 0.0270, 0.8561, 0.1643],\n",
      "                        [0.4519, 0.0489, 0.8217, 0.1029, 0.2037],\n",
      "                        [0.4429, 0.5648, 0.8133, 0.7929, 0.5278],\n",
      "                        ...,\n",
      "                        [0.2787, 0.1888, 0.1483, 0.7288, 0.9202],\n",
      "                        [0.6528, 0.5656, 0.5677, 0.2330, 0.4707],\n",
      "                        [0.7045, 0.6763, 0.4729, 0.8748, 0.5209]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:like:item', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.6834, 0.4936, 0.9149, 0.2149, 0.4884],\n",
      "                        [0.0524, 0.9432, 0.4448, 0.6677, 0.4240],\n",
      "                        [0.5167, 0.7590, 0.3888, 0.5743, 0.7016],\n",
      "                        ...,\n",
      "                        [0.0858, 0.3240, 0.0329, 0.0542, 0.4433],\n",
      "                        [0.3222, 0.4867, 0.7745, 0.3728, 0.8913],\n",
      "                        [0.3210, 0.0810, 0.3283, 0.3655, 0.1341]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:like:item', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.1656, 0.0723, 0.0628, 0.5738, 0.7722],\n",
      "                        [0.1290, 0.0576, 0.0119, 0.0860, 0.8274],\n",
      "                        [0.4964, 0.0884, 0.2959, 0.3679, 0.7033],\n",
      "                        ...,\n",
      "                        [0.4048, 0.7033, 0.2934, 0.7657, 0.8384],\n",
      "                        [0.4500, 0.4652, 0.6903, 0.5529, 0.3957],\n",
      "                        [0.2844, 0.8615, 0.7332, 0.3650, 0.7446]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:follow:user', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.8217, 0.5536, 0.2938, 0.1837, 0.9324],\n",
      "                        [0.8842, 0.1046, 0.7841, 0.4001, 0.2677],\n",
      "                        [0.3323, 0.2111, 0.1999, 0.8292, 0.0221],\n",
      "                        ...,\n",
      "                        [0.9346, 0.7372, 0.9564, 0.8447, 0.0013],\n",
      "                        [0.1146, 0.6189, 0.8817, 0.4089, 0.3669],\n",
      "                        [0.7379, 0.4268, 0.0257, 0.2049, 0.6355]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:follow:user', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.0631, 0.8269, 0.3711, 0.8949, 0.7186],\n",
      "                        [0.3398, 0.0094, 0.0960, 0.9435, 0.4578],\n",
      "                        [0.9937, 0.2810, 0.9538, 0.4726, 0.6980],\n",
      "                        ...,\n",
      "                        [0.3110, 0.7403, 0.8798, 0.1867, 0.2696],\n",
      "                        [0.8596, 0.1752, 0.8520, 0.9102, 0.2183],\n",
      "                        [0.9074, 0.3943, 0.3046, 0.2096, 0.3571]]),\n",
      "        metadata={},\n",
      "    )}\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded node classification task: OnDiskTask(validation_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([797,  93, 657,   6, 813, 832, 812, 503, 406, 572,   5, 482, 879, 380,\n",
      "                                955, 692, 282,  34, 760, 491, 178, 271, 320, 310, 304, 810, 702, 840,\n",
      "                                148,  84, 412, 708, 400, 733, 532, 759, 478, 805, 494, 365, 101, 966,\n",
      "                                225, 918, 437, 687, 401, 824, 732, 509, 846, 207, 276, 490,  40, 753,\n",
      "                                642, 356, 188, 119, 301,  17, 227, 170, 872,  73, 886,  90,  21, 216,\n",
      "                                468, 979, 411, 358, 446, 641, 612,  11, 531, 512, 780, 898, 417, 635,\n",
      "                                284, 640, 248, 457, 370, 245, 860, 588, 844, 858, 200, 683,  71,  50,\n",
      "                                484, 375, 327, 618, 122, 564, 776, 935, 397, 552, 608, 220, 117, 997,\n",
      "                                127, 539, 425, 153, 264, 162,  80,  62, 938, 705, 346, 751, 495,  95,\n",
      "                                 37, 790, 120, 182, 124, 309,  43,  70, 547, 828, 228, 176, 416, 693,\n",
      "                                724, 982, 890, 501, 986, 510, 336, 212, 432, 383, 793, 839, 508, 164,\n",
      "                                 65, 779, 783, 664, 758, 107, 771, 701, 901, 561, 650, 179, 294, 936,\n",
      "                                265, 449, 255, 847,  22, 764,  66, 688, 922, 236,  54,  51, 573,  38,\n",
      "                                372,  74, 818, 950, 806, 984, 345,  91, 348, 144, 361, 815,  97, 487,\n",
      "                                916,  13, 161,  81], dtype=torch.int32), tensor([4, 3, 3, 2, 9, 5, 1, 3, 9, 7, 8, 8, 7, 3, 7, 6, 6, 8, 4, 8, 7, 2, 1, 7,\n",
      "                                8, 6, 6, 3, 7, 8, 1, 7, 9, 8, 0, 7, 8, 2, 8, 0, 0, 1, 7, 4, 1, 0, 4, 2,\n",
      "                                3, 9, 6, 9, 1, 3, 2, 4, 0, 2, 2, 1, 8, 9, 5, 0, 6, 9, 4, 1, 9, 0, 6, 6,\n",
      "                                3, 3, 7, 7, 6, 7, 1, 3, 5, 8, 6, 8, 4, 9, 2, 8, 4, 3, 6, 6, 7, 5, 0, 3,\n",
      "                                6, 1, 4, 2, 9, 2, 9, 9, 4, 9, 0, 2, 2, 2, 7, 1, 1, 7, 8, 8, 6, 3, 8, 4,\n",
      "                                3, 4, 3, 5, 8, 1, 1, 7, 9, 8, 3, 6, 7, 8, 8, 5, 4, 6, 3, 8, 3, 2, 2, 5,\n",
      "                                5, 7, 8, 5, 7, 4, 8, 7, 2, 0, 0, 6, 2, 9, 1, 2, 9, 4, 3, 2, 3, 1, 3, 6,\n",
      "                                6, 6, 1, 7, 4, 6, 6, 6, 9, 4, 9, 6, 0, 1, 0, 1, 6, 8, 2, 9, 5, 6, 7, 8,\n",
      "                                2, 1, 9, 2, 7, 4, 9, 1])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([584, 924, 156, 724, 206, 931, 428, 236, 886, 810, 181, 876, 879, 500,\n",
      "                                221, 132, 142, 423, 720, 776, 460, 614, 323, 612, 188, 182, 758, 167,\n",
      "                                 49, 890, 355, 929, 920,  69, 539, 781, 827, 641, 581, 984, 345,  55,\n",
      "                                736,  12, 662,  79, 921, 587, 237,  22, 769, 582, 706, 945, 603, 825,\n",
      "                                651, 341, 169, 364,  77, 824, 877, 977, 512, 459, 343, 571, 528, 617,\n",
      "                                850, 219, 834, 277, 298, 215,  28, 467, 741, 744,  85, 223, 232, 416,\n",
      "                                 62, 350,  73, 838, 544, 168, 475,  25, 933, 494, 128,  94, 748, 152,\n",
      "                                 37, 259, 216, 385, 788,  16,  31, 677, 497, 990, 382, 908, 347, 210,\n",
      "                                691, 368, 130, 411, 326, 383, 107, 701, 938, 189, 623, 949, 927, 669,\n",
      "                                695, 766, 708, 118, 220, 487, 780, 922, 265, 956, 209, 388, 604, 868,\n",
      "                                619,  67, 289, 471, 396, 611, 217,  27, 588, 899, 538, 856, 166, 993,\n",
      "                                988, 705,  15, 791, 556, 829, 185, 661, 907, 889, 381, 887, 652, 644,\n",
      "                                594, 377, 968, 247, 468, 304, 558, 745, 117, 557, 291, 960, 894, 678,\n",
      "                                256, 697, 542, 898, 842, 389, 986, 300, 637, 593, 656, 936, 159, 143,\n",
      "                                547, 316, 647, 940], dtype=torch.int32), tensor([9, 5, 8, 3, 0, 3, 5, 1, 1, 5, 9, 4, 3, 9, 7, 4, 8, 2, 9, 1, 0, 1, 5, 6,\n",
      "                                9, 6, 0, 5, 5, 6, 3, 0, 2, 5, 0, 3, 0, 1, 3, 9, 2, 2, 1, 1, 0, 3, 8, 6,\n",
      "                                0, 6, 0, 3, 5, 3, 2, 7, 6, 5, 1, 6, 4, 5, 9, 7, 7, 3, 6, 7, 4, 6, 2, 9,\n",
      "                                2, 5, 8, 7, 9, 5, 3, 1, 0, 5, 9, 2, 9, 3, 1, 1, 4, 2, 7, 2, 5, 7, 7, 2,\n",
      "                                1, 1, 3, 4, 0, 6, 8, 0, 7, 2, 3, 1, 4, 7, 4, 4, 8, 4, 3, 3, 9, 2, 8, 1,\n",
      "                                2, 9, 6, 0, 7, 8, 5, 6, 0, 3, 3, 1, 6, 9, 0, 1, 0, 3, 0, 1, 8, 7, 5, 4,\n",
      "                                3, 3, 1, 9, 5, 2, 7, 0, 0, 1, 6, 1, 4, 5, 6, 0, 5, 3, 7, 4, 9, 5, 5, 3,\n",
      "                                0, 1, 6, 6, 7, 3, 4, 6, 3, 1, 2, 0, 1, 9, 2, 7, 4, 9, 3, 1, 4, 8, 0, 5,\n",
      "                                2, 6, 5, 7, 1, 9, 3, 3])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           train_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([843, 592, 167, 817, 442, 768, 369, 666, 328,  24, 977, 791, 175, 557,\n",
      "                                172, 321, 500, 281, 942, 735, 941, 218, 842, 159, 973, 204, 998, 499,\n",
      "                                466, 292, 186, 784, 755,  78, 822, 165, 191, 974, 929, 206, 527, 352,\n",
      "                                315, 232,  27, 595, 765,  39, 447, 424,  10, 900, 628,  14, 674, 864,\n",
      "                                268, 341, 395,  63, 952, 646, 875, 506,  28, 983,  83, 607, 548, 357,\n",
      "                                154, 889, 496, 747, 334, 486, 471, 969,  94, 928, 277, 703, 934, 270,\n",
      "                                744, 677, 911, 953, 377, 750, 235, 360, 606, 477,  36, 257, 521, 714,\n",
      "                                699, 483, 876, 198, 882, 600, 366, 827, 150,  59, 964, 303,  32, 394,\n",
      "                                388, 139, 529, 267, 696, 992, 379, 238, 568,  76, 636, 623, 409, 993,\n",
      "                                896, 880, 350, 428, 396, 926, 230, 253,  77, 332, 329, 192, 689, 530,\n",
      "                                355, 398, 349, 980,  33, 604, 937, 108, 123, 999, 140, 873, 376, 525,\n",
      "                                109, 434, 823, 981, 914, 180, 994, 256, 917, 408, 152, 788, 874, 581,\n",
      "                                 86, 183, 229, 231, 819, 110, 451, 431, 259, 145, 302, 311, 134, 907,\n",
      "                                330, 838, 715, 596, 958, 599, 299, 792, 234, 189, 295, 787,   9, 912,\n",
      "                                670,  48, 825, 363, 351, 387, 704, 404, 559, 249, 420, 746, 429, 845,\n",
      "                                795, 833, 266, 885, 166, 272, 541, 291, 603,  16,  61, 520, 373, 540,\n",
      "                                772, 749, 742, 116, 391, 190, 807, 318, 671, 141, 444, 526, 578, 659,\n",
      "                                697, 312, 480, 263, 566, 384, 672, 362, 920, 717, 802, 173,   0, 465,\n",
      "                                887, 919, 748, 185, 722, 946, 382, 633, 660, 430, 414, 241, 796, 684,\n",
      "                                571, 324, 222, 754,  56, 970, 645,  92, 686, 563, 685, 104, 649, 313,\n",
      "                                112, 422, 781, 976, 690, 223,  41, 390, 736, 965, 711, 213, 522, 492,\n",
      "                                 75, 720, 632, 865, 734, 634, 855, 317, 538, 567, 851, 461,  58,  42,\n",
      "                                386, 931, 773, 184, 808, 961, 560, 593, 359, 289, 723, 830, 274, 168,\n",
      "                                731, 617, 706,   8,  49, 252, 803, 441, 293, 816, 861, 440, 884, 333,\n",
      "                                129, 177, 143, 763, 513, 427, 135, 786,  46, 242,   4, 800, 157, 662,\n",
      "                                585, 769, 287, 489, 801, 534, 927, 214, 445, 837, 514, 448, 149, 193,\n",
      "                                667, 752, 939, 866, 821, 219,  64, 151, 433, 906, 602, 128,  60, 904,\n",
      "                                631, 103, 987, 211, 367, 995, 799, 594, 716, 243, 707, 556, 105, 111,\n",
      "                                811, 663, 475, 620, 205, 469, 405, 870, 403, 957, 611, 785, 668, 831,\n",
      "                                737, 261, 458, 456, 402, 423, 497, 925, 283, 761, 766, 535, 580, 888,\n",
      "                                575, 555, 859, 695, 710, 392, 587, 627, 485, 331, 389, 863, 344, 712,\n",
      "                                  7, 647, 247,  15, 511, 415, 756, 399, 902, 296, 488, 778, 146, 883,\n",
      "                                729,  18, 621, 280, 954, 340, 381,  99, 826, 968, 956, 464, 319,  53,\n",
      "                                849, 102, 725, 197, 777, 615, 435, 113, 597, 910, 988, 285, 479, 279,\n",
      "                                316,  30, 195, 903, 306, 565, 601, 326, 142, 258, 676, 694, 290,  52,\n",
      "                                745, 718, 297, 163, 551, 542, 347,  79, 721, 975, 570, 202, 726, 126,\n",
      "                                505, 418,  20, 691, 669, 203, 809, 774, 794, 698, 426, 949, 574, 239,\n",
      "                                314, 678, 877, 996, 960, 719, 209, 158, 895, 364,  44, 160, 419, 923,\n",
      "                                948,  55, 681, 728, 554, 848, 262, 273, 498, 298, 913, 169, 700, 943,\n",
      "                                853,  89, 959,  45, 201, 850, 854, 543, 269, 893, 125, 834, 673, 932,\n",
      "                                337, 605, 951, 644, 237,  69, 436, 452, 985, 624, 963, 156, 194, 353,\n",
      "                                868,  26, 275, 590, 385, 217, 651, 881,  96, 891,  67, 762, 474, 410,\n",
      "                                323, 550, 682, 215, 909, 354, 300, 589, 137, 549, 940, 524],\n",
      "                               dtype=torch.int32), tensor([8, 9, 3, 7, 4, 9, 7, 8, 6, 2, 9, 5, 4, 9, 7, 7, 1, 5, 3, 9, 1, 3, 8, 7,\n",
      "                                1, 1, 0, 9, 1, 5, 6, 4, 5, 2, 2, 4, 3, 1, 0, 1, 1, 4, 3, 7, 1, 8, 2, 5,\n",
      "                                3, 9, 8, 7, 7, 4, 0, 3, 3, 7, 8, 9, 4, 0, 3, 3, 0, 5, 3, 5, 5, 8, 9, 7,\n",
      "                                0, 1, 8, 6, 1, 7, 4, 2, 7, 0, 5, 0, 4, 0, 4, 3, 8, 0, 5, 4, 0, 1, 2, 0,\n",
      "                                3, 4, 2, 2, 3, 2, 3, 8, 9, 6, 7, 5, 3, 1, 8, 5, 9, 5, 4, 7, 6, 8, 5, 8,\n",
      "                                7, 3, 4, 1, 7, 6, 4, 8, 2, 3, 9, 2, 9, 6, 4, 0, 7, 6, 9, 3, 4, 0, 8, 9,\n",
      "                                7, 9, 5, 4, 9, 9, 2, 0, 1, 8, 4, 4, 9, 6, 0, 7, 3, 6, 8, 3, 2, 3, 5, 3,\n",
      "                                5, 2, 1, 2, 4, 5, 8, 6, 3, 2, 0, 7, 9, 0, 8, 5, 5, 5, 0, 7, 7, 2, 4, 7,\n",
      "                                3, 1, 5, 8, 4, 2, 9, 9, 9, 0, 6, 1, 4, 2, 5, 2, 8, 1, 4, 8, 2, 0, 4, 0,\n",
      "                                5, 3, 6, 1, 0, 0, 7, 2, 6, 0, 7, 4, 6, 8, 2, 5, 9, 6, 0, 4, 8, 3, 5, 8,\n",
      "                                5, 3, 0, 1, 4, 8, 3, 4, 3, 2, 3, 1, 2, 2, 1, 3, 9, 1, 0, 7, 0, 6, 0, 2,\n",
      "                                9, 0, 5, 4, 1, 2, 5, 6, 6, 5, 9, 1, 1, 7, 0, 1, 2, 5, 8, 4, 5, 2, 1, 1,\n",
      "                                6, 6, 7, 2, 3, 6, 3, 6, 0, 4, 0, 3, 1, 3, 9, 9, 0, 7, 4, 0, 9, 3, 4, 0,\n",
      "                                7, 9, 6, 0, 2, 9, 8, 9, 3, 5, 0, 0, 9, 6, 4, 3, 4, 4, 0, 3, 6, 3, 5, 1,\n",
      "                                6, 6, 5, 3, 0, 6, 5, 3, 3, 6, 9, 9, 0, 2, 9, 1, 2, 5, 2, 5, 4, 6, 1, 6,\n",
      "                                4, 0, 8, 9, 8, 4, 4, 0, 8, 3, 6, 9, 2, 7, 4, 9, 1, 7, 3, 7, 1, 5, 2, 0,\n",
      "                                4, 7, 0, 5, 3, 1, 0, 7, 8, 3, 3, 3, 0, 8, 6, 3, 8, 0, 6, 0, 8, 8, 0, 4,\n",
      "                                6, 9, 3, 8, 3, 1, 1, 1, 5, 3, 0, 2, 1, 8, 1, 8, 8, 7, 5, 4, 6, 1, 4, 2,\n",
      "                                1, 7, 5, 6, 5, 3, 5, 2, 2, 5, 3, 6, 1, 7, 1, 6, 6, 3, 7, 9, 0, 9, 3, 0,\n",
      "                                0, 0, 6, 7, 2, 6, 5, 2, 8, 1, 2, 4, 4, 1, 6, 4, 4, 1, 5, 8, 6, 5, 3, 1,\n",
      "                                3, 3, 0, 6, 1, 6, 5, 5, 1, 2, 7, 2, 5, 5, 0, 0, 2, 4, 6, 9, 4, 8, 5, 9,\n",
      "                                7, 7, 0, 4, 2, 0, 2, 5, 0, 8, 4, 4, 9, 5, 6, 7, 9, 6, 0, 2, 9, 1, 8, 3,\n",
      "                                6, 6, 7, 5, 3, 4, 8, 6, 9, 6, 1, 1, 2, 5, 2, 0, 9, 1, 9, 2, 8, 3, 0, 5,\n",
      "                                7, 2, 2, 5, 1, 0, 3, 6, 5, 4, 4, 9, 1, 7, 9, 0, 7, 2, 2, 2, 3, 0, 7, 4,\n",
      "                                8, 0, 6, 1, 5, 9, 7, 3, 1, 2, 3, 2, 9, 7, 2, 2, 8, 8, 1, 4, 7, 9, 6, 4])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([234, 817, 484, 240,  43, 812, 151, 654, 495, 482, 895, 723, 645, 951,\n",
      "                                583, 433, 862, 802, 716, 414, 845, 865, 444,  36, 796, 835,  80, 585,\n",
      "                                917,  91, 329, 602, 294, 689, 358, 268, 359,  24, 366, 162, 548, 658,\n",
      "                                114, 718, 336, 490, 864, 693, 607, 573, 729, 642, 129, 218, 183, 880,\n",
      "                                569, 671, 251, 306, 160, 925, 516, 173, 488, 108, 242, 970, 609,   8,\n",
      "                                806, 843, 335, 969, 782, 434, 667, 830, 666,  54, 793, 809, 507, 246,\n",
      "                                502, 568, 779, 740, 685, 398, 351, 486, 348, 699, 201, 897, 807, 402,\n",
      "                                688, 554, 852, 665, 422, 657, 896, 426, 106, 869, 972, 111, 370, 404,\n",
      "                                757, 418, 646, 361, 194, 580, 491, 288, 184, 958, 318, 814, 735,  42,\n",
      "                                447, 456, 244, 443,  61, 932, 478, 789, 374, 537, 285, 308, 746, 457,\n",
      "                                504, 412, 849, 971, 750, 694, 509,  40, 884, 698, 737, 979, 882, 144,\n",
      "                                901, 214, 175, 800, 710,   0, 284, 330, 296, 792, 826, 608, 566, 283,\n",
      "                                  9, 624, 371, 332,  44, 872, 451, 613, 713, 873, 751,  38, 941, 914,\n",
      "                                400, 828, 514, 419,  99, 533, 276, 610, 295, 305, 975, 310, 180, 489,\n",
      "                                578, 732, 822, 391, 211, 445, 664, 100, 248, 955, 939, 286, 260, 481,\n",
      "                                709, 784, 785, 520, 893, 649, 472, 801, 913, 437, 560, 673, 974, 923,\n",
      "                                640, 545, 638, 803, 208, 674,  70, 905, 427, 930,  98, 892, 831, 848,\n",
      "                                338, 197, 871, 535, 303, 397,  81,  33, 442, 634,  29, 711, 926,  76,\n",
      "                                904, 249, 597, 174, 605, 590, 357, 963, 700, 561, 263, 928, 337, 193,\n",
      "                                860, 465,   4, 243, 473, 756, 375, 598, 461, 102, 846, 360, 844, 511,\n",
      "                                324, 255, 948, 493, 373,  47, 902, 761, 449, 648, 815, 885, 961, 540,\n",
      "                                510, 496, 601, 150,  90, 238, 883, 186, 754, 752, 808,  46, 957,  63,\n",
      "                                768, 363, 191, 820,  11,  86, 636, 563, 526, 937, 942, 233, 154, 125,\n",
      "                                565, 372,   1, 213, 270, 485, 279,  17, 116, 841, 141, 104,  18, 878,\n",
      "                                 75, 258, 165, 290, 235, 959, 464, 287, 728,  88,   7, 704, 448, 334,\n",
      "                                858, 505, 954,  14, 413, 681, 145, 524, 947, 322, 384, 282,  45, 891,\n",
      "                                572, 944,  13, 439, 431, 241, 309, 469, 349, 980, 424, 409, 109, 912,\n",
      "                                207, 660, 989, 407, 231, 393, 253, 683, 909, 599,  58, 819, 455, 816,\n",
      "                                935, 110, 525, 356, 273,  48, 119, 133, 595, 531, 967, 564, 721, 635,\n",
      "                                498, 425, 430, 135, 994,   2, 576, 680, 342, 742, 749, 519, 331, 446,\n",
      "                                739, 245, 707, 659, 687, 772, 352, 450, 266, 730,  21, 714, 823, 328,\n",
      "                                435, 178, 406, 976, 911, 527, 953, 123, 813, 325, 847, 874, 570, 113,\n",
      "                                 32, 804, 344, 618, 177, 854, 264,   3,  82, 137,  95,  97, 340, 171,\n",
      "                                650, 866, 227, 302, 855, 138, 567,  89, 252, 212,  10, 985, 596,  92,\n",
      "                                606, 134, 616, 725, 228, 589, 840, 380, 158, 149, 454, 470, 559, 799,\n",
      "                                653, 910, 837, 274, 981, 574, 836, 811, 386,  26, 786, 997, 973, 477,\n",
      "                                702, 399, 549, 715, 592, 172, 575, 299, 620, 900, 250, 586, 670,  34,\n",
      "                                229, 112, 555,   6, 712, 833, 267, 962, 420, 354, 523, 861, 795, 726,\n",
      "                                317, 987, 224, 643, 127, 794, 170, 805,  78, 982, 832, 867, 369, 353,\n",
      "                                376, 727, 321,  60, 679,  68, 262, 755, 463,  87,  39, 631, 483, 146,\n",
      "                                200, 222, 513, 532,  50, 261,  57, 770, 676, 773, 226, 686,  51,  74,\n",
      "                                518, 272, 339, 906, 476, 760, 176, 672, 390, 628, 392, 839,  52, 774,\n",
      "                                239, 269, 327, 668, 915, 534, 731, 362, 919, 225, 517, 499],\n",
      "                               dtype=torch.int32), tensor([8, 1, 8, 4, 1, 0, 5, 1, 3, 4, 4, 4, 3, 5, 4, 4, 7, 9, 0, 9, 8, 8, 9, 0,\n",
      "                                8, 5, 7, 5, 8, 5, 9, 9, 9, 4, 8, 0, 4, 5, 5, 0, 2, 4, 8, 5, 3, 0, 0, 1,\n",
      "                                0, 1, 5, 1, 5, 4, 2, 1, 5, 4, 9, 7, 4, 4, 8, 1, 9, 4, 8, 8, 7, 4, 3, 3,\n",
      "                                6, 4, 4, 3, 1, 4, 3, 1, 2, 7, 7, 0, 3, 6, 7, 6, 3, 0, 0, 2, 9, 1, 9, 4,\n",
      "                                3, 9, 6, 6, 3, 7, 4, 5, 0, 5, 4, 1, 5, 2, 2, 8, 8, 3, 3, 4, 6, 6, 3, 6,\n",
      "                                7, 0, 8, 1, 8, 5, 7, 6, 3, 0, 6, 5, 1, 9, 1, 6, 4, 9, 2, 5, 0, 9, 9, 2,\n",
      "                                8, 6, 9, 1, 1, 3, 3, 6, 0, 9, 2, 9, 8, 4, 4, 1, 6, 6, 6, 9, 9, 6, 6, 3,\n",
      "                                1, 9, 2, 0, 2, 7, 3, 3, 1, 7, 4, 3, 1, 9, 9, 2, 6, 3, 9, 6, 6, 0, 3, 9,\n",
      "                                7, 2, 1, 9, 4, 5, 8, 0, 8, 6, 3, 1, 5, 5, 6, 9, 6, 5, 0, 1, 4, 4, 0, 9,\n",
      "                                9, 6, 7, 7, 9, 4, 0, 1, 5, 0, 2, 4, 0, 9, 4, 4, 9, 4, 6, 9, 2, 7, 5, 2,\n",
      "                                4, 8, 1, 3, 3, 5, 3, 3, 1, 3, 9, 0, 2, 0, 7, 2, 4, 2, 6, 6, 6, 5, 1, 4,\n",
      "                                5, 9, 8, 0, 3, 9, 8, 2, 6, 6, 9, 4, 4, 4, 6, 5, 1, 7, 8, 8, 4, 6, 3, 0,\n",
      "                                1, 1, 7, 5, 5, 2, 1, 8, 5, 3, 3, 8, 3, 9, 5, 4, 5, 1, 6, 1, 9, 5, 1, 2,\n",
      "                                3, 8, 5, 6, 5, 3, 1, 6, 6, 5, 1, 1, 0, 9, 4, 7, 3, 9, 9, 7, 4, 1, 7, 9,\n",
      "                                1, 4, 7, 8, 7, 9, 5, 7, 2, 8, 2, 3, 2, 6, 2, 7, 8, 4, 6, 8, 9, 4, 7, 3,\n",
      "                                0, 4, 5, 4, 1, 7, 2, 1, 6, 4, 4, 9, 1, 7, 4, 3, 5, 4, 2, 8, 5, 4, 6, 6,\n",
      "                                2, 3, 5, 9, 7, 7, 5, 1, 8, 3, 8, 4, 8, 0, 0, 6, 3, 8, 9, 8, 8, 7, 2, 4,\n",
      "                                4, 9, 4, 4, 9, 6, 8, 4, 7, 5, 9, 8, 6, 6, 0, 8, 4, 9, 4, 2, 9, 8, 0, 8,\n",
      "                                3, 1, 2, 9, 2, 4, 7, 3, 5, 4, 4, 8, 1, 7, 8, 8, 6, 8, 1, 9, 6, 6, 1, 7,\n",
      "                                0, 4, 4, 5, 7, 4, 7, 9, 9, 6, 8, 9, 0, 3, 8, 6, 1, 8, 5, 1, 3, 0, 9, 5,\n",
      "                                2, 7, 1, 4, 7, 2, 3, 1, 3, 3, 8, 2, 1, 8, 6, 2, 9, 4, 3, 2, 8, 6, 6, 5,\n",
      "                                9, 7, 6, 6, 2, 4, 2, 8, 8, 0, 7, 9, 9, 9, 7, 8, 5, 6, 3, 3, 1, 1, 3, 1,\n",
      "                                4, 3, 0, 1, 8, 5, 8, 4, 4, 4, 2, 6, 1, 1, 4, 0, 7, 8, 4, 5, 0, 7, 2, 6,\n",
      "                                8, 5, 8, 4, 6, 7, 3, 8, 1, 3, 5, 1, 8, 6, 5, 9, 2, 7, 4, 4, 2, 0, 1, 8,\n",
      "                                6, 0, 6, 8, 1, 7, 9, 4, 5, 1, 5, 9, 2, 2, 9, 5, 4, 8, 5, 7, 5, 4, 6, 6])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           test_set=HeteroItemSet(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([921, 226, 743,  85, 841, 368, 233, 767, 782, 648, 638, 990,  57, 989,\n",
      "                                814, 338, 407, 454, 260, 438, 210, 254, 897, 713, 789, 639, 507,  23,\n",
      "                                307, 244,  19, 138, 930, 613, 518,  88, 106,  87,  29, 675, 804, 852,\n",
      "                                610, 798, 653, 727, 899, 738, 517, 616, 467,  31, 470, 558, 740, 836,\n",
      "                                196, 562,  82, 820, 622, 199, 325, 502, 867, 246, 775, 421, 459, 770,\n",
      "                                378, 944,  98,  68, 371, 450, 625, 741, 892, 114, 637,  12, 978, 626,\n",
      "                                652,  35, 171, 472, 579, 679, 656, 131, 583, 739, 730, 545, 115, 945,\n",
      "                                224, 908, 251, 130, 630, 132, 460, 584, 121, 537, 516, 523, 305, 967,\n",
      "                                533, 862, 661, 481, 133, 515,   2, 643, 544, 894, 342, 473, 991, 343,\n",
      "                                462, 869, 933, 519, 582,  47, 155, 187, 598, 443, 463, 871, 654, 972,\n",
      "                                962, 136, 629, 569, 413, 278, 878, 476, 208, 339, 924, 829, 288, 680,\n",
      "                                553, 905, 577, 835, 655, 322, 546,   1, 393, 591, 609, 308, 455, 757,\n",
      "                                453, 504, 493, 536, 286, 856, 586, 118, 374, 709, 665, 614, 947, 147,\n",
      "                                619, 100, 857,  72, 658,  25, 174, 576, 335, 915, 971, 221, 240,   3,\n",
      "                                439, 181, 528, 250], dtype=torch.int32), tensor([8, 4, 8, 6, 0, 5, 9, 2, 3, 4, 4, 1, 7, 1, 3, 3, 6, 9, 1, 1, 7, 7, 1, 0,\n",
      "                                7, 2, 3, 9, 6, 8, 2, 3, 3, 0, 7, 1, 6, 4, 2, 9, 8, 9, 5, 1, 2, 2, 8, 9,\n",
      "                                6, 5, 1, 5, 2, 6, 9, 1, 3, 0, 8, 9, 5, 1, 9, 3, 2, 1, 5, 7, 5, 1, 9, 6,\n",
      "                                6, 6, 5, 5, 6, 4, 1, 4, 9, 4, 7, 1, 1, 1, 0, 5, 6, 1, 4, 1, 2, 0, 8, 8,\n",
      "                                8, 4, 0, 9, 6, 9, 9, 8, 9, 7, 8, 3, 8, 3, 5, 6, 4, 9, 0, 7, 5, 9, 6, 9,\n",
      "                                3, 9, 1, 3, 0, 4, 2, 5, 4, 8, 9, 8, 5, 8, 5, 3, 9, 1, 8, 2, 1, 9, 0, 8,\n",
      "                                6, 3, 3, 2, 0, 2, 8, 8, 5, 8, 8, 1, 0, 0, 2, 1, 5, 2, 2, 1, 7, 5, 4, 7,\n",
      "                                8, 2, 7, 1, 3, 5, 0, 1, 5, 3, 3, 9, 1, 4, 4, 5, 8, 5, 6, 1, 3, 0, 6, 1,\n",
      "                                8, 8, 5, 5, 6, 2, 6, 4])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([682, 627, 546, 622,  72, 508, 292,  30, 767, 983, 311, 415, 346, 387,\n",
      "                                625, 187, 692,  56, 629, 281, 630,   5, 313, 764, 777, 562, 441, 147,\n",
      "                                543, 765, 663, 320, 297, 615, 395, 121, 271, 148, 190, 521, 790, 278,\n",
      "                                453, 717, 452, 204,  83, 195, 684, 733, 198, 202, 155,  59, 124, 753,\n",
      "                                120, 139, 738, 312, 863, 140, 126, 903, 964, 888, 196, 466, 410, 280,\n",
      "                                101, 462, 946, 992, 474,  64, 438, 696, 918, 405, 199,  71, 743, 429,\n",
      "                                771, 394, 632, 916,  20, 307, 319,  23, 579, 591, 550,  53, 315, 703,\n",
      "                                722, 851,  84, 275, 529, 633, 996, 952, 163, 541, 859, 821, 164, 943,\n",
      "                                577, 991, 552, 103, 192, 314,  66, 950, 747, 881, 853, 783, 522, 797,\n",
      "                                870, 515, 690, 775, 501, 857,  96, 157, 818, 408, 436, 479, 115, 421,\n",
      "                                301, 203,  35,  41, 131, 293, 254, 734, 551, 639, 432, 367, 934, 600,\n",
      "                                675, 153, 205, 257, 440, 999, 333, 503, 536, 403, 530, 719, 762, 417,\n",
      "                                122, 161, 787, 966,  93, 875, 978, 995, 626, 621, 778, 553, 230, 458,\n",
      "                                965, 655, 480, 105, 798, 759, 379, 492,  19,  65, 136, 763, 378, 506,\n",
      "                                179, 998, 365, 401], dtype=torch.int32), tensor([2, 8, 0, 2, 9, 7, 4, 1, 9, 0, 7, 7, 4, 2, 9, 2, 8, 0, 0, 0, 8, 5, 9, 4,\n",
      "                                8, 9, 5, 0, 8, 9, 3, 2, 2, 6, 1, 8, 7, 4, 5, 4, 1, 2, 8, 7, 9, 6, 0, 3,\n",
      "                                8, 5, 8, 3, 4, 5, 1, 2, 0, 5, 1, 3, 3, 8, 5, 0, 5, 1, 0, 9, 9, 5, 6, 5,\n",
      "                                4, 1, 6, 1, 7, 5, 3, 9, 9, 5, 3, 3, 1, 7, 6, 5, 8, 8, 3, 8, 9, 1, 6, 6,\n",
      "                                0, 0, 2, 1, 6, 0, 2, 9, 7, 1, 3, 1, 0, 8, 8, 0, 1, 3, 6, 0, 7, 7, 3, 6,\n",
      "                                7, 5, 9, 5, 7, 3, 2, 7, 4, 7, 1, 5, 1, 3, 9, 5, 6, 4, 2, 7, 0, 9, 7, 4,\n",
      "                                3, 7, 4, 8, 8, 1, 7, 3, 8, 3, 6, 3, 0, 3, 7, 0, 4, 2, 0, 2, 2, 6, 0, 2,\n",
      "                                3, 5, 4, 9, 8, 8, 9, 3, 7, 8, 1, 9, 6, 0, 3, 1, 4, 2, 2, 6, 1, 8, 0, 0,\n",
      "                                4, 3, 4, 1, 9, 1, 6, 9])),\n",
      "                            names=('seeds', 'labels'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels'),\n",
      "           ),\n",
      "           metadata={'name': 'node_classification', 'num_classes': 10},)\n",
      "\n",
      "Loaded link prediction task: OnDiskTask(validation_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[827, 150],\n",
      "                                [739,  86],\n",
      "                                [ 21, 811],\n",
      "                                ...,\n",
      "                                [784, 785],\n",
      "                                [784, 940],\n",
      "                                [784, 414]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[106, 414],\n",
      "                                [108, 276],\n",
      "                                [ 84, 764],\n",
      "                                ...,\n",
      "                                [918,  44],\n",
      "                                [918, 594],\n",
      "                                [918, 102]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels', 'indexes'),\n",
      "           ),\n",
      "           train_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[ 91,  50],\n",
      "                                [535, 818],\n",
      "                                [936, 684],\n",
      "                                ...,\n",
      "                                [535, 644],\n",
      "                                [239, 602],\n",
      "                                [152, 646]], dtype=torch.int32),),\n",
      "                            names=('seeds',),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[821, 387],\n",
      "                                [259, 712],\n",
      "                                [ 99, 380],\n",
      "                                ...,\n",
      "                                [597, 595],\n",
      "                                [278, 450],\n",
      "                                [ 22, 837]], dtype=torch.int32),),\n",
      "                            names=('seeds',),\n",
      "                        )},\n",
      "               names=('seeds',),\n",
      "           ),\n",
      "           test_set=HeteroItemSet(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[873, 464],\n",
      "                                [521,  46],\n",
      "                                [147, 280],\n",
      "                                ...,\n",
      "                                [137, 101],\n",
      "                                [137, 760],\n",
      "                                [137, 494]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[350, 862],\n",
      "                                [912, 711],\n",
      "                                [894, 220],\n",
      "                                ...,\n",
      "                                [598, 639],\n",
      "                                [598, 734],\n",
      "                                [598, 750]], dtype=torch.int32), tensor([1., 1., 1.,  ..., 0., 0., 0.], dtype=torch.float64), tensor([   0,    1,    2,  ..., 1999, 1999, 1999])),\n",
      "                            names=('seeds', 'labels', 'indexes'),\n",
      "                        )},\n",
      "               names=('seeds', 'labels', 'indexes'),\n",
      "           ),\n",
      "           metadata={'name': 'link_prediction', 'num_classes': 10},)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dgl/python/dgl/graphbolt/impl/ondisk_dataset.py:463: GBWarning: Edge feature is stored, but edge IDs are not saved.\n",
      "  gb_warning(\"Edge feature is stored, but edge IDs are not saved.\")\n"
     ]
    }
   ],
   "source": [
    "dataset = gb.OnDiskDataset(base_dir).load()\n",
    "graph = dataset.graph\n",
    "print(f\"Loaded graph: {graph}\\n\")\n",
    "\n",
    "feature = dataset.feature\n",
    "print(f\"Loaded feature store: {feature}\\n\")\n",
    "\n",
    "tasks = dataset.tasks\n",
    "nc_task = tasks[0]\n",
    "print(f\"Loaded node classification task: {nc_task}\\n\")\n",
    "lp_task = tasks[1]\n",
    "print(f\"Loaded link prediction task: {lp_task}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
