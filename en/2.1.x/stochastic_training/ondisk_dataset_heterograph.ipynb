{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnFhPMaAfLtJ"
   },
   "source": [
    "# OnDiskDataset for Heterogeneous Graph\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/stochastic_training/ondisk_dataset_heterograph.ipynb) [![GitHub](https://img.shields.io/badge/-View%20on%20GitHub-181717?logo=github&logoColor=ffffff)](https://github.com/dmlc/dgl/blob/master/notebooks/stochastic_training/ondisk_dataset_heterograph.ipynb)\n",
    "\n",
    "This tutorial shows how to create `OnDiskDataset` for heterogeneous graph that could be used in **GraphBolt** framework. The major difference from creating dataset for homogeneous graph is that we need to specify node/edge types for edges, feature data, training/validation/test sets.\n",
    "\n",
    "By the end of this tutorial, you will be able to\n",
    "\n",
    "- organize graph structure data.\n",
    "- organize feature data.\n",
    "- organize training/validation/test set for specific tasks.\n",
    "\n",
    "To create an ``OnDiskDataset`` object, you need to organize all the data including graph structure, feature data and tasks into a directory. The directory should contain a ``metadata.yaml`` file that describes the metadata of the dataset.\n",
    "\n",
    "Now let's generate various data step by step and organize them together to instantiate `OnDiskDataset` finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlb19DtWgtzq"
   },
   "source": [
    "## Install DGL package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T05:16:31.845517Z",
     "iopub.status.busy": "2024-09-02T05:16:31.845215Z",
     "iopub.status.idle": "2024-09-02T05:16:35.587229Z",
     "shell.execute_reply": "2024-09-02T05:16:35.586474Z"
    },
    "id": "UojlT9ZGgyr9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels-test/repo.html\r\n",
      "Requirement already satisfied: dgl in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (2.2a240410)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (3.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (4.66.5)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (6.0.0)\r\n",
      "Requirement already satisfied: torchdata>=0.5.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (0.7.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from dgl) (2.2.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.8)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2024.7.4)\r\n",
      "Requirement already satisfied: torch>=2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torchdata>=0.5.0->dgl) (2.4.0+cpu)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from pandas->dgl) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.15.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.13.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.6.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/dgl-dev-cpu/lib/python3.10/site-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGL installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "os.environ['DGLBACKEND'] = \"pytorch\"\n",
    "\n",
    "# Install the CPU version.\n",
    "device = torch.device(\"cpu\")\n",
    "!pip install --pre dgl -f https://data.dgl.ai/wheels-test/repo.html\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    import dgl.graphbolt as gb\n",
    "    installed = True\n",
    "except ImportError as error:\n",
    "    installed = False\n",
    "    print(error)\n",
    "print(\"DGL installed!\" if installed else \"DGL not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2R7WnSbjsfbr"
   },
   "source": [
    "## Data preparation\n",
    "In order to demonstrate how to organize various data, let's create a base directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T05:16:35.590175Z",
     "iopub.status.busy": "2024-09-02T05:16:35.589697Z",
     "iopub.status.idle": "2024-09-02T05:16:35.593720Z",
     "shell.execute_reply": "2024-09-02T05:16:35.593091Z"
    },
    "id": "SZipbzyltLfO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created base directory: ./ondisk_dataset_heterograph\n"
     ]
    }
   ],
   "source": [
    "base_dir = './ondisk_dataset_heterograph'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "print(f\"Created base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhNtIn_xhlnl"
   },
   "source": [
    "### Generate graph structure data\n",
    "For heterogeneous graph, we need to save different edge edges(namely node pairs) into separate **Numpy** or **CSV** files.\n",
    "\n",
    "Note:\n",
    "- when saving to **Numpy**, the array requires to be in shape of `(2, N)`. This format is recommended as constructing graph from it is much faster than **CSV** file.\n",
    "- when saving to **CSV** file, do not save index and header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T05:16:35.596290Z",
     "iopub.status.busy": "2024-09-02T05:16:35.595806Z",
     "iopub.status.idle": "2024-09-02T05:16:35.616315Z",
     "shell.execute_reply": "2024-09-02T05:16:35.615710Z"
    },
    "id": "HcBt4G5BmSjr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of [user:like:item] edges: [[158 444]\n",
      " [838 685]\n",
      " [984 317]\n",
      " [461 207]\n",
      " [597 153]]\n",
      "\n",
      "[user:like:item] edges are saved into ./ondisk_dataset_heterograph/like-edges.csv\n",
      "\n",
      "Part of [user:follow:user] edges: [[ 11 245]\n",
      " [843 699]\n",
      " [884 218]\n",
      " [312 678]\n",
      " [188  74]]\n",
      "\n",
      "[user:follow:user] edges are saved into ./ondisk_dataset_heterograph/follow-edges.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For simplicity, we create a heterogeneous graph with\n",
    "# 2 node types: `user`, `item`\n",
    "# 2 edge types: `user:like:item`, `user:follow:user`\n",
    "# And each node/edge type has the same number of nodes/edges.\n",
    "num_nodes = 1000\n",
    "num_edges = 10 * num_nodes\n",
    "\n",
    "# Edge type: \"user:like:item\"\n",
    "like_edges_path = os.path.join(base_dir, \"like-edges.csv\")\n",
    "like_edges = np.random.randint(0, num_nodes, size=(num_edges, 2))\n",
    "print(f\"Part of [user:like:item] edges: {like_edges[:5, :]}\\n\")\n",
    "\n",
    "df = pd.DataFrame(like_edges)\n",
    "df.to_csv(like_edges_path, index=False, header=False)\n",
    "print(f\"[user:like:item] edges are saved into {like_edges_path}\\n\")\n",
    "\n",
    "# Edge type: \"user:follow:user\"\n",
    "follow_edges_path = os.path.join(base_dir, \"follow-edges.csv\")\n",
    "follow_edges = np.random.randint(0, num_nodes, size=(num_edges, 2))\n",
    "print(f\"Part of [user:follow:user] edges: {follow_edges[:5, :]}\\n\")\n",
    "\n",
    "df = pd.DataFrame(follow_edges)\n",
    "df.to_csv(follow_edges_path, index=False, header=False)\n",
    "print(f\"[user:follow:user] edges are saved into {follow_edges_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh-4cPtzpcaH"
   },
   "source": [
    "### Generate feature data for graph\n",
    "For feature data, numpy arrays and torch tensors are supported for now. Let's generate feature data for each node/edge type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T05:16:35.618894Z",
     "iopub.status.busy": "2024-09-02T05:16:35.618448Z",
     "iopub.status.idle": "2024-09-02T05:16:35.654719Z",
     "shell.execute_reply": "2024-09-02T05:16:35.654232Z"
    },
    "id": "_PVu1u5brBhF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of node[user] feature [feat_0]: [[0.38549653 0.64152928 0.30168674 0.14756797 0.66411125]\n",
      " [0.99240271 0.07772828 0.46538016 0.90870363 0.91559338]\n",
      " [0.06630096 0.78015702 0.92557002 0.52237759 0.35335571]]\n",
      "Node[user] feature [feat_0] is saved to ./ondisk_dataset_heterograph/node-user-feat-0.npy\n",
      "\n",
      "Part of node[user] feature [feat_1]: tensor([[0.7100, 0.2405, 0.2885, 0.4496, 0.9730],\n",
      "        [0.2667, 0.6266, 0.0149, 0.3903, 0.2006],\n",
      "        [0.9206, 0.2421, 0.4535, 0.2017, 0.2078]])\n",
      "Node[user] feature [feat_1] is saved to ./ondisk_dataset_heterograph/node-user-feat-1.pt\n",
      "\n",
      "Part of node[item] feature [feat_0]: [[0.03853216 0.30399021 0.23173685 0.6738901  0.33552533]\n",
      " [0.62233571 0.8091129  0.27524689 0.41953151 0.36606632]\n",
      " [0.44783659 0.96913945 0.34269439 0.24441943 0.09341842]]\n",
      "Node[item] feature [feat_0] is saved to ./ondisk_dataset_heterograph/node-item-feat-0.npy\n",
      "\n",
      "Part of node[item] feature [feat_1]: tensor([[0.2070, 0.2580, 0.2705, 0.3839, 0.9433],\n",
      "        [0.4090, 0.0882, 0.6113, 0.3062, 0.5593],\n",
      "        [0.2406, 0.0973, 0.4892, 0.7453, 0.3034]])\n",
      "Node[item] feature [feat_1] is saved to ./ondisk_dataset_heterograph/node-item-feat-1.pt\n",
      "\n",
      "Part of edge[user:like:item] feature [feat_0]: [[0.42610046 0.70038271 0.64484884 0.67660132 0.66856329]\n",
      " [0.18783392 0.03942026 0.90930719 0.61422993 0.07829528]\n",
      " [0.64839341 0.18134428 0.70990316 0.33549489 0.5700555 ]]\n",
      "Edge[user:like:item] feature [feat_0] is saved to ./ondisk_dataset_heterograph/edge-like-feat-0.npy\n",
      "\n",
      "Part of edge[user:like:item] feature [feat_1]: tensor([[0.4201, 0.3138, 0.3399, 0.7463, 0.6212],\n",
      "        [0.2006, 0.6508, 0.5889, 0.8474, 0.0381],\n",
      "        [0.5975, 0.7955, 0.2292, 0.9512, 0.2572]])\n",
      "Edge[user:like:item] feature [feat_1] is saved to ./ondisk_dataset_heterograph/edge-like-feat-1.pt\n",
      "\n",
      "Part of edge[user:follow:user] feature [feat_0]: [[0.59206141 0.76801255 0.38179972 0.15644782 0.56156206]\n",
      " [0.75962405 0.80767067 0.48233712 0.75052389 0.93516832]\n",
      " [0.82097706 0.46359794 0.85128149 0.85929186 0.52933747]]\n",
      "Edge[user:follow:user] feature [feat_0] is saved to ./ondisk_dataset_heterograph/edge-follow-feat-0.npy\n",
      "\n",
      "Part of edge[user:follow:user] feature [feat_1]: tensor([[0.6529, 0.1965, 0.4648, 0.1633, 0.2976],\n",
      "        [0.2142, 0.6420, 0.8123, 0.3233, 0.0410],\n",
      "        [0.5288, 0.3592, 0.2735, 0.9079, 0.5376]])\n",
      "Edge[user:follow:user] feature [feat_1] is saved to ./ondisk_dataset_heterograph/edge-follow-feat-1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate node[user] feature in numpy array.\n",
    "node_user_feat_0_path = os.path.join(base_dir, \"node-user-feat-0.npy\")\n",
    "node_user_feat_0 = np.random.rand(num_nodes, 5)\n",
    "print(f\"Part of node[user] feature [feat_0]: {node_user_feat_0[:3, :]}\")\n",
    "np.save(node_user_feat_0_path, node_user_feat_0)\n",
    "print(f\"Node[user] feature [feat_0] is saved to {node_user_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another node[user] feature in torch tensor\n",
    "node_user_feat_1_path = os.path.join(base_dir, \"node-user-feat-1.pt\")\n",
    "node_user_feat_1 = torch.rand(num_nodes, 5)\n",
    "print(f\"Part of node[user] feature [feat_1]: {node_user_feat_1[:3, :]}\")\n",
    "torch.save(node_user_feat_1, node_user_feat_1_path)\n",
    "print(f\"Node[user] feature [feat_1] is saved to {node_user_feat_1_path}\\n\")\n",
    "\n",
    "# Generate node[item] feature in numpy array.\n",
    "node_item_feat_0_path = os.path.join(base_dir, \"node-item-feat-0.npy\")\n",
    "node_item_feat_0 = np.random.rand(num_nodes, 5)\n",
    "print(f\"Part of node[item] feature [feat_0]: {node_item_feat_0[:3, :]}\")\n",
    "np.save(node_item_feat_0_path, node_item_feat_0)\n",
    "print(f\"Node[item] feature [feat_0] is saved to {node_item_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another node[item] feature in torch tensor\n",
    "node_item_feat_1_path = os.path.join(base_dir, \"node-item-feat-1.pt\")\n",
    "node_item_feat_1 = torch.rand(num_nodes, 5)\n",
    "print(f\"Part of node[item] feature [feat_1]: {node_item_feat_1[:3, :]}\")\n",
    "torch.save(node_item_feat_1, node_item_feat_1_path)\n",
    "print(f\"Node[item] feature [feat_1] is saved to {node_item_feat_1_path}\\n\")\n",
    "\n",
    "# Generate edge[user:like:item] feature in numpy array.\n",
    "edge_like_feat_0_path = os.path.join(base_dir, \"edge-like-feat-0.npy\")\n",
    "edge_like_feat_0 = np.random.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:like:item] feature [feat_0]: {edge_like_feat_0[:3, :]}\")\n",
    "np.save(edge_like_feat_0_path, edge_like_feat_0)\n",
    "print(f\"Edge[user:like:item] feature [feat_0] is saved to {edge_like_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another edge[user:like:item] feature in torch tensor\n",
    "edge_like_feat_1_path = os.path.join(base_dir, \"edge-like-feat-1.pt\")\n",
    "edge_like_feat_1 = torch.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:like:item] feature [feat_1]: {edge_like_feat_1[:3, :]}\")\n",
    "torch.save(edge_like_feat_1, edge_like_feat_1_path)\n",
    "print(f\"Edge[user:like:item] feature [feat_1] is saved to {edge_like_feat_1_path}\\n\")\n",
    "\n",
    "# Generate edge[user:follow:user] feature in numpy array.\n",
    "edge_follow_feat_0_path = os.path.join(base_dir, \"edge-follow-feat-0.npy\")\n",
    "edge_follow_feat_0 = np.random.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:follow:user] feature [feat_0]: {edge_follow_feat_0[:3, :]}\")\n",
    "np.save(edge_follow_feat_0_path, edge_follow_feat_0)\n",
    "print(f\"Edge[user:follow:user] feature [feat_0] is saved to {edge_follow_feat_0_path}\\n\")\n",
    "\n",
    "# Generate another edge[user:follow:user] feature in torch tensor\n",
    "edge_follow_feat_1_path = os.path.join(base_dir, \"edge-follow-feat-1.pt\")\n",
    "edge_follow_feat_1 = torch.rand(num_edges, 5)\n",
    "print(f\"Part of edge[user:follow:user] feature [feat_1]: {edge_follow_feat_1[:3, :]}\")\n",
    "torch.save(edge_follow_feat_1, edge_follow_feat_1_path)\n",
    "print(f\"Edge[user:follow:user] feature [feat_1] is saved to {edge_follow_feat_1_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyqgOtsIwzh_"
   },
   "source": [
    "### Generate tasks\n",
    "`OnDiskDataset` supports multiple tasks. For each task, we need to prepare training/validation/test sets respectively. Such sets usually vary among different tasks. In this tutorial, let's create a **Node Classification** task and **Link Prediction** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVxHaDIfzCkr"
   },
   "source": [
    "#### Node Classification Task\n",
    "For node classification task, we need **node IDs** and corresponding **labels** for each training/validation/test set. Like feature data, numpy arrays and torch tensors are supported for these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T05:16:35.657109Z",
     "iopub.status.busy": "2024-09-02T05:16:35.656752Z",
     "iopub.status.idle": "2024-09-02T05:16:35.672989Z",
     "shell.execute_reply": "2024-09-02T05:16:35.672504Z"
    },
    "id": "S5-fyBbHzTCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of train ids[user] for node classification: [783   6 425]\n",
      "NC train ids[user] are saved to ./ondisk_dataset_heterograph/nc-train-user-ids.npy\n",
      "\n",
      "Part of train labels[user] for node classification: tensor([4, 4, 2])\n",
      "NC train labels[user] are saved to ./ondisk_dataset_heterograph/nc-train-user-labels.pt\n",
      "\n",
      "Part of train ids[item] for node classification: [631 735 695]\n",
      "NC train ids[item] are saved to ./ondisk_dataset_heterograph/nc-train-item-ids.npy\n",
      "\n",
      "Part of train labels[item] for node classification: tensor([9, 8, 5])\n",
      "NC train labels[item] are saved to ./ondisk_dataset_heterograph/nc-train-item-labels.pt\n",
      "\n",
      "Part of val ids[user] for node classification: [240 257 128]\n",
      "NC val ids[user] are saved to ./ondisk_dataset_heterograph/nc-val-user-ids.npy\n",
      "\n",
      "Part of val labels[user] for node classification: tensor([8, 5, 6])\n",
      "NC val labels[user] are saved to ./ondisk_dataset_heterograph/nc-val-user-labels.pt\n",
      "\n",
      "Part of val ids[item] for node classification: [442 572 102]\n",
      "NC val ids[item] are saved to ./ondisk_dataset_heterograph/nc-val-item-ids.npy\n",
      "\n",
      "Part of val labels[item] for node classification: tensor([3, 3, 2])\n",
      "NC val labels[item] are saved to ./ondisk_dataset_heterograph/nc-val-item-labels.pt\n",
      "\n",
      "Part of test ids[user] for node classification: [187 460 630]\n",
      "NC test ids[user] are saved to ./ondisk_dataset_heterograph/nc-test-user-ids.npy\n",
      "\n",
      "Part of test labels[user] for node classification: tensor([0, 1, 9])\n",
      "NC test labels[user] are saved to ./ondisk_dataset_heterograph/nc-test-user-labels.pt\n",
      "\n",
      "Part of test ids[item] for node classification: [218 317 772]\n",
      "NC test ids[item] are saved to ./ondisk_dataset_heterograph/nc-test-item-ids.npy\n",
      "\n",
      "Part of test labels[item] for node classification: tensor([2, 2, 6])\n",
      "NC test labels[item] are saved to ./ondisk_dataset_heterograph/nc-test-item-labels.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration, let's generate item sets for each node type.\n",
    "num_trains = int(num_nodes * 0.6)\n",
    "num_vals = int(num_nodes * 0.2)\n",
    "num_tests = num_nodes - num_trains - num_vals\n",
    "\n",
    "user_ids = np.arange(num_nodes)\n",
    "np.random.shuffle(user_ids)\n",
    "\n",
    "item_ids = np.arange(num_nodes)\n",
    "np.random.shuffle(item_ids)\n",
    "\n",
    "# Train IDs for user.\n",
    "nc_train_user_ids_path = os.path.join(base_dir, \"nc-train-user-ids.npy\")\n",
    "nc_train_user_ids = user_ids[:num_trains]\n",
    "print(f\"Part of train ids[user] for node classification: {nc_train_user_ids[:3]}\")\n",
    "np.save(nc_train_user_ids_path, nc_train_user_ids)\n",
    "print(f\"NC train ids[user] are saved to {nc_train_user_ids_path}\\n\")\n",
    "\n",
    "# Train labels for user.\n",
    "nc_train_user_labels_path = os.path.join(base_dir, \"nc-train-user-labels.pt\")\n",
    "nc_train_user_labels = torch.randint(0, 10, (num_trains,))\n",
    "print(f\"Part of train labels[user] for node classification: {nc_train_user_labels[:3]}\")\n",
    "torch.save(nc_train_user_labels, nc_train_user_labels_path)\n",
    "print(f\"NC train labels[user] are saved to {nc_train_user_labels_path}\\n\")\n",
    "\n",
    "# Train IDs for item.\n",
    "nc_train_item_ids_path = os.path.join(base_dir, \"nc-train-item-ids.npy\")\n",
    "nc_train_item_ids = item_ids[:num_trains]\n",
    "print(f\"Part of train ids[item] for node classification: {nc_train_item_ids[:3]}\")\n",
    "np.save(nc_train_item_ids_path, nc_train_item_ids)\n",
    "print(f\"NC train ids[item] are saved to {nc_train_item_ids_path}\\n\")\n",
    "\n",
    "# Train labels for item.\n",
    "nc_train_item_labels_path = os.path.join(base_dir, \"nc-train-item-labels.pt\")\n",
    "nc_train_item_labels = torch.randint(0, 10, (num_trains,))\n",
    "print(f\"Part of train labels[item] for node classification: {nc_train_item_labels[:3]}\")\n",
    "torch.save(nc_train_item_labels, nc_train_item_labels_path)\n",
    "print(f\"NC train labels[item] are saved to {nc_train_item_labels_path}\\n\")\n",
    "\n",
    "# Val IDs for user.\n",
    "nc_val_user_ids_path = os.path.join(base_dir, \"nc-val-user-ids.npy\")\n",
    "nc_val_user_ids = user_ids[num_trains:num_trains+num_vals]\n",
    "print(f\"Part of val ids[user] for node classification: {nc_val_user_ids[:3]}\")\n",
    "np.save(nc_val_user_ids_path, nc_val_user_ids)\n",
    "print(f\"NC val ids[user] are saved to {nc_val_user_ids_path}\\n\")\n",
    "\n",
    "# Val labels for user.\n",
    "nc_val_user_labels_path = os.path.join(base_dir, \"nc-val-user-labels.pt\")\n",
    "nc_val_user_labels = torch.randint(0, 10, (num_vals,))\n",
    "print(f\"Part of val labels[user] for node classification: {nc_val_user_labels[:3]}\")\n",
    "torch.save(nc_val_user_labels, nc_val_user_labels_path)\n",
    "print(f\"NC val labels[user] are saved to {nc_val_user_labels_path}\\n\")\n",
    "\n",
    "# Val IDs for item.\n",
    "nc_val_item_ids_path = os.path.join(base_dir, \"nc-val-item-ids.npy\")\n",
    "nc_val_item_ids = item_ids[num_trains:num_trains+num_vals]\n",
    "print(f\"Part of val ids[item] for node classification: {nc_val_item_ids[:3]}\")\n",
    "np.save(nc_val_item_ids_path, nc_val_item_ids)\n",
    "print(f\"NC val ids[item] are saved to {nc_val_item_ids_path}\\n\")\n",
    "\n",
    "# Val labels for item.\n",
    "nc_val_item_labels_path = os.path.join(base_dir, \"nc-val-item-labels.pt\")\n",
    "nc_val_item_labels = torch.randint(0, 10, (num_vals,))\n",
    "print(f\"Part of val labels[item] for node classification: {nc_val_item_labels[:3]}\")\n",
    "torch.save(nc_val_item_labels, nc_val_item_labels_path)\n",
    "print(f\"NC val labels[item] are saved to {nc_val_item_labels_path}\\n\")\n",
    "\n",
    "# Test IDs for user.\n",
    "nc_test_user_ids_path = os.path.join(base_dir, \"nc-test-user-ids.npy\")\n",
    "nc_test_user_ids = user_ids[-num_tests:]\n",
    "print(f\"Part of test ids[user] for node classification: {nc_test_user_ids[:3]}\")\n",
    "np.save(nc_test_user_ids_path, nc_test_user_ids)\n",
    "print(f\"NC test ids[user] are saved to {nc_test_user_ids_path}\\n\")\n",
    "\n",
    "# Test labels for user.\n",
    "nc_test_user_labels_path = os.path.join(base_dir, \"nc-test-user-labels.pt\")\n",
    "nc_test_user_labels = torch.randint(0, 10, (num_tests,))\n",
    "print(f\"Part of test labels[user] for node classification: {nc_test_user_labels[:3]}\")\n",
    "torch.save(nc_test_user_labels, nc_test_user_labels_path)\n",
    "print(f\"NC test labels[user] are saved to {nc_test_user_labels_path}\\n\")\n",
    "\n",
    "# Test IDs for item.\n",
    "nc_test_item_ids_path = os.path.join(base_dir, \"nc-test-item-ids.npy\")\n",
    "nc_test_item_ids = item_ids[-num_tests:]\n",
    "print(f\"Part of test ids[item] for node classification: {nc_test_item_ids[:3]}\")\n",
    "np.save(nc_test_item_ids_path, nc_test_item_ids)\n",
    "print(f\"NC test ids[item] are saved to {nc_test_item_ids_path}\\n\")\n",
    "\n",
    "# Test labels for item.\n",
    "nc_test_item_labels_path = os.path.join(base_dir, \"nc-test-item-labels.pt\")\n",
    "nc_test_item_labels = torch.randint(0, 10, (num_tests,))\n",
    "print(f\"Part of test labels[item] for node classification: {nc_test_item_labels[:3]}\")\n",
    "torch.save(nc_test_item_labels, nc_test_item_labels_path)\n",
    "print(f\"NC test labels[item] are saved to {nc_test_item_labels_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhAcDCHQ_KJ0"
   },
   "source": [
    "#### Link Prediction Task\n",
    "For link prediction task, we need **node pairs** or **negative src/dsts** for each training/validation/test set. Like feature data, numpy arrays and torch tensors are supported for these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T05:16:35.675332Z",
     "iopub.status.busy": "2024-09-02T05:16:35.674998Z",
     "iopub.status.idle": "2024-09-02T05:16:35.691872Z",
     "shell.execute_reply": "2024-09-02T05:16:35.691382Z"
    },
    "id": "u0jCnXIcAQy4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of train node pairs[user:like:item] for link prediction: [[158 444]\n",
      " [838 685]\n",
      " [984 317]]\n",
      "LP train node pairs[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-train-like-node-pairs.npy\n",
      "\n",
      "Part of train node pairs[user:follow:user] for link prediction: [[ 11 245]\n",
      " [843 699]\n",
      " [884 218]]\n",
      "LP train node pairs[user:follow:user] are saved to ./ondisk_dataset_heterograph/lp-train-follow-node-pairs.npy\n",
      "\n",
      "Part of val node pairs[user:like:item] for link prediction: [[710  98]\n",
      " [ 33 211]\n",
      " [ 87 835]]\n",
      "LP val node pairs[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-node-pairs.npy\n",
      "\n",
      "Part of val negative dsts[user:like:item] for link prediction: tensor([[710,  11, 855, 292,  23,  86, 764, 122,  77,  14],\n",
      "        [424, 321, 575, 244, 982, 931, 905, 260, 585, 706],\n",
      "        [801, 707, 964, 719,   3, 323, 310, 369, 140, 161]])\n",
      "LP val negative dsts[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-val-like-neg-dsts.pt\n",
      "\n",
      "Part of val node pairs[user:follow:user] for link prediction: [[ 61 816]\n",
      " [432 868]\n",
      " [694 764]]\n",
      "LP val node pairs[user:follow:user] are saved to ./ondisk_dataset_heterograph/lp-val-follow-node-pairs.npy\n",
      "\n",
      "Part of val negative dsts[user:follow:user] for link prediction: tensor([[696, 362,  42, 899, 176, 300,  24, 177, 116, 116],\n",
      "        [945, 523, 269, 758, 896, 256, 690, 880, 309, 374],\n",
      "        [602, 464, 867, 250, 468, 886, 829, 103, 643, 147]])\n",
      "LP val negative dsts[user:follow:user] are saved to ./ondisk_dataset_heterograph/lp-val-follow-neg-dsts.pt\n",
      "\n",
      "Part of test node pairs[user:like:item] for link prediction: [741   4]\n",
      "LP test node pairs[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-node-pairs.npy\n",
      "\n",
      "Part of test negative dsts[user:like:item] for link prediction: tensor([[769,  98, 825, 503, 519, 299, 264, 725, 795,   5],\n",
      "        [900, 953, 516, 617, 831, 292, 774,  13, 193, 914],\n",
      "        [380, 294, 173, 569, 614, 100, 391, 462, 894, 524]])\n",
      "LP test negative dsts[user:like:item] are saved to ./ondisk_dataset_heterograph/lp-test-like-neg-dsts.pt\n",
      "\n",
      "Part of test node pairs[user:follow:user] for link prediction: [409 713]\n",
      "LP test node pairs[user:follow:user] are saved to ./ondisk_dataset_heterograph/lp-test-follow-node-pairs.npy\n",
      "\n",
      "Part of test negative dsts[user:follow:user] for link prediction: tensor([[  7, 373, 350, 310, 495, 891, 211,  46, 445, 920],\n",
      "        [216, 606, 571,   7, 992, 657, 680, 842, 192, 351],\n",
      "        [210,  77, 218, 865, 612,  72, 590,  27, 754, 287]])\n",
      "LP test negative dsts[user:follow:user] are saved to ./ondisk_dataset_heterograph/lp-test-follow-neg-dsts.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For illustration, let's generate item sets for each edge type.\n",
    "num_trains = int(num_edges * 0.6)\n",
    "num_vals = int(num_edges * 0.2)\n",
    "num_tests = num_edges - num_trains - num_vals\n",
    "\n",
    "# Train node pairs for user:like:item.\n",
    "lp_train_like_node_pairs_path = os.path.join(base_dir, \"lp-train-like-node-pairs.npy\")\n",
    "lp_train_like_node_pairs = like_edges[:num_trains, :]\n",
    "print(f\"Part of train node pairs[user:like:item] for link prediction: {lp_train_like_node_pairs[:3]}\")\n",
    "np.save(lp_train_like_node_pairs_path, lp_train_like_node_pairs)\n",
    "print(f\"LP train node pairs[user:like:item] are saved to {lp_train_like_node_pairs_path}\\n\")\n",
    "\n",
    "# Train node pairs for user:follow:user.\n",
    "lp_train_follow_node_pairs_path = os.path.join(base_dir, \"lp-train-follow-node-pairs.npy\")\n",
    "lp_train_follow_node_pairs = follow_edges[:num_trains, :]\n",
    "print(f\"Part of train node pairs[user:follow:user] for link prediction: {lp_train_follow_node_pairs[:3]}\")\n",
    "np.save(lp_train_follow_node_pairs_path, lp_train_follow_node_pairs)\n",
    "print(f\"LP train node pairs[user:follow:user] are saved to {lp_train_follow_node_pairs_path}\\n\")\n",
    "\n",
    "# Val node pairs for user:like:item.\n",
    "lp_val_like_node_pairs_path = os.path.join(base_dir, \"lp-val-like-node-pairs.npy\")\n",
    "lp_val_like_node_pairs = like_edges[num_trains:num_trains+num_vals, :]\n",
    "print(f\"Part of val node pairs[user:like:item] for link prediction: {lp_val_like_node_pairs[:3]}\")\n",
    "np.save(lp_val_like_node_pairs_path, lp_val_like_node_pairs)\n",
    "print(f\"LP val node pairs[user:like:item] are saved to {lp_val_like_node_pairs_path}\\n\")\n",
    "\n",
    "# Val negative dsts for user:like:item.\n",
    "lp_val_like_neg_dsts_path = os.path.join(base_dir, \"lp-val-like-neg-dsts.pt\")\n",
    "lp_val_like_neg_dsts = torch.randint(0, num_nodes, (num_vals, 10))\n",
    "print(f\"Part of val negative dsts[user:like:item] for link prediction: {lp_val_like_neg_dsts[:3]}\")\n",
    "torch.save(lp_val_like_neg_dsts, lp_val_like_neg_dsts_path)\n",
    "print(f\"LP val negative dsts[user:like:item] are saved to {lp_val_like_neg_dsts_path}\\n\")\n",
    "\n",
    "# Val node pairs for user:follow:user.\n",
    "lp_val_follow_node_pairs_path = os.path.join(base_dir, \"lp-val-follow-node-pairs.npy\")\n",
    "lp_val_follow_node_pairs = follow_edges[num_trains:num_trains+num_vals, :]\n",
    "print(f\"Part of val node pairs[user:follow:user] for link prediction: {lp_val_follow_node_pairs[:3]}\")\n",
    "np.save(lp_val_follow_node_pairs_path, lp_val_follow_node_pairs)\n",
    "print(f\"LP val node pairs[user:follow:user] are saved to {lp_val_follow_node_pairs_path}\\n\")\n",
    "\n",
    "# Val negative dsts for user:follow:user.\n",
    "lp_val_follow_neg_dsts_path = os.path.join(base_dir, \"lp-val-follow-neg-dsts.pt\")\n",
    "lp_val_follow_neg_dsts = torch.randint(0, num_nodes, (num_vals, 10))\n",
    "print(f\"Part of val negative dsts[user:follow:user] for link prediction: {lp_val_follow_neg_dsts[:3]}\")\n",
    "torch.save(lp_val_follow_neg_dsts, lp_val_follow_neg_dsts_path)\n",
    "print(f\"LP val negative dsts[user:follow:user] are saved to {lp_val_follow_neg_dsts_path}\\n\")\n",
    "\n",
    "# Test node paris for user:like:item.\n",
    "lp_test_like_node_pairs_path = os.path.join(base_dir, \"lp-test-like-node-pairs.npy\")\n",
    "lp_test_like_node_pairs = like_edges[-num_tests, :]\n",
    "print(f\"Part of test node pairs[user:like:item] for link prediction: {lp_test_like_node_pairs[:3]}\")\n",
    "np.save(lp_test_like_node_pairs_path, lp_test_like_node_pairs)\n",
    "print(f\"LP test node pairs[user:like:item] are saved to {lp_test_like_node_pairs_path}\\n\")\n",
    "\n",
    "# Test negative dsts for user:like:item.\n",
    "lp_test_like_neg_dsts_path = os.path.join(base_dir, \"lp-test-like-neg-dsts.pt\")\n",
    "lp_test_like_neg_dsts = torch.randint(0, num_nodes, (num_tests, 10))\n",
    "print(f\"Part of test negative dsts[user:like:item] for link prediction: {lp_test_like_neg_dsts[:3]}\")\n",
    "torch.save(lp_test_like_neg_dsts, lp_test_like_neg_dsts_path)\n",
    "print(f\"LP test negative dsts[user:like:item] are saved to {lp_test_like_neg_dsts_path}\\n\")\n",
    "\n",
    "# Test node paris for user:follow:user.\n",
    "lp_test_follow_node_pairs_path = os.path.join(base_dir, \"lp-test-follow-node-pairs.npy\")\n",
    "lp_test_follow_node_pairs = follow_edges[-num_tests, :]\n",
    "print(f\"Part of test node pairs[user:follow:user] for link prediction: {lp_test_follow_node_pairs[:3]}\")\n",
    "np.save(lp_test_follow_node_pairs_path, lp_test_follow_node_pairs)\n",
    "print(f\"LP test node pairs[user:follow:user] are saved to {lp_test_follow_node_pairs_path}\\n\")\n",
    "\n",
    "# Test negative dsts for user:follow:user.\n",
    "lp_test_follow_neg_dsts_path = os.path.join(base_dir, \"lp-test-follow-neg-dsts.pt\")\n",
    "lp_test_follow_neg_dsts = torch.randint(0, num_nodes, (num_tests, 10))\n",
    "print(f\"Part of test negative dsts[user:follow:user] for link prediction: {lp_test_follow_neg_dsts[:3]}\")\n",
    "torch.save(lp_test_follow_neg_dsts, lp_test_follow_neg_dsts_path)\n",
    "print(f\"LP test negative dsts[user:follow:user] are saved to {lp_test_follow_neg_dsts_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbk6-wxRK-6S"
   },
   "source": [
    "## Organize Data into YAML File\n",
    "Now we need to create a `metadata.yaml` file which contains the paths, dadta types of graph structure, feature data, training/validation/test sets. Please note that all path should be relative to `metadata.yaml`.\n",
    "\n",
    "For heterogeneous graph, we need to specify the node/edge type in **type** fields. For edge type, canonical etype is required which is a string that's concatenated by source node type, etype, and destination node type together with `:`.\n",
    "\n",
    "Notes:\n",
    "- all path should be relative to `metadata.yaml`.\n",
    "- Below fields are optional and not specified in below example.\n",
    "  - `in_memory`: indicates whether to load dada into memory or `mmap`. Default is `True`.\n",
    "\n",
    "Please refer to [YAML specification](https://github.com/dmlc/dgl/blob/master/docs/source/stochastic_training/ondisk-dataset-specification.rst) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T05:16:35.694108Z",
     "iopub.status.busy": "2024-09-02T05:16:35.693903Z",
     "iopub.status.idle": "2024-09-02T05:16:35.700550Z",
     "shell.execute_reply": "2024-09-02T05:16:35.700069Z"
    },
    "id": "ddGTWW61Lpwp"
   },
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "    dataset_name: heterogeneous_graph_nc_lp\n",
    "    graph:\n",
    "      nodes:\n",
    "        - type: user\n",
    "          num: {num_nodes}\n",
    "        - type: item\n",
    "          num: {num_nodes}\n",
    "      edges:\n",
    "        - type: \"user:like:item\"\n",
    "          format: csv\n",
    "          path: {os.path.basename(like_edges_path)}\n",
    "        - type: \"user:follow:user\"\n",
    "          format: csv\n",
    "          path: {os.path.basename(follow_edges_path)}\n",
    "    feature_data:\n",
    "      - domain: node\n",
    "        type: user\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(node_user_feat_0_path)}\n",
    "      - domain: node\n",
    "        type: user\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(node_user_feat_1_path)}\n",
    "      - domain: node\n",
    "        type: item\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(node_item_feat_0_path)}\n",
    "      - domain: node\n",
    "        type: item\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(node_item_feat_1_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:like:item\"\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(edge_like_feat_0_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:like:item\"\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(edge_like_feat_1_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:follow:user\"\n",
    "        name: feat_0\n",
    "        format: numpy\n",
    "        path: {os.path.basename(edge_follow_feat_0_path)}\n",
    "      - domain: edge\n",
    "        type: \"user:follow:user\"\n",
    "        name: feat_1\n",
    "        format: torch\n",
    "        path: {os.path.basename(edge_follow_feat_1_path)}\n",
    "    tasks:\n",
    "      - name: node_classification\n",
    "        num_classes: 10\n",
    "        train_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seed_nodes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_train_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_train_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seed_nodes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_train_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_train_item_labels_path)}\n",
    "        validation_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seed_nodes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_val_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_val_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seed_nodes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_val_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_val_item_labels_path)}\n",
    "        test_set:\n",
    "          - type: user\n",
    "            data:\n",
    "              - name: seed_nodes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_test_user_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_test_user_labels_path)}\n",
    "          - type: item\n",
    "            data:\n",
    "              - name: seed_nodes\n",
    "                format: numpy\n",
    "                path: {os.path.basename(nc_test_item_ids_path)}\n",
    "              - name: labels\n",
    "                format: torch\n",
    "                path: {os.path.basename(nc_test_item_labels_path)}\n",
    "      - name: link_prediction\n",
    "        num_classes: 10\n",
    "        train_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: node_pairs\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_train_like_node_pairs_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: node_pairs\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_train_follow_node_pairs_path)}\n",
    "        validation_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: node_pairs\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_like_node_pairs_path)}\n",
    "              - name: negative_dsts\n",
    "                format: torch\n",
    "                path: {os.path.basename(lp_val_like_neg_dsts_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: node_pairs\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_val_follow_node_pairs_path)}\n",
    "              - name: negative_dsts\n",
    "                format: torch\n",
    "                path: {os.path.basename(lp_val_follow_neg_dsts_path)}\n",
    "        test_set:\n",
    "          - type: \"user:like:item\"\n",
    "            data:\n",
    "              - name: node_pairs\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_like_node_pairs_path)}\n",
    "              - name: negative_dsts\n",
    "                format: torch\n",
    "                path: {os.path.basename(lp_test_like_neg_dsts_path)}\n",
    "          - type: \"user:follow:user\"\n",
    "            data:\n",
    "              - name: node_pairs\n",
    "                format: numpy\n",
    "                path: {os.path.basename(lp_test_follow_node_pairs_path)}\n",
    "              - name: negative_dsts\n",
    "                format: torch\n",
    "                path: {os.path.basename(lp_test_follow_neg_dsts_path)}\n",
    "\"\"\"\n",
    "metadata_path = os.path.join(base_dir, \"metadata.yaml\")\n",
    "with open(metadata_path, \"w\") as f:\n",
    "  f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEfybHGhOW7O"
   },
   "source": [
    "## Instantiate `OnDiskDataset`\n",
    "Now we're ready to load dataset via `dgl.graphbolt.OnDiskDataset`. When instantiating, we just pass in the base directory where `metadata.yaml` file lies.\n",
    "\n",
    "During first instantiation, GraphBolt preprocesses the raw data such as constructing `FusedCSCSamplingGraph` from edges. All data including graph, feature data, training/validation/test sets are put into `preprocessed` directory after preprocessing. Any following dataset loading will skip the preprocess stage.\n",
    "\n",
    "After preprocessing, `load()` is required to be called explicitly in order to load graph, feature data and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T05:16:35.702758Z",
     "iopub.status.busy": "2024-09-02T05:16:35.702400Z",
     "iopub.status.idle": "2024-09-02T05:16:35.848077Z",
     "shell.execute_reply": "2024-09-02T05:16:35.847562Z"
    },
    "id": "W58CZoSzOiyo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The on-disk dataset is re-preprocessing, so the existing preprocessed dataset has been removed.\n",
      "Start to preprocess the on-disk dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish preprocessing the on-disk dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph: FusedCSCSamplingGraph(csc_indptr=tensor([    0,     8,    23,  ..., 19979, 19985, 20000], dtype=torch.int32),\n",
      "                      indices=tensor([1481, 1035, 1795,  ..., 1331, 1028, 1348], dtype=torch.int32),\n",
      "                      total_num_nodes=2000, num_edges={'user:follow:user': 10000, 'user:like:item': 10000},\n",
      "                      node_type_offset=tensor([   0, 1000, 2000], dtype=torch.int32),\n",
      "                      type_per_edge=tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.uint8),\n",
      "                      node_type_to_id={'item': 0, 'user': 1},\n",
      "                      edge_type_to_id={'user:follow:user': 0, 'user:like:item': 1},)\n",
      "\n",
      "Loaded feature store: TorchBasedFeatureStore(\n",
      "    {(<OnDiskFeatureDataDomain.NODE: 'node'>, 'user', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.3855, 0.6415, 0.3017, 0.1476, 0.6641],\n",
      "                        [0.9924, 0.0777, 0.4654, 0.9087, 0.9156],\n",
      "                        [0.0663, 0.7802, 0.9256, 0.5224, 0.3534],\n",
      "                        ...,\n",
      "                        [0.0369, 0.6813, 0.0968, 0.5371, 0.6892],\n",
      "                        [0.0864, 0.8249, 0.5202, 0.0149, 0.5459],\n",
      "                        [0.5323, 0.0197, 0.2692, 0.7999, 0.5572]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'user', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.7100, 0.2405, 0.2885, 0.4496, 0.9730],\n",
      "                        [0.2667, 0.6266, 0.0149, 0.3903, 0.2006],\n",
      "                        [0.9206, 0.2421, 0.4535, 0.2017, 0.2078],\n",
      "                        ...,\n",
      "                        [0.4428, 0.5906, 0.4400, 0.0084, 0.0123],\n",
      "                        [0.5330, 0.9931, 0.4180, 0.1809, 0.7197],\n",
      "                        [0.4884, 0.6634, 0.5388, 0.0416, 0.3750]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'item', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.0385, 0.3040, 0.2317, 0.6739, 0.3355],\n",
      "                        [0.6223, 0.8091, 0.2752, 0.4195, 0.3661],\n",
      "                        [0.4478, 0.9691, 0.3427, 0.2444, 0.0934],\n",
      "                        ...,\n",
      "                        [0.4583, 0.6198, 0.4319, 0.8689, 0.6168],\n",
      "                        [0.7533, 0.5011, 0.3670, 0.5130, 0.1776],\n",
      "                        [0.8833, 0.3244, 0.2958, 0.3765, 0.7597]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.NODE: 'node'>, 'item', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.2070, 0.2580, 0.2705, 0.3839, 0.9433],\n",
      "                        [0.4090, 0.0882, 0.6113, 0.3062, 0.5593],\n",
      "                        [0.2406, 0.0973, 0.4892, 0.7453, 0.3034],\n",
      "                        ...,\n",
      "                        [0.5605, 0.4961, 0.7817, 0.0578, 0.8285],\n",
      "                        [0.6403, 0.6874, 0.8852, 0.2093, 0.1015],\n",
      "                        [0.1458, 0.8151, 0.7867, 0.4925, 0.3148]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:like:item', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.4261, 0.7004, 0.6448, 0.6766, 0.6686],\n",
      "                        [0.1878, 0.0394, 0.9093, 0.6142, 0.0783],\n",
      "                        [0.6484, 0.1813, 0.7099, 0.3355, 0.5701],\n",
      "                        ...,\n",
      "                        [0.5876, 0.1391, 0.4609, 0.8826, 0.6060],\n",
      "                        [0.5527, 0.8893, 0.5381, 0.9071, 0.2813],\n",
      "                        [0.3696, 0.5844, 0.9281, 0.5085, 0.6012]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:like:item', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.4201, 0.3138, 0.3399, 0.7463, 0.6212],\n",
      "                        [0.2006, 0.6508, 0.5889, 0.8474, 0.0381],\n",
      "                        [0.5975, 0.7955, 0.2292, 0.9512, 0.2572],\n",
      "                        ...,\n",
      "                        [0.3379, 0.4597, 0.3834, 0.5815, 0.8054],\n",
      "                        [0.5039, 0.3404, 0.3693, 0.8403, 0.7181],\n",
      "                        [0.6201, 0.9732, 0.7144, 0.2747, 0.4378]]),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:follow:user', 'feat_0'): TorchBasedFeature(\n",
      "        feature=tensor([[0.5921, 0.7680, 0.3818, 0.1564, 0.5616],\n",
      "                        [0.7596, 0.8077, 0.4823, 0.7505, 0.9352],\n",
      "                        [0.8210, 0.4636, 0.8513, 0.8593, 0.5293],\n",
      "                        ...,\n",
      "                        [0.7329, 0.4315, 0.7218, 0.7232, 0.1431],\n",
      "                        [0.0725, 0.7650, 0.0203, 0.8736, 0.7289],\n",
      "                        [0.0643, 0.3802, 0.8527, 0.5067, 0.5213]], dtype=torch.float64),\n",
      "        metadata={},\n",
      "    ), (<OnDiskFeatureDataDomain.EDGE: 'edge'>, 'user:follow:user', 'feat_1'): TorchBasedFeature(\n",
      "        feature=tensor([[0.6529, 0.1965, 0.4648, 0.1633, 0.2976],\n",
      "                        [0.2142, 0.6420, 0.8123, 0.3233, 0.0410],\n",
      "                        [0.5288, 0.3592, 0.2735, 0.9079, 0.5376],\n",
      "                        ...,\n",
      "                        [0.0184, 0.4927, 0.6617, 0.3743, 0.7703],\n",
      "                        [0.1188, 0.5767, 0.1570, 0.3005, 0.9841],\n",
      "                        [0.1864, 0.8455, 0.7597, 0.8812, 0.5029]]),\n",
      "        metadata={},\n",
      "    )}\n",
      ")\n",
      "\n",
      "Loaded node classification task: OnDiskTask(validation_set=ItemSetDict(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([240, 257, 128, 225, 911,  94, 789, 707, 680, 766, 183, 163, 148, 937,\n",
      "                                591, 389, 668, 792, 311, 150, 416, 571, 118, 641, 701, 760, 375,  73,\n",
      "                                433, 382, 260, 353,  55, 691,  69, 264, 468, 578, 110, 624, 554, 413,\n",
      "                                519, 373, 673, 131, 354, 660, 542, 435, 450, 431, 169, 734, 809, 585,\n",
      "                                687, 949, 782, 638, 356, 456, 432, 236, 475, 879, 951,  17,  75, 420,\n",
      "                                335, 133, 521,  71, 686, 963, 762, 904,  15, 331, 251,  11, 750, 653,\n",
      "                                249, 106, 883, 564, 209, 307, 995, 759, 834, 871,  98, 741, 229,  42,\n",
      "                                961, 443, 570, 424,  65, 901, 319, 922, 421, 797, 273, 256, 350, 870,\n",
      "                                589, 228, 531, 815, 316, 713, 813, 374, 972, 558, 593, 715, 866, 408,\n",
      "                                121, 269, 678, 341, 167, 480, 748, 692, 676, 737, 962, 288, 852, 694,\n",
      "                                464, 817,  57,  56, 220,   7, 215, 621, 318, 326, 575, 556, 643, 955,\n",
      "                                541, 619, 761, 175, 898, 842, 688, 971, 124, 357, 546, 814, 969, 428,\n",
      "                                254, 533, 530, 211, 664, 658, 610, 305, 404,  54,  97, 369, 477, 174,\n",
      "                                651,  16, 895, 159,  95,  85, 323, 964,   0, 945, 505, 122, 740, 312,\n",
      "                                839, 205, 920,  20], dtype=torch.int32), tensor([8, 5, 6, 3, 6, 5, 6, 3, 1, 9, 6, 0, 8, 5, 2, 6, 1, 1, 1, 5, 5, 7, 3, 0,\n",
      "                                5, 5, 9, 0, 3, 0, 5, 7, 5, 4, 2, 1, 7, 2, 8, 6, 0, 4, 9, 8, 6, 4, 7, 6,\n",
      "                                8, 9, 1, 8, 1, 5, 4, 1, 2, 8, 4, 0, 9, 3, 1, 5, 2, 0, 4, 5, 7, 8, 8, 0,\n",
      "                                7, 3, 7, 5, 0, 2, 1, 1, 1, 2, 1, 5, 4, 3, 0, 9, 5, 3, 7, 9, 3, 0, 5, 6,\n",
      "                                2, 7, 3, 8, 0, 0, 3, 8, 3, 4, 4, 4, 1, 4, 3, 0, 7, 1, 3, 7, 9, 1, 5, 8,\n",
      "                                1, 1, 9, 9, 1, 3, 1, 4, 9, 8, 0, 2, 1, 1, 0, 8, 0, 1, 5, 2, 2, 0, 5, 1,\n",
      "                                2, 2, 9, 0, 1, 6, 2, 0, 2, 9, 5, 0, 6, 3, 6, 5, 0, 8, 5, 7, 3, 6, 1, 4,\n",
      "                                3, 1, 4, 3, 7, 8, 8, 3, 8, 1, 6, 0, 7, 9, 4, 5, 1, 7, 1, 6, 2, 3, 2, 8,\n",
      "                                8, 3, 2, 0, 0, 0, 4, 3])),\n",
      "                            names=('seed_nodes', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([442, 572, 102,  63, 730, 824, 689, 267, 347, 683, 937, 254,   3, 417,\n",
      "                                467, 334, 700, 260, 483, 997, 505, 879, 785, 164, 867, 946, 893, 532,\n",
      "                                452, 425, 182, 520, 613, 262, 277,  27,  72, 952, 922, 508, 252,  15,\n",
      "                                715, 266, 672, 771, 372, 666,  77, 562, 776, 734, 833, 450, 742, 463,\n",
      "                                453, 517, 154, 283, 469, 132, 757,  18, 314, 580, 840, 752, 199, 707,\n",
      "                                 43, 656, 933,  66, 173, 384, 358,  70, 650, 676, 395, 294, 490, 370,\n",
      "                                886, 341, 854, 391, 503, 350, 853, 635, 434, 863,  81, 596,  59,  30,\n",
      "                                 96, 966, 301, 351, 759, 362, 140,  68, 552, 177, 601, 855, 621, 103,\n",
      "                                535, 307, 288, 389, 767, 844, 802, 402,  21,  32, 662, 859, 878, 195,\n",
      "                                858, 313, 519, 636, 494, 242, 181, 659, 918, 897, 399, 265, 284,  34,\n",
      "                                 87, 619, 810, 430, 287, 555, 903,  71, 818, 816, 388, 880, 456, 303,\n",
      "                                118, 444, 538,   9, 419, 240, 592, 663, 361, 275, 985,  94, 652, 146,\n",
      "                                796,  14, 202,  65, 603, 791, 331, 299, 251, 633, 576, 658, 852, 245,\n",
      "                                841, 454,   8, 965, 472, 634,  36, 185, 660, 524, 152, 829, 477, 688,\n",
      "                                687, 243, 972, 711], dtype=torch.int32), tensor([3, 3, 2, 6, 0, 4, 2, 2, 4, 3, 0, 5, 7, 8, 2, 4, 8, 5, 4, 1, 1, 7, 6, 1,\n",
      "                                6, 5, 3, 3, 9, 7, 6, 7, 5, 5, 3, 3, 0, 9, 2, 7, 0, 0, 3, 4, 3, 3, 9, 2,\n",
      "                                4, 0, 4, 4, 8, 4, 4, 1, 8, 8, 7, 8, 9, 0, 4, 4, 1, 1, 1, 2, 6, 0, 8, 3,\n",
      "                                4, 3, 0, 5, 5, 4, 9, 1, 7, 7, 3, 5, 0, 9, 6, 7, 2, 4, 8, 8, 8, 1, 0, 1,\n",
      "                                7, 8, 2, 0, 3, 1, 1, 1, 5, 6, 7, 7, 4, 5, 0, 2, 9, 3, 8, 4, 0, 8, 9, 3,\n",
      "                                0, 3, 5, 9, 0, 5, 5, 9, 9, 0, 2, 3, 7, 2, 1, 2, 7, 4, 1, 4, 5, 7, 5, 3,\n",
      "                                8, 3, 7, 0, 7, 6, 2, 2, 1, 6, 4, 9, 0, 0, 1, 1, 4, 1, 7, 4, 9, 2, 4, 5,\n",
      "                                5, 8, 3, 2, 5, 1, 3, 2, 4, 6, 8, 9, 1, 5, 4, 1, 3, 0, 5, 0, 5, 4, 8, 7,\n",
      "                                8, 7, 0, 2, 3, 6, 4, 9])),\n",
      "                            names=('seed_nodes', 'labels'),\n",
      "                        )},\n",
      "               names=('seed_nodes', 'labels'),\n",
      "           ),\n",
      "           train_set=ItemSetDict(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([783,   6, 425, 886, 726, 982, 965, 265, 524, 123, 990, 891, 315, 780,\n",
      "                                482, 604, 672, 178, 397, 716, 720,  33, 616,  21, 184, 695, 219, 831,\n",
      "                                577, 126, 978, 378, 572, 496, 860,  23, 484, 959, 845,  70, 263,  91,\n",
      "                                808, 258,   8,  31, 213, 280, 608, 154, 116, 287,   5, 977, 396, 532,\n",
      "                                370, 944, 363, 345, 933, 968, 317, 882, 145,  12, 642, 787, 979, 884,\n",
      "                                  1, 347, 873, 788, 669, 926, 523, 580, 278, 371, 579, 513, 751, 667,\n",
      "                                226, 708, 623, 516, 781, 486, 191, 993, 384, 306, 422, 285, 885, 244,\n",
      "                                878, 957, 596, 980, 509, 960, 266, 843, 798,   3, 914, 684, 108, 465,\n",
      "                                136, 221, 943, 566, 518, 165, 675, 942, 467, 152, 343, 696, 910, 206,\n",
      "                                330, 342, 233, 888, 129, 391, 719, 216, 924, 685, 916, 999, 338, 698,\n",
      "                                733, 828, 289, 637, 310, 217, 262, 512, 281, 434, 359, 954, 706, 628,\n",
      "                                939, 401, 162, 728, 390, 399, 976, 180, 804, 309, 889, 130, 320, 609,\n",
      "                                632,  37,  90, 385, 437, 497, 654, 479, 796, 242,  44, 511, 869, 488,\n",
      "                                552, 938, 864, 501, 352, 799, 652, 590, 528, 510, 559, 297, 230, 157,\n",
      "                                908, 239, 367, 666, 721,  10, 490, 514, 921, 700, 854, 606, 646, 948,\n",
      "                                617, 998,  47, 202, 917, 445, 339, 403, 250, 958, 139, 810, 368, 970,\n",
      "                                868, 603, 247, 941, 529, 626, 166, 101, 137, 631,  72, 200, 774, 156,\n",
      "                                851, 112, 846, 483,  28, 253, 702, 681, 677, 544, 298, 246, 414, 928,\n",
      "                                395, 991, 405,  18, 337, 195, 634, 861, 801, 300, 874, 499, 896,  45,\n",
      "                                194, 915, 855, 690, 697, 400, 778, 291, 567, 155, 752, 844, 454, 442,\n",
      "                                238, 415, 555, 103, 973,  59, 975, 747, 662, 934, 525, 267, 907, 336,\n",
      "                                714, 803,  38, 366,  99, 718, 526, 304, 313, 481, 132, 919, 576, 573,\n",
      "                                850, 614, 485,  86, 786, 663, 779, 314, 212,  92, 657, 274, 827, 453,\n",
      "                                841, 611, 659,  61, 835, 534, 930, 909,  76, 985, 474, 466, 271, 114,\n",
      "                                471, 807,  67, 729, 929, 735, 838, 351, 203, 640, 284, 469, 196, 905,\n",
      "                                151, 549, 932, 656, 821, 245, 333, 107, 492, 158, 756, 113, 362, 218,\n",
      "                                725, 767, 472, 438, 784, 473, 863,  49,  53, 795, 743, 598,  13, 986,\n",
      "                                536, 758, 355, 109, 125, 563,  14, 412, 940, 515, 856,  81, 927, 539,\n",
      "                                470, 600, 648,  51, 739, 997, 527, 775, 140, 887, 840, 894, 235, 491,\n",
      "                                 24, 724, 186, 448, 100, 569, 936, 749, 777, 388, 446, 372, 988, 419,\n",
      "                                294, 423, 296, 170, 447,  60, 502, 383, 199, 325, 455, 639, 104, 992,\n",
      "                                818, 227, 903, 947, 858, 848,  29, 618, 770, 935, 224, 551, 458, 292,\n",
      "                                602, 825, 207, 862, 411, 565, 723, 537,  74, 674, 346, 950, 440, 500,\n",
      "                                557, 670, 407, 704, 192, 349, 457, 261, 601, 876, 731, 679, 177, 605,\n",
      "                                275, 181, 268,   2, 703,  40, 198, 772, 111, 223, 332, 364, 820, 615,\n",
      "                                635, 270, 952, 974, 989, 237, 811, 361, 279, 800, 645, 802, 709, 115,\n",
      "                                 88, 452, 302, 329, 459, 173, 620, 410, 785,  35,  93, 380,  48, 693,\n",
      "                                776, 365, 682, 543,  26,  96, 584, 588,  41, 769, 241, 816,  22, 182,\n",
      "                                794, 900, 208, 732, 746, 324, 386, 348, 394, 360,  87, 520,  27, 594,\n",
      "                                327, 765, 881, 406, 506, 204, 823, 629, 393, 463, 744,  34, 272, 574,\n",
      "                                913, 102,  68, 522,  25, 830, 197, 994, 595, 308, 612, 946, 449, 426,\n",
      "                                587, 149,  66, 508, 829, 436, 853, 255, 931, 984,   4, 880, 562, 771,\n",
      "                                899,  62,  64, 430, 683, 171, 824, 478, 143, 806, 742, 398],\n",
      "                               dtype=torch.int32), tensor([4, 4, 2, 1, 6, 6, 1, 3, 9, 2, 2, 0, 9, 0, 4, 2, 0, 1, 1, 3, 8, 2, 1, 6,\n",
      "                                7, 7, 4, 0, 2, 1, 3, 6, 7, 0, 4, 6, 5, 7, 3, 4, 2, 6, 3, 3, 5, 3, 2, 3,\n",
      "                                9, 9, 1, 5, 6, 5, 8, 7, 6, 1, 6, 1, 2, 8, 8, 0, 7, 8, 4, 6, 8, 9, 9, 2,\n",
      "                                8, 1, 2, 1, 1, 7, 4, 8, 0, 7, 6, 1, 8, 9, 9, 1, 3, 5, 9, 8, 6, 0, 3, 0,\n",
      "                                3, 8, 0, 8, 6, 6, 9, 4, 9, 8, 5, 0, 3, 7, 0, 8, 1, 7, 5, 4, 0, 9, 3, 1,\n",
      "                                4, 6, 7, 6, 3, 2, 1, 9, 8, 5, 4, 1, 9, 9, 4, 5, 2, 0, 9, 2, 6, 9, 1, 7,\n",
      "                                6, 4, 1, 2, 6, 4, 2, 3, 2, 5, 7, 8, 3, 2, 7, 1, 3, 0, 0, 0, 6, 2, 9, 6,\n",
      "                                7, 1, 9, 6, 5, 9, 6, 1, 6, 6, 0, 3, 2, 2, 6, 7, 2, 8, 0, 1, 8, 5, 6, 9,\n",
      "                                0, 6, 9, 6, 0, 3, 5, 6, 6, 4, 8, 8, 3, 8, 7, 6, 8, 1, 4, 0, 8, 0, 4, 5,\n",
      "                                7, 9, 2, 5, 2, 1, 4, 6, 4, 4, 2, 9, 1, 3, 3, 9, 6, 6, 5, 5, 1, 8, 5, 5,\n",
      "                                2, 8, 4, 8, 3, 5, 6, 3, 6, 9, 5, 9, 8, 3, 8, 7, 1, 0, 1, 3, 5, 4, 4, 4,\n",
      "                                3, 4, 8, 4, 8, 9, 6, 7, 1, 8, 9, 3, 1, 3, 1, 8, 4, 5, 9, 6, 7, 5, 7, 3,\n",
      "                                6, 8, 7, 0, 4, 2, 1, 8, 5, 7, 9, 2, 7, 2, 3, 4, 8, 9, 8, 7, 9, 4, 7, 3,\n",
      "                                3, 9, 7, 9, 3, 1, 6, 6, 1, 2, 4, 4, 2, 9, 3, 8, 6, 6, 4, 7, 5, 8, 3, 5,\n",
      "                                9, 0, 0, 9, 5, 6, 5, 7, 2, 0, 3, 2, 4, 1, 9, 6, 1, 1, 5, 5, 3, 6, 9, 6,\n",
      "                                8, 5, 8, 6, 8, 0, 2, 5, 8, 2, 3, 6, 0, 7, 6, 3, 3, 7, 2, 8, 9, 7, 8, 2,\n",
      "                                9, 3, 1, 3, 1, 5, 8, 9, 0, 5, 9, 3, 1, 4, 8, 4, 6, 5, 4, 0, 9, 6, 4, 1,\n",
      "                                5, 8, 5, 0, 1, 4, 4, 0, 1, 1, 3, 7, 1, 2, 8, 9, 5, 3, 2, 6, 3, 1, 2, 1,\n",
      "                                3, 1, 7, 7, 9, 1, 1, 8, 6, 5, 1, 6, 2, 2, 2, 2, 5, 9, 8, 0, 8, 6, 8, 4,\n",
      "                                6, 6, 2, 2, 8, 9, 1, 5, 4, 7, 9, 1, 0, 1, 1, 9, 7, 4, 7, 2, 9, 4, 8, 8,\n",
      "                                7, 7, 9, 5, 0, 9, 5, 7, 6, 4, 2, 1, 9, 3, 3, 7, 8, 4, 8, 4, 5, 3, 6, 5,\n",
      "                                9, 4, 6, 6, 9, 0, 9, 9, 9, 8, 5, 3, 5, 1, 9, 7, 4, 8, 4, 5, 6, 3, 6, 6,\n",
      "                                1, 2, 8, 0, 8, 6, 0, 3, 6, 6, 5, 3, 2, 5, 2, 8, 1, 7, 2, 8, 1, 3, 8, 3,\n",
      "                                9, 3, 5, 4, 8, 2, 8, 6, 0, 4, 9, 0, 0, 2, 8, 0, 6, 4, 9, 4, 6, 7, 9, 8,\n",
      "                                4, 5, 4, 0, 2, 6, 0, 0, 0, 6, 8, 9, 1, 6, 8, 8, 0, 8, 2, 1, 8, 8, 1, 7])),\n",
      "                            names=('seed_nodes', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([631, 735, 695, 208, 147, 276,  67, 325, 448, 857,  97, 191, 573, 479,\n",
      "                                976, 895, 148,  74,  40, 128, 394, 722, 983, 748, 566, 678, 178,  11,\n",
      "                                970, 168,  60, 104, 534, 336, 495, 426, 798, 201, 466, 214, 795,  53,\n",
      "                                861, 727, 353, 746, 360,   2, 422, 951, 223, 755, 949, 773, 856, 883,\n",
      "                                793, 259, 775, 345, 762, 511, 285, 221, 911, 263, 554, 227, 171, 203,\n",
      "                                958, 626, 980, 691, 928,  50, 408, 822, 158, 582, 264, 645, 211, 560,\n",
      "                                310,  33, 860, 247, 945, 801, 825, 629, 349, 924, 404,  90, 274, 827,\n",
      "                                354, 156, 324, 814, 124, 298, 188, 536,  83, 197, 196, 253, 220, 938,\n",
      "                                823, 894, 941, 698, 279, 346, 587,  25,  44, 513, 703, 123,  38, 790,\n",
      "                                507, 500, 960, 913, 577, 927, 237, 190, 437, 365, 255, 344, 121, 721,\n",
      "                                125, 680, 987,  16, 161, 533, 557, 127, 968, 739, 241, 737, 628, 654,\n",
      "                                192,  58, 865, 272, 366, 756, 806, 170, 914, 459, 959, 258, 726, 998,\n",
      "                                315,  20, 499, 820,  46, 708, 784, 471, 421, 359, 476,   7, 318, 904,\n",
      "                                139, 228, 213, 514, 765, 868, 327, 559, 896, 409, 482, 447, 455, 129,\n",
      "                                993, 591, 110, 145, 256, 813, 799, 540, 179, 875, 323, 561, 747, 612,\n",
      "                                269, 261, 690, 564, 382, 563,  89, 122, 403, 393, 642, 579, 550, 217,\n",
      "                                971, 942, 461, 429, 548, 719, 296, 212, 496, 498, 438, 874, 585, 204,\n",
      "                                187, 109, 764, 982, 244, 617, 376, 374, 198, 807, 379,  45, 901, 112,\n",
      "                                969, 644, 488,  42, 367, 699, 725, 873, 440, 470, 877, 706, 789, 304,\n",
      "                                936, 497, 436, 641, 805, 770, 144, 516, 989, 300, 673,  98,  84, 649,\n",
      "                                815, 615, 948, 238, 305, 167, 232, 736,  54, 235, 491, 994,  62, 728,\n",
      "                                907, 302, 343, 239, 200, 981, 415, 882, 930, 522, 280, 979, 597, 480,\n",
      "                                222, 984,  85, 908, 113, 622, 956, 803, 458, 991, 750, 786, 530,  37,\n",
      "                                333, 492, 847, 330, 731, 925, 990, 363, 779, 545, 568, 718, 679, 400,\n",
      "                                386, 547, 481,  79, 630, 396, 705, 605, 180, 504, 618, 693, 427, 162,\n",
      "                                549,  23, 518, 931, 593, 575, 570, 460, 526, 717, 270, 751, 355, 639,\n",
      "                                788,  10, 800, 835, 741, 306,  31, 977, 133, 720, 338, 724, 902, 528,\n",
      "                                248,  92, 149, 655,  24, 289, 309, 290, 669, 714, 950, 623, 661, 418,\n",
      "                                754, 589, 435, 674, 510, 834, 411, 166, 604, 286, 850, 546, 541, 733,\n",
      "                                848, 445, 329, 544, 766, 939, 193, 637, 692,  13, 712, 743, 312, 527,\n",
      "                                961, 451,   5, 410, 670, 553, 610, 383, 817, 932, 782, 512, 667, 210,\n",
      "                                769, 709, 583, 257, 864, 397, 763, 943, 686, 574, 999, 849, 493, 684,\n",
      "                                169, 141, 761,  80, 964, 696, 694,   4, 175, 506, 920, 973, 648, 335,\n",
      "                                364, 225, 295, 608, 186, 407, 342, 378, 935, 551, 862, 273, 955, 988,\n",
      "                                246, 967, 531, 157, 716, 657, 625, 515, 401, 953, 539, 588, 598,  61,\n",
      "                                380, 729, 889, 947, 606,  41, 647, 565, 143, 768, 651, 869, 339, 230,\n",
      "                                142, 607, 909, 594, 890, 843,  28,  91, 826, 643, 760, 809, 681, 831,\n",
      "                                509, 537, 740, 595, 995, 600, 783, 578, 387,  69,  73, 794,   1, 525,\n",
      "                                484, 611, 910, 131, 954, 249, 292, 957,  57, 701, 405, 891, 281,  76,\n",
      "                                962, 521, 475,   0, 906, 792, 446, 846, 996, 183, 398, 704, 233, 457,\n",
      "                                423, 489, 884, 523, 836, 278, 866, 120, 443, 373, 838, 153, 340, 710,\n",
      "                                753, 665,  78, 584, 758, 348, 432, 205, 944, 888, 219, 194, 744, 151,\n",
      "                                609, 842, 114, 321, 150, 845,  51, 107, 632, 986, 119, 322],\n",
      "                               dtype=torch.int32), tensor([9, 8, 5, 5, 8, 3, 0, 9, 2, 2, 7, 1, 1, 5, 6, 2, 1, 0, 6, 4, 2, 1, 6, 2,\n",
      "                                5, 1, 2, 1, 6, 8, 8, 2, 5, 8, 1, 3, 2, 0, 0, 6, 6, 2, 2, 6, 6, 4, 3, 5,\n",
      "                                3, 4, 9, 9, 8, 2, 4, 3, 6, 7, 3, 5, 3, 4, 1, 2, 9, 2, 9, 6, 9, 9, 8, 9,\n",
      "                                1, 2, 3, 0, 3, 0, 9, 1, 2, 9, 4, 8, 8, 4, 3, 7, 5, 9, 0, 3, 1, 6, 9, 8,\n",
      "                                8, 6, 8, 7, 1, 5, 4, 4, 4, 0, 3, 6, 1, 7, 0, 7, 9, 2, 4, 9, 0, 6, 8, 6,\n",
      "                                7, 5, 9, 8, 9, 9, 2, 7, 5, 8, 4, 3, 2, 6, 7, 3, 1, 4, 0, 6, 4, 4, 3, 1,\n",
      "                                4, 1, 4, 7, 2, 1, 0, 3, 3, 9, 1, 5, 5, 3, 4, 0, 5, 1, 7, 2, 7, 4, 9, 6,\n",
      "                                1, 6, 3, 9, 6, 4, 0, 9, 1, 0, 3, 7, 7, 6, 7, 2, 9, 8, 8, 5, 1, 5, 0, 8,\n",
      "                                6, 5, 2, 2, 9, 1, 7, 7, 8, 3, 2, 2, 6, 5, 5, 4, 8, 1, 5, 6, 2, 6, 8, 3,\n",
      "                                2, 6, 7, 8, 7, 1, 0, 6, 7, 4, 8, 2, 0, 6, 3, 2, 8, 1, 0, 2, 1, 0, 1, 7,\n",
      "                                2, 9, 2, 0, 5, 8, 1, 5, 5, 5, 5, 0, 7, 8, 5, 4, 9, 1, 5, 7, 4, 0, 4, 3,\n",
      "                                9, 6, 4, 9, 7, 1, 0, 4, 0, 5, 9, 5, 5, 8, 1, 6, 7, 5, 6, 0, 6, 9, 8, 1,\n",
      "                                8, 7, 6, 9, 7, 5, 4, 6, 9, 0, 6, 0, 9, 8, 9, 6, 8, 4, 3, 2, 4, 1, 0, 3,\n",
      "                                0, 7, 1, 5, 8, 5, 4, 0, 8, 5, 5, 4, 9, 8, 8, 5, 6, 5, 4, 0, 0, 9, 1, 4,\n",
      "                                9, 4, 0, 6, 3, 2, 6, 9, 2, 5, 4, 9, 8, 2, 5, 8, 6, 0, 4, 4, 4, 7, 9, 6,\n",
      "                                0, 1, 4, 5, 3, 3, 9, 0, 3, 6, 7, 5, 0, 5, 5, 6, 6, 9, 8, 1, 7, 4, 9, 3,\n",
      "                                0, 7, 2, 9, 8, 9, 8, 1, 6, 3, 8, 5, 3, 2, 3, 0, 1, 1, 8, 4, 1, 2, 0, 3,\n",
      "                                2, 4, 8, 3, 9, 2, 5, 2, 6, 5, 4, 9, 3, 0, 0, 7, 1, 8, 7, 3, 8, 5, 5, 6,\n",
      "                                9, 3, 9, 5, 1, 3, 5, 4, 8, 0, 5, 4, 8, 5, 0, 1, 6, 9, 5, 7, 0, 9, 8, 8,\n",
      "                                6, 5, 6, 5, 5, 0, 6, 3, 6, 2, 8, 9, 1, 0, 1, 0, 5, 9, 5, 1, 0, 5, 8, 1,\n",
      "                                1, 3, 4, 4, 8, 5, 6, 4, 7, 0, 5, 0, 3, 5, 1, 6, 6, 5, 8, 5, 7, 7, 9, 7,\n",
      "                                2, 3, 5, 0, 2, 9, 9, 1, 7, 0, 7, 3, 9, 2, 2, 2, 7, 6, 7, 1, 1, 6, 4, 3,\n",
      "                                1, 5, 6, 5, 7, 1, 8, 8, 9, 5, 4, 8, 7, 2, 3, 5, 5, 0, 5, 0, 8, 1, 8, 0,\n",
      "                                6, 4, 2, 1, 9, 6, 9, 7, 1, 0, 6, 5, 5, 1, 2, 5, 3, 6, 6, 6, 4, 4, 0, 0,\n",
      "                                3, 1, 5, 0, 4, 7, 9, 8, 4, 8, 5, 2, 4, 8, 5, 9, 9, 6, 7, 8, 4, 0, 8, 9])),\n",
      "                            names=('seed_nodes', 'labels'),\n",
      "                        )},\n",
      "               names=('seed_nodes', 'labels'),\n",
      "           ),\n",
      "           test_set=ItemSetDict(\n",
      "               itemsets={'user': ItemSet(\n",
      "                            items=(tensor([187, 460, 630, 231, 127, 665, 179, 712, 462, 358, 699, 812, 282, 764,\n",
      "                                906, 754, 190, 912, 153, 429, 243,   9, 189, 655, 376, 625, 489,  52,\n",
      "                                340, 301, 172, 793, 176, 875, 409, 753, 293, 276, 402, 661, 647, 689,\n",
      "                                967, 859, 188, 832,  19, 344, 146, 561, 444, 439, 283, 387, 996, 918,\n",
      "                                392,  63, 538, 201, 711, 545, 902,  78, 418, 923,  79, 214, 427, 987,\n",
      "                                507, 147, 379, 893, 582, 966, 210, 286, 185, 138, 493, 597, 494, 755,\n",
      "                                451, 649,  83, 550, 583, 277, 160,  39, 119,  82, 833, 193,  77, 872,\n",
      "                                142, 295, 790, 768, 504, 822, 248, 535, 161, 517, 745, 234, 953, 290,\n",
      "                                836, 981, 232, 757, 259, 299, 849, 377, 925, 892, 303, 322,  46, 671,\n",
      "                                568, 381,  58, 476, 540, 560, 636, 135, 865, 252, 607, 897, 553, 710,\n",
      "                                441, 805, 581, 105, 956, 461,  84, 548, 503, 164, 120, 599, 763,  32,\n",
      "                                487, 736, 328,  30, 722, 650,  80, 877, 857, 837, 983, 727, 622, 222,\n",
      "                                819, 791, 627, 586, 738,  89, 705, 730, 547, 144, 633, 644, 592,  50,\n",
      "                                134, 773, 498, 890, 117, 141, 717, 417, 334, 867, 613,  36, 826, 321,\n",
      "                                168, 495, 847,  43], dtype=torch.int32), tensor([0, 1, 9, 2, 1, 6, 3, 6, 2, 8, 2, 1, 9, 5, 2, 6, 6, 9, 2, 4, 1, 3, 8, 0,\n",
      "                                1, 3, 1, 1, 1, 1, 0, 2, 8, 6, 8, 8, 3, 8, 4, 7, 2, 8, 9, 8, 5, 0, 4, 2,\n",
      "                                7, 3, 3, 9, 3, 4, 7, 5, 8, 4, 6, 3, 1, 4, 6, 3, 4, 8, 7, 9, 8, 8, 1, 4,\n",
      "                                8, 9, 3, 3, 8, 7, 2, 0, 6, 8, 3, 2, 3, 8, 9, 0, 1, 1, 2, 4, 7, 0, 7, 1,\n",
      "                                9, 6, 5, 1, 9, 3, 9, 4, 8, 5, 0, 9, 8, 2, 9, 6, 3, 8, 3, 7, 7, 4, 6, 7,\n",
      "                                1, 9, 5, 8, 4, 2, 3, 5, 0, 0, 0, 8, 9, 4, 9, 9, 7, 4, 5, 0, 4, 4, 9, 3,\n",
      "                                7, 2, 1, 1, 2, 2, 3, 1, 9, 2, 5, 2, 8, 7, 4, 6, 0, 4, 5, 0, 8, 7, 2, 8,\n",
      "                                6, 6, 6, 4, 8, 6, 0, 0, 3, 8, 0, 7, 0, 6, 7, 0, 7, 3, 5, 8, 0, 9, 9, 7,\n",
      "                                9, 1, 0, 5, 9, 3, 3, 6])),\n",
      "                            names=('seed_nodes', 'labels'),\n",
      "                        ), 'item': ItemSet(\n",
      "                            items=(tensor([218, 317, 772,  95, 486, 371, 390, 900, 624, 326,  49, 851, 558,  82,\n",
      "                                808, 916, 675, 671, 556, 668, 749, 485, 134, 616, 677, 501, 234, 138,\n",
      "                                428, 332,  56, 106, 207,  55, 172, 929, 627, 934, 821, 640, 487, 777,\n",
      "                                567, 830,  75, 184, 462, 876, 646, 250, 174, 921, 406, 586, 137, 638,\n",
      "                                871, 319, 685, 165, 420, 439, 159, 614, 702, 832, 723, 745, 224,  52,\n",
      "                                293, 206, 732, 176, 311, 271, 115, 375, 682, 381, 581, 870, 155, 599,\n",
      "                                 39, 356, 797, 912, 268, 291, 392, 412, 369, 963, 226,  47, 136, 468,\n",
      "                                337, 738, 229, 126, 839, 297, 117, 780, 209, 282, 697, 100, 189,  86,\n",
      "                                872,  26, 781,   6, 416, 116, 108, 837, 542, 316, 328, 812, 529, 449,\n",
      "                                975, 828, 543,  93, 464, 160, 923, 163,  64, 320,  29, 887, 431,  99,\n",
      "                                590, 236,  48, 231, 502, 308, 778, 368,  88, 357, 978,  35, 352, 135,\n",
      "                                899, 974, 892, 385, 216, 478, 465, 441,  12, 898, 915, 433, 413, 713,\n",
      "                                215, 653, 804, 414, 602, 101, 940, 620, 473,  17, 811, 787, 905, 819,\n",
      "                                105, 992, 919, 474, 774, 130, 424, 664, 569, 377,  19,  22, 111, 917,\n",
      "                                881, 885, 926, 571], dtype=torch.int32), tensor([2, 2, 6, 2, 2, 8, 8, 3, 9, 0, 3, 3, 1, 7, 0, 1, 4, 7, 2, 6, 0, 5, 7, 5,\n",
      "                                4, 3, 4, 2, 5, 6, 9, 2, 5, 8, 7, 4, 3, 1, 1, 5, 4, 1, 8, 8, 6, 7, 9, 2,\n",
      "                                9, 0, 1, 6, 1, 0, 0, 7, 9, 0, 3, 7, 9, 3, 8, 2, 3, 1, 0, 8, 2, 4, 2, 5,\n",
      "                                2, 2, 2, 5, 4, 6, 9, 1, 8, 0, 3, 7, 7, 6, 7, 3, 7, 3, 5, 4, 4, 9, 6, 7,\n",
      "                                0, 2, 5, 5, 7, 6, 8, 6, 3, 0, 3, 4, 3, 0, 1, 5, 7, 0, 9, 5, 6, 6, 4, 9,\n",
      "                                1, 4, 2, 9, 3, 9, 6, 4, 5, 3, 8, 3, 7, 2, 7, 6, 9, 8, 1, 4, 8, 8, 2, 3,\n",
      "                                3, 4, 5, 5, 5, 2, 2, 8, 7, 5, 4, 5, 1, 8, 8, 3, 5, 3, 2, 4, 9, 0, 5, 6,\n",
      "                                1, 2, 7, 7, 8, 7, 2, 7, 7, 0, 0, 2, 2, 6, 9, 3, 1, 2, 9, 2, 0, 9, 2, 3,\n",
      "                                2, 5, 8, 3, 7, 8, 0, 1])),\n",
      "                            names=('seed_nodes', 'labels'),\n",
      "                        )},\n",
      "               names=('seed_nodes', 'labels'),\n",
      "           ),\n",
      "           metadata={'name': 'node_classification', 'num_classes': 10},)\n",
      "\n",
      "Loaded link prediction task: OnDiskTask(validation_set=ItemSetDict(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[710,  98],\n",
      "                                [ 33, 211],\n",
      "                                [ 87, 835],\n",
      "                                ...,\n",
      "                                [473, 159],\n",
      "                                [432, 756],\n",
      "                                [827, 138]], dtype=torch.int32), tensor([[710,  11, 855,  ..., 122,  77,  14],\n",
      "                                [424, 321, 575,  ..., 260, 585, 706],\n",
      "                                [801, 707, 964,  ..., 369, 140, 161],\n",
      "                                ...,\n",
      "                                [961, 824, 525,  ..., 424, 427, 141],\n",
      "                                [ 34, 595, 465,  ..., 575, 898,  36],\n",
      "                                [139, 723, 537,  ..., 591,  31, 872]], dtype=torch.int32)),\n",
      "                            names=('node_pairs', 'negative_dsts'),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[ 61, 816],\n",
      "                                [432, 868],\n",
      "                                [694, 764],\n",
      "                                ...,\n",
      "                                [707, 115],\n",
      "                                [674,  39],\n",
      "                                [780, 370]], dtype=torch.int32), tensor([[696, 362,  42,  ..., 177, 116, 116],\n",
      "                                [945, 523, 269,  ..., 880, 309, 374],\n",
      "                                [602, 464, 867,  ..., 103, 643, 147],\n",
      "                                ...,\n",
      "                                [ 90, 655, 294,  ..., 600, 170, 855],\n",
      "                                [495, 563, 475,  ..., 333, 403, 743],\n",
      "                                [140, 801, 712,  ..., 497, 209, 278]], dtype=torch.int32)),\n",
      "                            names=('node_pairs', 'negative_dsts'),\n",
      "                        )},\n",
      "               names=('node_pairs', 'negative_dsts'),\n",
      "           ),\n",
      "           train_set=ItemSetDict(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([[158, 444],\n",
      "                                [838, 685],\n",
      "                                [984, 317],\n",
      "                                ...,\n",
      "                                [615, 338],\n",
      "                                [559, 993],\n",
      "                                [524,  77]], dtype=torch.int32),),\n",
      "                            names=('node_pairs',),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([[ 11, 245],\n",
      "                                [843, 699],\n",
      "                                [884, 218],\n",
      "                                ...,\n",
      "                                [293, 646],\n",
      "                                [ 42, 288],\n",
      "                                [984,  73]], dtype=torch.int32),),\n",
      "                            names=('node_pairs',),\n",
      "                        )},\n",
      "               names=('node_pairs',),\n",
      "           ),\n",
      "           test_set=ItemSetDict(\n",
      "               itemsets={'user:like:item': ItemSet(\n",
      "                            items=(tensor([741,   4], dtype=torch.int32), tensor([[769,  98, 825,  ..., 725, 795,   5],\n",
      "                                [900, 953, 516,  ...,  13, 193, 914],\n",
      "                                [380, 294, 173,  ..., 462, 894, 524],\n",
      "                                ...,\n",
      "                                [ 38,  64, 795,  ..., 975, 239, 472],\n",
      "                                [781, 892, 767,  ..., 924, 719, 938],\n",
      "                                [634, 410,  10,  ..., 899, 814, 938]], dtype=torch.int32)),\n",
      "                            names=('node_pairs', 'negative_dsts'),\n",
      "                        ), 'user:follow:user': ItemSet(\n",
      "                            items=(tensor([409, 713], dtype=torch.int32), tensor([[  7, 373, 350,  ...,  46, 445, 920],\n",
      "                                [216, 606, 571,  ..., 842, 192, 351],\n",
      "                                [210,  77, 218,  ...,  27, 754, 287],\n",
      "                                ...,\n",
      "                                [ 23, 532, 682,  ...,  49, 527, 781],\n",
      "                                [925,  48, 141,  ..., 135, 147, 611],\n",
      "                                [888, 776, 792,  ..., 597, 191, 397]], dtype=torch.int32)),\n",
      "                            names=('node_pairs', 'negative_dsts'),\n",
      "                        )},\n",
      "               names=('node_pairs', 'negative_dsts'),\n",
      "           ),\n",
      "           metadata={'name': 'link_prediction', 'num_classes': 10},)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/regression_test/dgl/python/dgl/graphbolt/internal/utils.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n",
      "/home/ubuntu/regression_test/dgl/python/dgl/graphbolt/impl/ondisk_dataset.py:464: DGLWarning: Edge feature is stored, but edge IDs are not saved.\n",
      "  dgl_warning(\"Edge feature is stored, but edge IDs are not saved.\")\n",
      "/home/ubuntu/regression_test/dgl/python/dgl/graphbolt/impl/ondisk_dataset.py:856: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(graph_topology.path)\n"
     ]
    }
   ],
   "source": [
    "dataset = gb.OnDiskDataset(base_dir).load()\n",
    "graph = dataset.graph\n",
    "print(f\"Loaded graph: {graph}\\n\")\n",
    "\n",
    "feature = dataset.feature\n",
    "print(f\"Loaded feature store: {feature}\\n\")\n",
    "\n",
    "tasks = dataset.tasks\n",
    "nc_task = tasks[0]\n",
    "print(f\"Loaded node classification task: {nc_task}\\n\")\n",
    "lp_task = tasks[1]\n",
    "print(f\"Loaded link prediction task: {lp_task}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
