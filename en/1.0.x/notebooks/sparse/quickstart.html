<!DOCTYPE html>

<html class="writer-html5" data-content_root="../../" lang="en">
<head>
<meta charset="utf-8"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Quickstart — DGL 2.4 documentation</title>
<link href="../../_static/pygments.css?v=80d5e7a1" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/theme.css?v=19f00094" rel="stylesheet" type="text/css"/>
<link href="../../_static/graphviz.css?v=fd3f3429" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="../../_static/nbsphinx-code-cells.css?v=2aa19091" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/custom.css?v=0bf289b5" rel="stylesheet" type="text/css"/>
<!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script src="../../_static/jquery.js?v=5d32c60e"></script>
<script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
<script src="../../_static/documentation_options.js?v=9caaf7ed"></script>
<script src="../../_static/doctools.js?v=9a2dae69"></script>
<script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../_static/copybutton.js?v=ccdb6887"></script>
<script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
<script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="../../_static/js/theme.js"></script>
<link href="../../genindex.html" rel="index" title="Index"/>
<link href="../../search.html" rel="search" title="Search"/>
<link href="gcn.html" rel="next" title="Building a Graph Convolutional Network Using Sparse Matrices"/>
<link href="index.html" rel="prev" title="Tutorials: dgl.sparse"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../../index.html">
            DGL
          </a>
<div class="version">
                2.4
              </div>
<div role="search">
<form action="../../search.html" class="wy-form" id="rtd-search-form" method="get">
<input aria-label="Search docs" name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div><div aria-label="Navigation menu" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../stochastic_training/index.html">🆕 Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide_cn/index.html">用户指南【包含过时信息】</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide_ko/index.html">사용자 가이드[시대에 뒤쳐진]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../graphtransformer/index.html">🆕 Tutorial: Graph Transformer</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials: dgl.sparse</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="gcn.html">Building a Graph Convolutional Network Using Sparse Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="graph_diffusion.html">Graph Diffusion in Graph Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="hgnn.html">Hypergraph Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="graph_transformer.html">Graph Transformer in a Nutshell</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.graphbolt.html">🆕 dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resources.html">Resources</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift"><nav aria-label="Mobile navigation menu" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../../index.html">DGL</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="Page navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a aria-label="Home" class="icon icon-home" href="../../index.html"></a></li>
<li class="breadcrumb-item"><a href="index.html">Tutorials: dgl.sparse</a></li>
<li class="breadcrumb-item active">Quickstart</li>
<li class="wy-breadcrumbs-aside">
<a href="../../_sources/notebooks/sparse/quickstart.nblink.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<section id="Quickstart">
<h1>Quickstart<a class="headerlink" href="#Quickstart" title="Link to this heading"></a></h1>
<p>The tutorial provides a quick walkthrough of the classes and operators provided by the <code class="docutils literal notranslate"><span class="pre">dgl.sparse</span></code> package.</p>
<p><a class="reference external" href="https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/sparse/quickstart.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a> <a class="reference external" href="https://github.com/dmlc/dgl/blob/master/notebooks/sparse/quickstart.ipynb"><img alt="GitHub" src="https://img.shields.io/badge/-View%20on%20GitHub-181717?logo=github&amp;logoColor=ffffff"/></a></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the required packages.</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="c1"># Uncomment following commands to download Pytorch and DGL</span>
<span class="c1"># !pip install torch==2.0.0+cpu torchvision==0.15.1+cpu torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu &gt; /dev/null</span>
<span class="c1"># !pip install  dgl==1.1.0 -f https://data.dgl.ai/wheels/repo.html &gt; /dev/null</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'TORCH'</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'DGLBACKEND'</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"pytorch"</span>


<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">dgl.sparse</span> <span class="k">as</span> <span class="nn">dglsp</span>
    <span class="n">installed</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">installed</span> <span class="o">=</span> <span class="kc">False</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"DGL installed!"</span> <span class="k">if</span> <span class="n">installed</span> <span class="k">else</span> <span class="s2">"DGL not found!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
DGL installed!
</pre></div></div>
</div>
<section id="Sparse-Matrix">
<h2>Sparse Matrix<a class="headerlink" href="#Sparse-Matrix" title="Link to this heading"></a></h2>
<p>The core abstraction of DGL’s sparse package is the <code class="docutils literal notranslate"><span class="pre">SparseMatrix</span></code> class. Compared with other sparse matrix libraries (such as <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.sparse</span></code>), DGL’s <code class="docutils literal notranslate"><span class="pre">SparseMatrix</span></code> is specialized for the deep learning workloads on structure data (e.g., Graph Neural Networks), with the following features:</p>
<ul class="simple">
<li><p><strong>Auto sparse format.</strong> Don’t bother choosing between different sparse formats. There is only one <code class="docutils literal notranslate"><span class="pre">SparseMatrix</span></code> and it will select the best format for the operation to be performed.</p></li>
<li><p><strong>Non-zero elements can be scalar or vector.</strong> Easy for modeling relations (e.g., edges) by vector representation.</p></li>
<li><p><strong>Fully PyTorch compatible.</strong> The package is built upon PyTorch and is natively compatible with other tools in the PyTorch ecosystem.</p></li>
</ul>
<section id="Creating-a-DGL-Sparse-Matrix">
<h3>Creating a DGL Sparse Matrix<a class="headerlink" href="#Creating-a-DGL-Sparse-Matrix" title="Link to this heading"></a></h3>
<p>The simplest way to create a sparse matrix is using the <code class="docutils literal notranslate"><span class="pre">spmatrix</span></code> API by providing the indices of the non-zero elements. The indices are stored in a tensor of shape <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">nnz)</span></code>, where the <code class="docutils literal notranslate"><span class="pre">i</span></code>-th non-zero element is stored at position <code class="docutils literal notranslate"><span class="pre">(indices[0][i],</span> <span class="pre">indices[1][i])</span></code>. The code below creates a 3x3 sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">dgl.sparse</span> <span class="k">as</span> <span class="nn">dglsp</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>  <span class="c1"># 1.0 is default value for nnz elements.</span>

<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"In dense format:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
SparseMatrix(indices=tensor([[1, 1, 2],
                             [0, 2, 0]]),
             values=tensor([1., 1., 1.]),
             shape=(3, 3), nnz=3)

In dense format:
tensor([[0., 0., 0.],
        [1., 0., 1.],
        [1., 0., 0.]])
</pre></div></div>
</div>
<p>If not specified, the shape is inferred automatically from the indices but you can specify it explicitly too.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="n">A1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Implicit Shape: </span><span class="si">{</span><span class="n">A1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="n">A2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Explicit Shape: </span><span class="si">{</span><span class="n">A2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Implicit Shape: (2, 3)
tensor([[1., 0., 1.],
        [1., 0., 0.]])

Explicit Shape: (3, 3)
tensor([[1., 0., 1.],
        [1., 0., 0.],
        [0., 0., 0.]])
</pre></div></div>
</div>
<p>Both scalar values and vector values can be set for nnz elements in Sparse Matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="c1"># The length of the value should match the nnz elements represented by the</span>
<span class="c1"># sparse matrix format.</span>
<span class="n">scalar_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">vector_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"-----Scalar Values-----"</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">scalar_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"In dense format:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"-----Vector Values-----"</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">vector_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"In dense format:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-----Scalar Values-----
SparseMatrix(indices=tensor([[1, 1, 2],
                             [0, 2, 0]]),
             values=tensor([1., 2., 3.]),
             shape=(3, 3), nnz=3)

In dense format:
tensor([[0., 0., 0.],
        [1., 0., 2.],
        [3., 0., 0.]])

-----Vector Values-----
SparseMatrix(indices=tensor([[1, 1, 2],
                             [0, 2, 0]]),
             values=tensor([[1., 1.],
                            [2., 2.],
                            [3., 3.]]),
             shape=(3, 3), nnz=3, val_size=(2,))

In dense format:
tensor([[[0., 0.],
         [0., 0.],
         [0., 0.]],

        [[1., 1.],
         [0., 0.],
         [2., 2.]],

        [[3., 3.],
         [0., 0.],
         [0., 0.]]])
</pre></div></div>
</div>
<p><em>Duplicated indices</em></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Whether A contains duplicate indices: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">has_duplicate</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">coalesce</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Whether B contains duplicate indices: </span><span class="si">{</span><span class="n">B</span><span class="o">.</span><span class="n">has_duplicate</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
SparseMatrix(indices=tensor([[0, 0, 0, 1],
                             [0, 2, 2, 0]]),
             values=tensor([1., 2., 3., 4.]),
             shape=(2, 3), nnz=4)
Whether A contains duplicate indices: True

SparseMatrix(indices=tensor([[0, 0, 1],
                             [0, 2, 0]]),
             values=tensor([1., 5., 4.]),
             shape=(2, 3), nnz=3)
Whether B contains duplicate indices: False
</pre></div></div>
</div>
<p><strong>val_like</strong></p>
<p>You can create a new sparse matrix by retaining the non-zero indices of a given sparse matrix but with different non-zero values.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

<span class="n">new_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">val_like</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">new_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
SparseMatrix(indices=tensor([[1, 1, 2],
                             [0, 2, 0]]),
             values=tensor([4., 5., 6.]),
             shape=(3, 3), nnz=3)
</pre></div></div>
</div>
<p><strong>Create a sparse matrix from various sparse formats</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">from_coo()</span></code>: Create a sparse matrix from <a class="reference external" href="https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)">COO</a> format.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">from_csr()</span></code>: Create a sparse matrix from <a class="reference external" href="https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)">CSR</a> format.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">from_csc()</span></code>: Create a sparse matrix from <a class="reference external" href="https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS)">CSC</a> format.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">col</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"-----Create from COO format-----"</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">from_coo</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"In dense format:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="n">indptr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"-----Create from CSR format-----"</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">from_csr</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"In dense format:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"-----Create from CSC format-----"</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">from_csc</span><span class="p">(</span><span class="n">indptr</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"In dense format:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-----Create from COO format-----
SparseMatrix(indices=tensor([[0, 1, 2, 2, 2],
                             [1, 2, 0, 1, 2]]),
             values=tensor([1., 1., 1., 1., 1.]),
             shape=(3, 3), nnz=5)

In dense format:
tensor([[0., 1., 0.],
        [0., 0., 1.],
        [1., 1., 1.]])

-----Create from CSR format-----
SparseMatrix(indices=tensor([[0, 1, 2, 2, 2],
                             [1, 2, 0, 1, 2]]),
             values=tensor([1., 1., 1., 1., 1.]),
             shape=(3, 3), nnz=5)

In dense format:
tensor([[0., 1., 0.],
        [0., 0., 1.],
        [1., 1., 1.]])

-----Create from CSC format-----
SparseMatrix(indices=tensor([[1, 2, 0, 1, 2],
                             [0, 1, 2, 2, 2]]),
             values=tensor([1., 1., 1., 1., 1.]),
             shape=(3, 3), nnz=5)

In dense format:
tensor([[0., 0., 1.],
        [1., 0., 1.],
        [0., 1., 1.]])
</pre></div></div>
</div>
</section>
<section id="Attributes-and-methods-of-a-DGL-Sparse-Matrix">
<h3>Attributes and methods of a DGL Sparse Matrix<a class="headerlink" href="#Attributes-and-methods-of-a-DGL-Sparse-Matrix" title="Link to this heading"></a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Shape of sparse matrix: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The number of nonzero elements of sparse matrix: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">nnz</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Datatype of sparse matrix: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Device sparse matrix is stored on: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Get the values of the nonzero elements: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">val</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Get the row indices of the nonzero elements: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">row</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Get the column indices of the nonzero elements: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">col</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Get the coordinate (COO) representation: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">coo</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Get the compressed sparse row (CSR) representation: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">csr</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Get the compressed sparse column (CSC) representation: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">csc</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Shape of sparse matrix: (3, 3)
The number of nonzero elements of sparse matrix: 4
Datatype of sparse matrix: torch.float32
Device sparse matrix is stored on: cpu
Get the values of the nonzero elements: tensor([1., 2., 3., 4.])
Get the row indices of the nonzero elements: tensor([0, 1, 1, 2])
Get the column indices of the nonzero elements: tensor([1, 0, 2, 0])
Get the coordinate (COO) representation: (tensor([0, 1, 1, 2]), tensor([1, 0, 2, 0]))
Get the compressed sparse row (CSR) representation: (tensor([0, 1, 3, 4]), tensor([1, 0, 2, 0]), tensor([0, 1, 2, 3]))
Get the compressed sparse column (CSC) representation: (tensor([0, 2, 3, 4]), tensor([1, 2, 0, 1]), tensor([1, 3, 0, 2]))
</pre></div></div>
</div>
<p><strong>dtype and/or device conversion</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

<span class="n">B</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Device sparse matrix is stored on: </span><span class="si">{</span><span class="n">B</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Datatype of sparse matrix: </span><span class="si">{</span><span class="n">B</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Device sparse matrix is stored on: cpu
Datatype of sparse matrix: torch.int32
</pre></div></div>
</div>
<p>Similar to pytorch, we also provide various fine-grained APIs (<a class="reference external" href="https://docs.dgl.ai/en/latest/api/python/dgl.sparse_v0.html">Doc</a>) for dtype and/or device conversion.</p>
</section>
</section>
<section id="Diagonal-Matrix">
<h2>Diagonal Matrix<a class="headerlink" href="#Diagonal-Matrix" title="Link to this heading"></a></h2>
<p>Diagonal Matrix is a special type of Sparse Matrix, in which the entries outside the main diagonal are all zero.</p>
<section id="Initializing-a-DGL-Diagonal-Sparse-Matrix">
<h3>Initializing a DGL Diagonal Sparse Matrix<a class="headerlink" href="#Initializing-a-DGL-Diagonal-Sparse-Matrix" title="Link to this heading"></a></h3>
<p>A DGL Diagonal Sparse Matrix can be initiate by <code class="docutils literal notranslate"><span class="pre">dglsp.diag()</span></code>.</p>
<p>Identity Matrix is a special type of Diagonal Sparse Matrix, in which all the value on the diagonal are 1.0. Use <code class="docutils literal notranslate"><span class="pre">dglsp.identity()</span></code> to initiate a Diagonal Sparse Matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>

<span class="n">I</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
SparseMatrix(indices=tensor([[0, 1, 2, 3],
                             [0, 1, 2, 3]]),
             values=tensor([1., 2., 3., 4.]),
             shape=(4, 4), nnz=4)
SparseMatrix(indices=tensor([[0, 1, 2],
                             [0, 1, 2]]),
             values=tensor([1., 1., 1.]),
             shape=(3, 3), nnz=3)
</pre></div></div>
</div>
</section>
</section>
<section id="Operations-on-Sparse-Matrix">
<h2>Operations on Sparse Matrix<a class="headerlink" href="#Operations-on-Sparse-Matrix" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Elementwise operations</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">+</span> <span class="pre">B</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">-</span> <span class="pre">B</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">*</span> <span class="pre">B</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">/</span> <span class="pre">B</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">**</span> <span class="pre">scalar</span></code></p></li>
</ul>
</li>
<li><p>Broadcast operations</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">sp_&lt;op&gt;_v()</span></code></p></li>
</ul>
</li>
<li><p>Reduce operations</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">reduce()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sum()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">smax()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">smin()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">smean()</span></code></p></li>
</ul>
</li>
<li><p>Matrix transformations</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">SparseMatrix.transpose()</span></code> or <code class="docutils literal notranslate"><span class="pre">SparseMatrix.T</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SparseMatrix.neg()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SparseMatrix.inv()</span></code></p></li>
</ul>
</li>
<li><p>Matrix multiplication</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">matmul()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sddmm()</span></code></p></li>
</ul>
</li>
</ul>
<p><em>We are using dense format to print sparse matrix in this tutorial since it is more intuitive to read.</em></p>
<section id="Elementwise-operations">
<h3><em>Elementwise operations</em><a class="headerlink" href="#Elementwise-operations" title="Link to this heading"></a></h3>
<p><strong>add(A, B), equivalent to A + B</strong></p>
<p>Element-wise addition on two sparse matrices, returning a sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">D1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.</span><span class="p">])</span>
<span class="n">D2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 + A2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">+</span> <span class="n">A2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 + D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">+</span> <span class="n">D1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 + D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">+</span> <span class="n">D2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A1:
tensor([[0., 0., 0.],
        [1., 0., 2.],
        [3., 0., 0.]])
A2:
tensor([[4., 0., 0.],
        [0., 0., 5.],
        [0., 6., 0.]])
D1:
tensor([[-1.,  0.,  0.],
        [ 0., -2.,  0.],
        [ 0.,  0., -3.]])
D2:
tensor([[-4.,  0.,  0.],
        [ 0., -5.,  0.],
        [ 0.,  0., -6.]])
A1 + A2:
tensor([[4., 0., 0.],
        [1., 0., 7.],
        [3., 6., 0.]])
A1 + D1:
tensor([[-1.,  0.,  0.],
        [ 1., -2.,  2.],
        [ 3.,  0., -3.]])
D1 + D2:
tensor([[-5.,  0.,  0.],
        [ 0., -7.,  0.],
        [ 0.,  0., -9.]])
</pre></div></div>
</div>
<p><strong>sub(A, B), equivalent to A - B</strong></p>
<p>Element-wise substraction on two sparse matrices, returning a sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">D1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.</span><span class="p">])</span>
<span class="n">D2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 - A2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">-</span> <span class="n">A2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 - D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">-</span> <span class="n">D1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 - A1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">-</span> <span class="n">A1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 - D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">-</span> <span class="n">D2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A1:
tensor([[0., 0., 0.],
        [1., 0., 2.],
        [3., 0., 0.]])
A2:
tensor([[4., 0., 0.],
        [0., 0., 5.],
        [0., 6., 0.]])
D1:
tensor([[-1.,  0.,  0.],
        [ 0., -2.,  0.],
        [ 0.,  0., -3.]])
D2:
tensor([[-4.,  0.,  0.],
        [ 0., -5.,  0.],
        [ 0.,  0., -6.]])
A1 - A2:
tensor([[-4.,  0.,  0.],
        [ 1.,  0., -3.],
        [ 3., -6.,  0.]])
A1 - D1:
tensor([[1., 0., 0.],
        [1., 2., 2.],
        [3., 0., 3.]])
D1 - A1:
tensor([[-1.,  0.,  0.],
        [-1., -2., -2.],
        [-3.,  0., -3.]])
D1 - D2:
tensor([[3., 0., 0.],
        [0., 3., 0.],
        [0., 0., 3.]])
</pre></div></div>
</div>
<p><strong>mul(A, B), equivalent to A * B</strong></p>
<p>Element-wise multiplication on two sparse matrices or on a sparse matrix and a scalar, returning a sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 * 3:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"3 * A1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="mi">3</span> <span class="o">*</span> <span class="n">A1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 * A2"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">*</span> <span class="n">A2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">D1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 * A2"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">*</span> <span class="n">A2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.</span><span class="p">])</span>
<span class="n">D2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 * -2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">*</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-2 * D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">D1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 * D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">*</span> <span class="n">D2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A1:
tensor([[0., 0., 0.],
        [1., 0., 2.],
        [3., 0., 0.]])
A2:
tensor([[1., 0., 0.],
        [0., 0., 2.],
        [3., 4., 0.]])
A1 * 3:
tensor([[0., 0., 0.],
        [3., 0., 6.],
        [9., 0., 0.]])
3 * A1:
tensor([[0., 0., 0.],
        [3., 0., 6.],
        [9., 0., 0.]])
A1 * A2
tensor([[0., 0., 0.],
        [0., 0., 4.],
        [9., 0., 0.]])
D1:
tensor([[-1.,  0.,  0.],
        [ 0., -2.,  0.],
        [ 0.,  0., -3.]])
D1 * A2
tensor([[-1.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])
D2:
tensor([[-4.,  0.,  0.],
        [ 0., -5.,  0.],
        [ 0.,  0., -6.]])
D1 * -2:
tensor([[2., 0., 0.],
        [0., 4., 0.],
        [0., 0., 6.]])
-2 * D1:
tensor([[2., 0., 0.],
        [0., 4., 0.],
        [0., 0., 6.]])
D1 * D2:
tensor([[ 4.,  0.,  0.],
        [ 0., 10.,  0.],
        [ 0.,  0., 18.]])
</pre></div></div>
</div>
<p><strong>div(A, B), equivalent to A / B</strong></p>
<p>Element-wise multiplication on two sparse matrices or on a sparse matrix and a scalar, returning a sparse matrix. If both <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code> are sparse matrices, both of them must have the same sparsity. And the returned matrix has the same order of non-zero entries as <code class="docutils literal notranslate"><span class="pre">A</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 / 2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 / A2"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">/</span> <span class="n">A2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">D1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.</span><span class="p">])</span>
<span class="n">D2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 / D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">/</span> <span class="n">D2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 / 2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A1:
tensor([[0., 0., 0.],
        [1., 0., 2.],
        [3., 0., 0.]])
A1 / 2:
tensor([[0.0000, 0.0000, 0.0000],
        [0.5000, 0.0000, 1.0000],
        [1.5000, 0.0000, 0.0000]])
A1 / A2
tensor([[0., 0., 0.],
        [1., 0., 1.],
        [1., 0., 0.]])
D1:
tensor([[-1.,  0.,  0.],
        [ 0., -2.,  0.],
        [ 0.,  0., -3.]])
D2:
tensor([[-4.,  0.,  0.],
        [ 0., -5.,  0.],
        [ 0.,  0., -6.]])
D1 / D2:
tensor([[0.2500, 0.0000, 0.0000],
        [0.0000, 0.4000, 0.0000],
        [0.0000, 0.0000, 0.5000]])
D1 / 2:
tensor([[-0.5000,  0.0000,  0.0000],
        [ 0.0000, -1.0000,  0.0000],
        [ 0.0000,  0.0000, -1.5000]])
</pre></div></div>
</div>
<p><strong>power(A, B), equivalent to A ** B</strong></p>
<p>Element-wise power of a sparse matrix and a scalar, returning a sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A ** 3:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 ** 2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A:
tensor([[0., 0., 0.],
        [1., 0., 2.],
        [3., 0., 0.]])
A ** 3:
tensor([[ 0.,  0.,  0.],
        [ 1.,  0.,  8.],
        [27.,  0.,  0.]])
D:
tensor([[-1.,  0.,  0.],
        [ 0., -2.,  0.],
        [ 0.,  0., -3.]])
D1 ** 2:
tensor([[1., 0., 0.],
        [0., 4., 0.],
        [0., 0., 9.]])
</pre></div></div>
</div>
</section>
<section id="Broadcast-operations">
<h3><em>Broadcast operations</em><a class="headerlink" href="#Broadcast-operations" title="Link to this heading"></a></h3>
<p>**sp_&lt;op&gt;_v(A, v)**</p>
<p>Broadcast operations on a sparse matrix and a vector, returning a sparse matrix. <code class="docutils literal notranslate"><span class="pre">v</span></code> is broadcasted to the shape of <code class="docutils literal notranslate"><span class="pre">A</span></code> and then the operator is applied on the non-zero values of <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">&lt;op&gt;</span></code> can be add, sub, mul, and div.</p>
<p>There are two cases regarding the shape of <code class="docutils literal notranslate"><span class="pre">v</span></code>:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">v</span></code> is a vector of shape <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">A.shape[1])</span></code> or <code class="docutils literal notranslate"><span class="pre">(A.shape[1])</span></code>. In this case, <code class="docutils literal notranslate"><span class="pre">v</span></code> is broadcasted on the row dimension of <code class="docutils literal notranslate"><span class="pre">A</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">v</span></code> is a vector of shape <code class="docutils literal notranslate"><span class="pre">(A.shape[0],</span> <span class="pre">1)</span></code>. In this case, <code class="docutils literal notranslate"><span class="pre">v</span></code> is broadcasted on the column dimension of <code class="docutils literal notranslate"><span class="pre">A</span></code>.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">v1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"v1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"sp_add_v(A, v1)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dglsp</span><span class="o">.</span><span class="n">sp_add_v</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">v2</span> <span class="o">=</span> <span class="n">v1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"v2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">v2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"sp_add_v(A, v2)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dglsp</span><span class="o">.</span><span class="n">sp_add_v</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">v3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"v3:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">v3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"sp_add_v(A, v3)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dglsp</span><span class="o">.</span><span class="n">sp_add_v</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v3</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A:
tensor([[ 0,  0,  0, 20],
        [10,  0,  0,  0],
        [ 0,  0, 30,  0]])
v1:
tensor([1, 2, 3, 4])
sp_add_v(A, v1)
tensor([[ 0,  0,  0, 24],
        [11,  0,  0,  0],
        [ 0,  0, 33,  0]])
v2:
tensor([[1, 2, 3, 4]])
sp_add_v(A, v2)
tensor([[ 0,  0,  0, 24],
        [11,  0,  0,  0],
        [ 0,  0, 33,  0]])
v3:
tensor([[1],
        [2],
        [3]])
sp_add_v(A, v3)
tensor([[ 0,  0,  0, 21],
        [12,  0,  0,  0],
        [ 0,  0, 33,  0]])
</pre></div></div>
</div>
</section>
<section id="Reduce-operations">
<h3><em>Reduce operations</em><a class="headerlink" href="#Reduce-operations" title="Link to this heading"></a></h3>
<p>All DGL sparse reduce operations only consider non-zero elements. To distinguish them from dense PyTorch reduce operations that consider zero elements, we use name <code class="docutils literal notranslate"><span class="pre">smax</span></code>, <code class="docutils literal notranslate"><span class="pre">smin</span></code> and <code class="docutils literal notranslate"><span class="pre">smean</span></code> (<code class="docutils literal notranslate"><span class="pre">s</span></code> stands for sparse).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="c1"># O1, O2 will have the same value.</span>
<span class="n">O1</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'sum'</span><span class="p">)</span>
<span class="n">O2</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Reduce with reducer:sum along dim = 0:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">O1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="c1"># O3, O4 will have the same value.</span>
<span class="n">O3</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'smax'</span><span class="p">)</span>
<span class="n">O4</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">smax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Reduce with reducer:max along dim = 0:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">O3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="c1"># O5, O6 will have the same value.</span>
<span class="n">O5</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'smin'</span><span class="p">)</span>
<span class="n">O6</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">smin</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Reduce with reducer:min along dim = 0:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">O5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="c1"># O7, O8 will have the same value.</span>
<span class="n">O7</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'smean'</span><span class="p">)</span>
<span class="n">O8</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">smean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Reduce with reducer:smean along dim = 0:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">O7</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[0., 2., 4.],
        [1., 0., 0.],
        [0., 3., 0.]])

Reduce with reducer:sum along dim = 0:
tensor([6., 1., 3.])

Reduce with reducer:max along dim = 0:
tensor([4., 1., 3.])

Reduce with reducer:min along dim = 0:
tensor([2., 1., 3.])

Reduce with reducer:smean along dim = 0:
tensor([3., 1., 3.])

</pre></div></div>
</div>
</section>
<section id="Matrix-transformations">
<h3><em>Matrix transformations</em><a class="headerlink" href="#Matrix-transformations" title="Link to this heading"></a></h3>
<p><em>Sparse Matrix</em></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Get transpose of sparse matrix."</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="c1"># Alias</span>
<span class="c1"># A.transpose()</span>
<span class="c1"># A.t()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Get a sparse matrix with the negation of the original nonzero values."</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[0., 1., 0.],
        [2., 0., 3.],
        [4., 0., 0.]])

Get transpose of sparse matrix.
tensor([[0., 2., 4.],
        [1., 0., 0.],
        [0., 3., 0.]])

Get a sparse matrix with the negation of the original nonzero values.
tensor([[ 0., -1.,  0.],
        [-2.,  0., -3.],
        [-4.,  0.,  0.]])

</pre></div></div>
</div>
</section>
<section id="Matrix-multiplication">
<h3><em>Matrix multiplication</em><a class="headerlink" href="#Matrix-multiplication" title="Link to this heading"></a></h3>
<p><strong>matmul(A, B), equivalent to A @ B</strong></p>
<p>Matrix multiplication on sparse matrices and/or dense matrix. There are two cases as follows.</p>
<p><strong>SparseMatrix @ SparseMatrix -&gt; SparseMatrix:</strong></p>
<p>For a <span class="math notranslate nohighlight">\(L \times M\)</span> sparse matrix A and a <span class="math notranslate nohighlight">\(M \times N\)</span> sparse matrix B, the shape of <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">@</span> <span class="pre">B</span></code> will be <span class="math notranslate nohighlight">\(L \times N\)</span> sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">D1</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D1</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">4.</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.</span><span class="p">])</span>
<span class="n">D2</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D2</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 @ A2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">@</span> <span class="n">A2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A1 @ D1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">A1</span> <span class="o">@</span> <span class="n">D1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 @ A1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">@</span> <span class="n">A1</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D1 @ D2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">D1</span> <span class="o">@</span> <span class="n">D2</span><span class="p">)</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A1:
tensor([[0., 0., 0.],
        [1., 0., 2.],
        [3., 0., 0.]])
A2:
tensor([[4., 0., 0.],
        [0., 0., 5.],
        [0., 6., 0.]])
D1:
tensor([[-1.,  0.,  0.],
        [ 0., -2.,  0.],
        [ 0.,  0., -3.]])
D2:
tensor([[-4.,  0.,  0.],
        [ 0., -5.,  0.],
        [ 0.,  0., -6.]])
A1 @ A2:
tensor([[ 0.,  0.,  0.],
        [ 4., 12.,  0.],
        [12.,  0.,  0.]])
A1 @ D1:
tensor([[ 0.,  0.,  0.],
        [-1.,  0., -6.],
        [-3.,  0.,  0.]])
D1 @ A1:
tensor([[ 0.,  0.,  0.],
        [-2.,  0., -4.],
        [-9.,  0.,  0.]])
D1 @ D2:
tensor([[ 4.,  0.,  0.],
        [ 0., 10.,  0.],
        [ 0.,  0., 18.]])
</pre></div></div>
</div>
<p><strong>SparseMatrix @ Tensor -&gt; Tensor:</strong></p>
<p>For a <span class="math notranslate nohighlight">\(L \times M\)</span> sparse matrix A and a <span class="math notranslate nohighlight">\(M \times N\)</span> dense matrix B, the shape of <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">@</span> <span class="pre">B</span></code> will be <span class="math notranslate nohighlight">\(L \times N\)</span> dense matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"D:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">11.</span><span class="p">,</span> <span class="mf">22.</span><span class="p">],</span> <span class="p">[</span><span class="mf">33.</span><span class="p">,</span> <span class="mf">44.</span><span class="p">],</span> <span class="p">[</span><span class="mf">55.</span><span class="p">,</span> <span class="mf">66.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"X:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A @ X:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"D @ X:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A:
tensor([[0., 0., 0.],
        [1., 0., 2.],
        [3., 0., 0.]])
D:
tensor([[-1.,  0.,  0.],
        [ 0., -2.,  0.],
        [ 0.,  0., -3.]])
X:
tensor([[11., 22.],
        [33., 44.],
        [55., 66.]])
A @ X:
tensor([[  0.,   0.],
        [121., 154.],
        [ 33.,  66.]])
D @ X:
tensor([[ -11.,  -22.],
        [ -66.,  -88.],
        [-165., -198.]])
</pre></div></div>
</div>
<p>This operator also supports batched sparse-dense matrix multiplication. The sparse matrix A should have shape <span class="math notranslate nohighlight">\(L \times M\)</span>, where the non-zero values are vectors of length <span class="math notranslate nohighlight">\(K\)</span>. The dense matrix B should have shape <span class="math notranslate nohighlight">\(M \times N \times K\)</span>. The output is a dense matrix of shape <span class="math notranslate nohighlight">\(L \times N \times K\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">]]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"X:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"A @ X:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A:
tensor([[[0., 0.],
         [0., 0.],
         [0., 0.]],

        [[1., 1.],
         [0., 0.],
         [2., 2.]],

        [[3., 3.],
         [0., 0.],
         [0., 0.]]])
X:
tensor([[[1., 1.],
         [1., 2.]],

        [[1., 3.],
         [1., 4.]],

        [[1., 5.],
         [1., 6.]]])
A @ X:
tensor([[[ 0.,  0.],
         [ 0.,  0.]],

        [[ 3., 11.],
         [ 3., 14.]],

        [[ 3.,  3.],
         [ 3.,  6.]]])
</pre></div></div>
</div>
<p><strong>Sampled-Dense-Dense Matrix Multiplication (SDDMM)</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">sddmm</span></code> matrix-multiplies two dense matrices X1 and X2, then elementwise-multiplies the result with sparse matrix A at the nonzero locations. This is designed for sparse matrix with scalar values.</p>
<div class="math notranslate nohighlight">
\[out = (X_1 @ X_2) * A\]</div>
<p>For a <span class="math notranslate nohighlight">\(L \times N\)</span> sparse matrix A, a <span class="math notranslate nohighlight">\(L \times M\)</span> dense matrix X1 and a <span class="math notranslate nohighlight">\(M \times N\)</span> dense matrix X2, <code class="docutils literal notranslate"><span class="pre">sddmm(A,</span> <span class="pre">X1,</span> <span class="pre">X2)</span></code> will be a <span class="math notranslate nohighlight">\(L \times N\)</span> sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"X1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"X2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>

<span class="n">O</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">sddmm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"dglsp.sddmm(A, X1, X2):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">O</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A:
tensor([[0., 0., 0., 0.],
        [0., 0., 1., 2.],
        [0., 0., 0., 3.]])
X1:
tensor([[ 0.3471, -0.1945, -1.5097,  0.7811,  1.2748],
        [-0.1656, -1.3935, -0.7953,  0.4507, -0.1917],
        [-1.4689,  0.5499, -0.4853, -0.4078,  0.1828]])
X2:
tensor([[ 0.4891,  0.8764,  1.5716,  0.5316],
        [-2.0849,  2.3157, -0.1347,  0.3110],
        [-0.9291,  0.1487,  0.7515,  0.4638],
        [-0.4021,  0.9450,  1.1683, -0.1853],
        [ 0.4244, -2.6518, -0.8808, -1.0965]])
dglsp.sddmm(A, X1, X2):
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0252, -1.5271],
        [ 0.0000,  0.0000,  0.0000, -2.8793]])
</pre></div></div>
</div>
<p>This operator also supports batched sampled-dense-dense matrix multiplication. For a <span class="math notranslate nohighlight">\(L \times N\)</span> sparse matrix A with non-zero vector values of length <span class="math notranslate nohighlight">\(𝐾\)</span>, a <span class="math notranslate nohighlight">\(L \times M \times K\)</span> dense matrix X1 and a <span class="math notranslate nohighlight">\(M \times N \times K\)</span> dense matrix X2, <code class="docutils literal notranslate"><span class="pre">sddmm(A,</span> <span class="pre">X1,</span> <span class="pre">X2)</span></code> will be a <span class="math notranslate nohighlight">\(L \times N \times K\)</span> sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"A:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="n">X1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"X1:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"X2:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>

<span class="n">O</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">sddmm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"dglsp.sddmm(A, X1, X2):"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">O</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
A:
tensor([[[0., 0.],
         [0., 0.],
         [0., 0.],
         [0., 0.]],

        [[0., 0.],
         [0., 0.],
         [1., 1.],
         [2., 2.]],

        [[0., 0.],
         [0., 0.],
         [0., 0.],
         [3., 3.]]])
X1:
tensor([[[ 0.1359,  0.2786],
         [-0.9732,  0.5334],
         [-0.9758, -0.6175],
         [ 0.4014, -0.5210],
         [-0.0537, -0.8934]],

        [[-0.4646, -0.3364],
         [-0.2217,  0.6576],
         [ 0.8214, -0.2737],
         [ 1.4741,  0.5795],
         [-0.7861, -2.1430]],

        [[ 0.4699,  0.1671],
         [ 0.2199,  0.5500],
         [-1.2037, -0.4075],
         [ 0.0926, -0.7754],
         [-0.9068, -0.6686]]])
X2:
tensor([[[ 0.4024,  0.5086],
         [-0.0575,  0.1972],
         [ 1.1563, -0.2520],
         [ 0.2973, -0.2891]],

        [[-0.2147,  1.9438],
         [ 0.6457,  0.0734],
         [-1.6680,  0.5196],
         [ 0.7596, -1.1444]],

        [[ 0.3792, -1.1221],
         [-0.2055, -0.5243],
         [-0.5580, -0.4558],
         [-2.0326,  1.4424]],

        [[ 0.1349, -2.1453],
         [-1.1075,  0.8044],
         [ 1.3182, -0.1639],
         [-0.4192, -1.4963]],

        [[ 1.3023,  0.8617],
         [-1.0489, -0.1789],
         [ 0.1725,  0.2870],
         [ 0.9317, -1.7694]]])
dglsp.sddmm(A, X1, X2):
tensor([[[ 0.0000,  0.0000],
         [ 0.0000,  0.0000],
         [ 0.0000,  0.0000],
         [ 0.0000,  0.0000]],

        [[ 0.0000,  0.0000],
         [ 0.0000,  0.0000],
         [ 1.1818, -0.1589],
         [-6.6530,  3.7489]],

        [[ 0.0000,  0.0000],
         [ 0.0000,  0.0000],
         [ 0.0000,  0.0000],
         [ 5.6091,  3.2334]]])
</pre></div></div>
</div>
</section>
</section>
<section id="Non-linear-activation-functions">
<h2>Non-linear activation functions<a class="headerlink" href="#Non-linear-activation-functions" title="Link to this heading"></a></h2>
<section id="Element-wise-functions">
<h3>Element-wise functions<a class="headerlink" href="#Element-wise-functions" title="Link to this heading"></a></h3>
<p>Most activation functions are element-wise and can be further grouped into two categories:</p>
<p><strong>Sparse-preserving functions</strong> such as <code class="docutils literal notranslate"><span class="pre">sin()</span></code>, <code class="docutils literal notranslate"><span class="pre">tanh()</span></code>, <code class="docutils literal notranslate"><span class="pre">sigmoid()</span></code>, <code class="docutils literal notranslate"><span class="pre">relu()</span></code>, etc. You can directly apply them on the <code class="docutils literal notranslate"><span class="pre">val</span></code> tensor of the sparse matrix and then recreate a new matrix of the same sparsity using <code class="docutils literal notranslate"><span class="pre">val_like</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Apply tanh."</span><span class="p">)</span>
<span class="n">A_new</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">val_like</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">val</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A_new</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[ 0.0000, -0.6030,  0.0000],
        [ 1.2860,  0.0000, -0.0838],
        [-0.2079,  0.0000,  0.0000]])
Apply tanh.
tensor([[ 0.0000, -0.5392,  0.0000],
        [ 0.8581,  0.0000, -0.0836],
        [-0.2049,  0.0000,  0.0000]])
</pre></div></div>
</div>
<p><strong>Non-sparse-preserving functions</strong> such as <code class="docutils literal notranslate"><span class="pre">exp()</span></code>, <code class="docutils literal notranslate"><span class="pre">cos()</span></code>, etc. You can first convert the sparse matrix to dense before applying the functions.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Apply exp."</span><span class="p">)</span>
<span class="n">A_new</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">to_dense</span><span class="p">()</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A_new</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[ 0.0000, -0.1111,  0.0000],
        [ 0.4965,  0.0000,  3.7734],
        [ 0.0673,  0.0000,  0.0000]])
Apply exp.
tensor([[ 1.0000,  0.8948,  1.0000],
        [ 1.6430,  1.0000, 43.5275],
        [ 1.0696,  1.0000,  1.0000]])
</pre></div></div>
</div>
</section>
<section id="Softmax">
<h3>Softmax<a class="headerlink" href="#Softmax" title="Link to this heading"></a></h3>
<p>Apply row-wise softmax to the nonzero entries of the sparse matrix.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">softmax</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"In dense format:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">softmax</span><span class="p">()</span><span class="o">.</span><span class="n">to_dense</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
SparseMatrix(indices=tensor([[0, 1, 1, 2],
                             [1, 0, 2, 0]]),
             values=tensor([1.0000, 0.2689, 0.7311, 1.0000]),
             shape=(3, 3), nnz=4)
In dense format:
tensor([[0.0000, 1.0000, 0.0000],
        [0.2689, 0.0000, 0.7311],
        [1.0000, 0.0000, 0.0000]])


</pre></div></div>
</div>
</section>
</section>
<section id="Exercise-#1">
<h2>Exercise #1<a class="headerlink" href="#Exercise-#1" title="Link to this heading"></a></h2>
<p><em>Let’s test what you’ve learned. Feel free to</em> <a class="reference external" href="https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/sparse/quickstart.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a><em>.</em></p>
<p>Given a sparse symmetrical adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, calculate its symmetrically normalized adjacency matrix:</p>
<div class="math notranslate nohighlight">
\[norm = \bar{D}^{-\frac{1}{2}}\bar{A}\bar{D}^{-\frac{1}{2}}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\bar{A} = A + I\)</span>, <span class="math notranslate nohighlight">\(I\)</span> is the identity matrix, and <span class="math notranslate nohighlight">\(\bar{D}\)</span> is the diagonal node degree matrix of <span class="math notranslate nohighlight">\(\bar{A}\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">asym_A</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="c1"># Step 1: create symmetrical adjacency matrix A from asym_A.</span>
<span class="c1"># A =</span>

<span class="c1"># Step 2: calculate A_hat from A.</span>
<span class="c1"># A_hat =</span>

<span class="c1"># Step 3: diagonal node degree matrix of A_hat</span>
<span class="c1"># D_hat =</span>

<span class="c1"># Step 4: calculate the norm from D_hat and A_hat.</span>
<span class="c1"># norm =</span>
</pre></div>
</div>
</div>
</section>
<section id="Exercise-#2">
<h2>Exercise #2<a class="headerlink" href="#Exercise-#2" title="Link to this heading"></a></h2>
<p>Let’s implement a simplified version of the Graph Attention Network (GAT) layer.</p>
<p>A GAT layer has two inputs: the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> and the node input features <span class="math notranslate nohighlight">\(X\)</span>. The idea of GAT layer is to update each node’s representation with a weighted average of the node’s own representation and its neighbors’ representations. In particular, when computing the output for node <span class="math notranslate nohighlight">\(i\)</span>, the GAT layer does the following: 1. Compute the scores <span class="math notranslate nohighlight">\(S_{ij}\)</span> representing the attention logit from neighbor <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>. <span class="math notranslate nohighlight">\(S_{ij}\)</span> is a function of
<span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>’s input features <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[S_{ij} = LeakyReLU(X_i^\top v_1 + X_j^\top v_2)\]</div>
<p>, where <span class="math notranslate nohighlight">\(v_1\)</span> and <span class="math notranslate nohighlight">\(v_2\)</span> are trainable vectors. 2. Compute a softmax attention <span class="math notranslate nohighlight">\(R_{ij} = \exp S_{ij} / \left( \sum_{j' \in \mathcal{N}_i} s_{ij'} \right)\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{N}_j\)</span> means the neighbors of <span class="math notranslate nohighlight">\(j\)</span>. This means that <span class="math notranslate nohighlight">\(R\)</span> is a row-wise softmax attention of <span class="math notranslate nohighlight">\(S\)</span>. 3. Compute the weighted average <span class="math notranslate nohighlight">\(H_i = \sum_{j' : j' \in \mathcal{N}_i} R_{j'} X_{j'} W\)</span>, where <span class="math notranslate nohighlight">\(W\)</span> is a trainable matrix.</p>
<p>The following code defined all the parameters you need but only completes step 1. Could you implement step 2 and step 3?</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">SimplifiedGAT</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># A: A sparse matrix with size (N, N).  A[i, j] represent the edge from j to i.</span>
        <span class="c1"># X: A dense matrix with size (N, D)</span>
        <span class="c1"># Step 1: compute S[i, j]</span>
        <span class="n">Xv1</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">v1</span>
        <span class="n">Xv2</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">v2</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">Xv1</span><span class="p">[</span><span class="n">A</span><span class="o">.</span><span class="n">col</span><span class="p">]</span> <span class="o">+</span> <span class="n">Xv2</span><span class="p">[</span><span class="n">A</span><span class="o">.</span><span class="n">row</span><span class="p">])</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">dglsp</span><span class="o">.</span><span class="n">val_like</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

        <span class="c1"># Step 2: compute R[i, j] which is the row-wise attention of $S$.</span>
        <span class="c1"># EXERCISE: replace the statement below.</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">S</span>

        <span class="c1"># Step 3: compute H.</span>
        <span class="c1"># EXERCISE: replace the statement below.</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">X</span>

        <span class="k">return</span> <span class="n">H</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test:</span>
<span class="c1"># Let's use the symmetric A created above.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">module</span> <span class="o">=</span> <span class="n">SimplifiedGAT</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
</section>
</div>
</div>
<footer><div aria-label="Footer" class="rst-footer-buttons" role="navigation">
<a accesskey="p" class="btn btn-neutral float-left" href="index.html" rel="prev" title="Tutorials: dgl.sparse"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
<a accesskey="n" class="btn btn-neutral float-right" href="gcn.html" rel="next" title="Building a Graph Convolutional Network Using Sparse Matrices">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
</div>
<hr/>
<div role="contentinfo">
<p>© Copyright 2018, DGL Team.</p>
</div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
</div>
</div>
</section>
</div>
<div aria-label="Versions" class="rst-versions" data-toggle="rst-versions" role="note">
<span class="rst-current-version" data-toggle="rst-current-version">
<span class="fa fa-book"> Read the Docs</span>
<span id="version-placeholder">v: latest</span>
<span class="fa fa-caret-down"></span>
</span>
<div class="rst-other-versions">
<dl>
<dt>Versions</dt>
<div id="version-list">
<!-- 动态插入的版本列表将出现在这里 -->
</div>
</dl>
<dl>
<dt>Downloads</dt>
<!-- 下载内容 -->
</dl>
<dl>
<dt>On Read the Docs</dt>
<dd><a href="//doc-build.dgl.ai/projects/dgl/?fromdocs=dgl">Project Home</a></dd>
<dd><a href="//doc-build.dgl.ai/builds/dgl/?fromdocs=dgl">Builds</a></dd>
</dl>
</div>
</div>
<script>
        document.addEventListener("DOMContentLoaded", function() {
            fetch('/dgl_docs/branches.json')
                .then(response => response.json())
                .then(data => {
                    var versionListDiv = document.getElementById('version-list');
                    data.branches.forEach(function(branch) {
                        var dd = document.createElement('dd');
                        var a = document.createElement('a');
                        a.href = branch.url;
                        a.textContent = branch.name;
                        dd.appendChild(a);
                        versionListDiv.appendChild(dd);
                    });
                })
                .catch(error => console.error('Error loading branches:', error));
        });
        document.addEventListener("DOMContentLoaded", function() {
            // 获取当前路径
            var path = window.location.pathname;
            var versionPlaceholder = document.getElementById('version-placeholder');

            // 检查路径中是否包含 'en'
            if (path.includes('/en/')) {
                // 提取 'en' 后的文件夹作为版本号
                var parts = path.split('/en/');
                if (parts[1]) {
                    var folders = parts[1].split('/');
                    if (folders.length > 0 && folders[0]) {
                        versionPlaceholder.textContent = 'v: ' + folders[0];
                    } else {
                        versionPlaceholder.textContent = 'v: latest';
                    }
                } else {
                    versionPlaceholder.textContent = 'v: latest';
                }
            } else {
                versionPlaceholder.textContent = 'v: latest';
            }
        });
    </script>

<div aria-label="Versions" class="rst-versions" data-toggle="rst-versions" role="note">
<span class="rst-current-version" data-toggle="rst-current-version">
<span class="fa fa-book"> Read the Docs</span>
<span id="version-placeholder">v: latest</span>
<span class="fa fa-caret-down"></span>
</span>
<div class="rst-other-versions">
<dl>
<dt>Versions</dt>
<div id="version-list">
<!-- 动态插入的版本列表将出现在这里 -->
</div>
</dl>
<dl>
<dt>Downloads</dt>
<!-- 下载内容 -->
</dl>
<dl>
<dt>On Read the Docs</dt>
<dd><a href="//doc-build.dgl.ai/projects/dgl/?fromdocs=dgl">Project Home</a></dd>
<dd><a href="//doc-build.dgl.ai/builds/dgl/?fromdocs=dgl">Builds</a></dd>
</dl>
</div>
</div>
<script>
        document.addEventListener("DOMContentLoaded", function() {
            fetch('/dgl_docs/branches.json')
                .then(response => response.json())
                .then(data => {
                    var versionListDiv = document.getElementById('version-list');
                    data.branches.forEach(function(branch) {
                        var dd = document.createElement('dd');
                        var a = document.createElement('a');
                        a.href = branch.url;
                        a.textContent = branch.name;
                        dd.appendChild(a);
                        versionListDiv.appendChild(dd);
                    });
                })
                .catch(error => console.error('Error loading branches:', error));
        });
        document.addEventListener("DOMContentLoaded", function() {
            // 获取当前路径
            var path = window.location.pathname;
            var versionPlaceholder = document.getElementById('version-placeholder');

            // 检查路径中是否包含 'en'
            if (path.includes('/en/')) {
                // 提取 'en' 后的文件夹作为版本号
                var parts = path.split('/en/');
                if (parts[1]) {
                    var folders = parts[1].split('/');
                    if (folders.length > 0 && folders[0]) {
                        versionPlaceholder.textContent = 'v: ' + folders[0];
                    } else {
                        versionPlaceholder.textContent = 'v: latest';
                    }
                } else {
                    versionPlaceholder.textContent = 'v: latest';
                }
            } else {
                versionPlaceholder.textContent = 'v: latest';
            }
        });
    </script>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
</body>
</html>