<!DOCTYPE html>

<html class="writer-html5" data-content_root="../../" lang="en">
<head>
<meta charset="utf-8"/><meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Single Machine Multi-GPU Minibatch Node Classification ‚Äî DGL 1.1.3 documentation</title>
<link href="../../_static/pygments.css?v=80d5e7a1" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/theme.css?v=19f00094" rel="stylesheet" type="text/css"/>
<link href="../../_static/graphviz.css?v=fd3f3429" rel="stylesheet" type="text/css"/>
<link href="../../_static/copybutton.css?v=76b2166b" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery.css?v=d2d258e8" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-binder.css?v=f4aeca0c" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" rel="stylesheet" type="text/css"/>
<link href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" rel="stylesheet" type="text/css"/>
<link href="../../_static/css/custom.css?v=0bf289b5" rel="stylesheet" type="text/css"/>
<!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script src="../../_static/jquery.js?v=5d32c60e"></script>
<script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
<script src="../../_static/documentation_options.js?v=cb7bf70b"></script>
<script src="../../_static/doctools.js?v=9a2dae69"></script>
<script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
<script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
<script src="../../_static/copybutton.js?v=ccdb6887"></script>
<script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="../../_static/js/theme.js"></script>
<link href="../../genindex.html" rel="index" title="Index"/>
<link href="../../search.html" rel="search" title="Search"/>
<link href="../dist/index.html" rel="next" title="Distributed training"/>
<link href="1_graph_classification.html" rel="prev" title="Single Machine Multi-GPU Minibatch Graph Classification"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="../../index.html">
            DGL
          </a>
<div class="version">
                1.1.3
              </div>
<div role="search">
<form action="../../search.html" class="wy-form" id="rtd-search-form" method="get">
<input aria-label="Search docs" name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div><div aria-label="Navigation menu" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide_cn/index.html">Áî®Êà∑ÊåáÂçó</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide_ko/index.html">ÏÇ¨Ïö©Ïûê Í∞ÄÏù¥Îìú</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/sparse/index.html">üÜï Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../large/index.html">Stochastic Training of GNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Training on Multiple GPUs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="1_graph_classification.html">Single Machine Multi-GPU Minibatch Graph Classification</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Single Machine Multi-GPU Minibatch Node Classification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/nn-tensorflow.html">dgl.nn (TensorFlow)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/nn-mxnet.html">dgl.nn (MXNet)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.sparse_v0.html">üÜï dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resources.html">Resources</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift"><nav aria-label="Mobile navigation menu" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="../../index.html">DGL</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="Page navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a aria-label="Home" class="icon icon-home" href="../../index.html"></a></li>
<li class="breadcrumb-item"><a href="index.html">Training on Multiple GPUs</a></li>
<li class="breadcrumb-item active">Single Machine Multi-GPU Minibatch Node Classification</li>
<li class="wy-breadcrumbs-aside">
<a href="../../_sources/tutorials/multi/2_node_classification.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-tutorials-multi-2-node-classification-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="single-machine-multi-gpu-minibatch-node-classification">
<span id="sphx-glr-tutorials-multi-2-node-classification-py"></span><h1>Single Machine Multi-GPU Minibatch Node Classification<a class="headerlink" href="#single-machine-multi-gpu-minibatch-node-classification" title="Link to this heading">ÔÉÅ</a></h1>
<p>In this tutorial, you will learn how to use multiple GPUs in training a
graph neural network (GNN) for node classification.</p>
<p>(Time estimate: 8 minutes)</p>
<p>This tutorial assumes that you have read the <a class="reference internal" href="../large/L1_large_node_classification.html"><span class="doc">Training GNN with Neighbor
Sampling for Node Classification</span></a>
tutorial. It also assumes that you know the basics of training general
models with multi-GPU with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">this tutorial</a>
from PyTorch for general multi-GPU training with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>.  Also,
see the first section of <a class="reference internal" href="1_graph_classification.html"><span class="doc">the multi-GPU graph classification
tutorial</span></a>
for an overview of using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> with DGL.</p>
</div>
<section id="loading-dataset">
<h2>Loading Dataset<a class="headerlink" href="#loading-dataset" title="Link to this heading">ÔÉÅ</a></h2>
<p>OGB already prepared the data as a <code class="docutils literal notranslate"><span class="pre">DGLGraph</span></code> object. The following code is
copy-pasted from the <a class="reference internal" href="../large/L1_large_node_classification.html"><span class="doc">Training GNN with Neighbor Sampling for Node
Classification</span></a>
tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<a class="sphx-glr-backref-module-os sphx-glr-backref-type-py-data" href="https://docs.python.org/3/library/os.html#os.environ" title="os.environ"><span class="n">os</span><span class="o">.</span><span class="n">environ</span></a><span class="p">[</span><span class="s2">"DGLBACKEND"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"pytorch"</span>
<span class="kn">import</span> <span class="nn">dgl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">tqdm</span>
<span class="kn">from</span> <span class="nn">dgl.nn</span> <span class="kn">import</span> <span class="n">SAGEConv</span>
<span class="kn">from</span> <span class="nn">ogb.nodeproppred</span> <span class="kn">import</span> <span class="n">DglNodePropPredDataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">DglNodePropPredDataset</span><span class="p">(</span><span class="s2">"ogbn-arxiv"</span><span class="p">)</span>

<span class="n">graph</span><span class="p">,</span> <span class="n">node_labels</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Add reverse edges since ogbn-arxiv is unidirectional.</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">add_reverse_edges</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_labels</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">node_features</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">"feat"</span><span class="p">]</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">num_features</span></a> <span class="o">=</span> <span class="n">node_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">num_classes</span></a> <span class="o">=</span> <span class="p">(</span><span class="n">node_labels</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict"><span class="n">idx_split</span></a> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_idx_split</span><span class="p">()</span>
<span class="n">train_nids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict"><span class="n">idx_split</span></a><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">valid_nids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict"><span class="n">idx_split</span></a><span class="p">[</span><span class="s2">"valid"</span><span class="p">]</span>
<span class="n">test_nids</span> <span class="o">=</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict"><span class="n">idx_split</span></a><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>  <span class="c1"># Test node IDs, not used in the tutorial though.</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip

  0%|          | 0/81 [00:00&lt;?, ?it/s]
Downloaded 0.00 GB:   0%|          | 0/81 [00:00&lt;?, ?it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:00&lt;00:25,  3.13it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:00&lt;00:25,  3.13it/s]
Downloaded 0.00 GB:   1%|          | 1/81 [00:00&lt;00:25,  3.13it/s]
Downloaded 0.00 GB:   4%|‚ñé         | 3/81 [00:00&lt;00:09,  7.85it/s]
Downloaded 0.00 GB:   4%|‚ñé         | 3/81 [00:00&lt;00:09,  7.85it/s]
Downloaded 0.00 GB:   4%|‚ñé         | 3/81 [00:00&lt;00:09,  7.85it/s]
Downloaded 0.01 GB:   4%|‚ñé         | 3/81 [00:00&lt;00:09,  7.85it/s]
Downloaded 0.01 GB:   4%|‚ñé         | 3/81 [00:00&lt;00:09,  7.85it/s]
Downloaded 0.01 GB:   4%|‚ñé         | 3/81 [00:00&lt;00:09,  7.85it/s]
Downloaded 0.01 GB:  10%|‚ñâ         | 8/81 [00:00&lt;00:03, 19.21it/s]
Downloaded 0.01 GB:  10%|‚ñâ         | 8/81 [00:00&lt;00:03, 19.21it/s]
Downloaded 0.01 GB:  10%|‚ñâ         | 8/81 [00:00&lt;00:03, 19.21it/s]
Downloaded 0.01 GB:  10%|‚ñâ         | 8/81 [00:00&lt;00:03, 19.21it/s]
Downloaded 0.01 GB:  10%|‚ñâ         | 8/81 [00:00&lt;00:03, 19.21it/s]
Downloaded 0.01 GB:  10%|‚ñâ         | 8/81 [00:00&lt;00:03, 19.21it/s]
Downloaded 0.01 GB:  10%|‚ñâ         | 8/81 [00:00&lt;00:03, 19.21it/s]
Downloaded 0.01 GB:  17%|‚ñà‚ñã        | 14/81 [00:00&lt;00:02, 29.98it/s]
Downloaded 0.01 GB:  17%|‚ñà‚ñã        | 14/81 [00:00&lt;00:02, 29.98it/s]
Downloaded 0.02 GB:  17%|‚ñà‚ñã        | 14/81 [00:00&lt;00:02, 29.98it/s]
Downloaded 0.02 GB:  17%|‚ñà‚ñã        | 14/81 [00:00&lt;00:02, 29.98it/s]
Downloaded 0.02 GB:  17%|‚ñà‚ñã        | 14/81 [00:00&lt;00:02, 29.98it/s]
Downloaded 0.02 GB:  17%|‚ñà‚ñã        | 14/81 [00:00&lt;00:02, 29.98it/s]
Downloaded 0.02 GB:  17%|‚ñà‚ñã        | 14/81 [00:00&lt;00:02, 29.98it/s]
Downloaded 0.02 GB:  17%|‚ñà‚ñã        | 14/81 [00:00&lt;00:02, 29.98it/s]
Downloaded 0.02 GB:  26%|‚ñà‚ñà‚ñå       | 21/81 [00:00&lt;00:01, 40.95it/s]
Downloaded 0.02 GB:  26%|‚ñà‚ñà‚ñå       | 21/81 [00:00&lt;00:01, 40.95it/s]
Downloaded 0.02 GB:  26%|‚ñà‚ñà‚ñå       | 21/81 [00:00&lt;00:01, 40.95it/s]
Downloaded 0.02 GB:  26%|‚ñà‚ñà‚ñå       | 21/81 [00:00&lt;00:01, 40.95it/s]
Downloaded 0.02 GB:  26%|‚ñà‚ñà‚ñå       | 21/81 [00:00&lt;00:01, 40.95it/s]
Downloaded 0.03 GB:  26%|‚ñà‚ñà‚ñå       | 21/81 [00:00&lt;00:01, 40.95it/s]
Downloaded 0.03 GB:  26%|‚ñà‚ñà‚ñå       | 21/81 [00:00&lt;00:01, 40.95it/s]
Downloaded 0.03 GB:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/81 [00:00&lt;00:01, 46.04it/s]
Downloaded 0.03 GB:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/81 [00:00&lt;00:01, 46.04it/s]
Downloaded 0.03 GB:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/81 [00:00&lt;00:01, 46.04it/s]
Downloaded 0.03 GB:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/81 [00:00&lt;00:01, 46.04it/s]
Downloaded 0.03 GB:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/81 [00:00&lt;00:01, 46.04it/s]
Downloaded 0.03 GB:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/81 [00:00&lt;00:01, 46.04it/s]
Downloaded 0.03 GB:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/81 [00:00&lt;00:01, 46.04it/s]
Downloaded 0.03 GB:  33%|‚ñà‚ñà‚ñà‚ñé      | 27/81 [00:00&lt;00:01, 46.04it/s]
Downloaded 0.03 GB:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/81 [00:00&lt;00:00, 52.79it/s]
Downloaded 0.03 GB:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/81 [00:00&lt;00:00, 52.79it/s]
Downloaded 0.04 GB:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/81 [00:01&lt;00:00, 52.79it/s]
Downloaded 0.04 GB:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/81 [00:01&lt;00:00, 52.79it/s]
Downloaded 0.04 GB:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/81 [00:01&lt;00:00, 52.79it/s]
Downloaded 0.04 GB:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/81 [00:01&lt;00:00, 52.79it/s]
Downloaded 0.04 GB:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/81 [00:01&lt;00:00, 52.79it/s]
Downloaded 0.04 GB:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 34/81 [00:01&lt;00:00, 52.79it/s]
Downloaded 0.04 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.04 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.04 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.04 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.04 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.04 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.05 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.05 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.05 GB:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 41/81 [00:01&lt;00:00, 56.65it/s]
Downloaded 0.05 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.05 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.05 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.05 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.05 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.05 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.05 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.05 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.06 GB:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 49/81 [00:01&lt;00:00, 60.45it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 57/81 [00:01&lt;00:00, 63.00it/s]
Downloaded 0.06 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.06 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.07 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.07 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.07 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.07 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.07 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.07 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.07 GB:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 65/81 [00:01&lt;00:00, 64.71it/s]
Downloaded 0.07 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.07 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.07 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.07 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.08 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.08 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.08 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.08 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.08 GB:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 73/81 [00:01&lt;00:00, 65.90it/s]
Downloaded 0.08 GB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:01&lt;00:00, 49.45it/s]
Extracting dataset/arxiv.zip
Loading necessary files...
This might take a while.
Processing graphs...

  0%|          | 0/1 [00:00&lt;?, ?it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 29537.35it/s]
Converting graphs into DGL objects...

  0%|          | 0/1 [00:00&lt;?, ?it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 277.75it/s]
Saving...
</pre></div>
</div>
</section>
<section id="defining-model">
<h2>Defining Model<a class="headerlink" href="#defining-model" title="Link to this heading">ÔÉÅ</a></h2>
<p>The model will be again identical to the <a class="reference internal" href="../large/L1_large_node_classification.html"><span class="doc">Training GNN with Neighbor
Sampling for Node Classification</span></a>
tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_feats</span><span class="p">,</span> <span class="n">h_feats</span><span class="p">,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">num_classes</span></a><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">SAGEConv</span><span class="p">(</span><span class="n">in_feats</span><span class="p">,</span> <span class="n">h_feats</span><span class="p">,</span> <span class="n">aggregator_type</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">SAGEConv</span><span class="p">(</span><span class="n">h_feats</span><span class="p">,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">num_classes</span></a><span class="p">,</span> <span class="n">aggregator_type</span><span class="o">=</span><span class="s2">"mean"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h_feats</span> <span class="o">=</span> <span class="n">h_feats</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mfgs</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h_dst</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span> <span class="n">mfgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_dst_nodes</span><span class="p">()]</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">mfgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h_dst</span><span class="p">))</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h_dst</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:</span> <span class="n">mfgs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">num_dst_nodes</span><span class="p">()]</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">mfgs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">h_dst</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">h</span>
</pre></div>
</div>
</section>
<section id="defining-training-procedure">
<h2>Defining Training Procedure<a class="headerlink" href="#defining-training-procedure" title="Link to this heading">ÔÉÅ</a></h2>
<p>The training procedure will be slightly different from what you saw
previously, in the sense that you will need to</p>
<ul class="simple">
<li><p>Initialize a distributed training context with <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>.</p></li>
<li><p>Wrap your model with <code class="docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code>.</p></li>
<li><p>Add a <code class="docutils literal notranslate"><span class="pre">use_ddp=True</span></code> argument to the DGL dataloader you wish to run
together with DDP.</p></li>
</ul>
<p>You will also need to wrap the training loop inside a function so that
you can spawn subprocesses to run it.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">proc_id</span><span class="p">,</span> <span class="n">devices</span><span class="p">):</span>
    <span class="c1"># Initialize distributed training context.</span>
    <span class="n">dev_id</span> <span class="o">=</span> <span class="n">devices</span><span class="p">[</span><span class="n">proc_id</span><span class="p">]</span>
    <span class="n">dist_init_method</span> <span class="o">=</span> <span class="s2">"tcp://</span><span class="si">{master_ip}</span><span class="s2">:</span><span class="si">{master_port}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">master_ip</span><span class="o">=</span><span class="s2">"127.0.0.1"</span><span class="p">,</span> <span class="n">master_port</span><span class="o">=</span><span class="s2">"12345"</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
            <span class="n">backend</span><span class="o">=</span><span class="s2">"gloo"</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">dist_init_method</span><span class="p">,</span>
            <span class="n">world_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">proc_id</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">dev_id</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dev_id</span><span class="p">))</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
            <span class="n">backend</span><span class="o">=</span><span class="s2">"nccl"</span><span class="p">,</span>
            <span class="n">init_method</span><span class="o">=</span><span class="n">dist_init_method</span><span class="p">,</span>
            <span class="n">world_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">proc_id</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Define training and validation dataloader, copied from the previous tutorial</span>
    <span class="c1"># but with one line of difference: use_ddp to enable distributed data parallel</span>
    <span class="c1"># data loading.</span>
    <span class="n">sampler</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">NeighborSampler</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class" href="https://docs.python.org/3/library/typing.html#typing.Generic" title="typing.Generic"><span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">DataLoader</span></a><span class="p">(</span>
        <span class="c1"># The following arguments are specific to DataLoader.</span>
        <span class="n">graph</span><span class="p">,</span>  <span class="c1"># The graph</span>
        <span class="n">train_nids</span><span class="p">,</span>  <span class="c1"># The node IDs to iterate over in minibatches</span>
        <span class="n">sampler</span><span class="p">,</span>  <span class="c1"># The neighbor sampler</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>  <span class="c1"># Put the sampled MFGs on CPU or GPU</span>
        <span class="n">use_ddp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Make it work with distributed data parallel</span>
        <span class="c1"># The following arguments are inherited from PyTorch DataLoader.</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>  <span class="c1"># Per-device batch size.</span>
        <span class="c1"># The effective batch size is this number times the number of GPUs.</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Whether to shuffle the nodes for every epoch</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Whether to drop the last incomplete batch</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Number of sampler processes</span>
    <span class="p">)</span>
    <span class="n">valid_dataloader</span> <span class="o">=</span> <a class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class" href="https://docs.python.org/3/library/typing.html#typing.Generic" title="typing.Generic"><span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">DataLoader</span></a><span class="p">(</span>
        <span class="n">graph</span><span class="p">,</span>
        <span class="n">valid_nids</span><span class="p">,</span>
        <span class="n">sampler</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">use_ddp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">num_features</span></a><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance" href="https://docs.python.org/3/library/functions.html#int" title="builtins.int"><span class="n">num_classes</span></a><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Wrap the model with distributed data parallel module.</span>
    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-abc sphx-glr-backref-type-py-class" href="https://docs.python.org/3/library/abc.html#abc.ABC" title="abc.ABC"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span></a><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_device</span><span class="o">=</span><span class="kc">None</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <a class="sphx-glr-backref-module-abc sphx-glr-backref-type-py-class" href="https://docs.python.org/3/library/abc.html#abc.ABC" title="abc.ABC"><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span></a><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">device</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>

    <span class="c1"># Define optimizer</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">best_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">best_model_path</span> <span class="o">=</span> <span class="s2">"./model.pt"</span>

    <span class="c1"># Copied from previous tutorial with changes highlighted.</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="k">as</span> <span class="n">tq</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">mfgs</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tq</span><span class="p">):</span>
                <span class="c1"># feature copy from CPU to GPU takes place here</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">mfgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">srcdata</span><span class="p">[</span><span class="s2">"feat"</span><span class="p">]</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">mfgs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dstdata</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]</span>

                <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">mfgs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="n">accuracy</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span>
                    <span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                    <span class="n">predictions</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                <span class="p">)</span>

                <span class="n">tq</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">"loss"</span><span class="p">:</span> <span class="s2">"</span><span class="si">%.03f</span><span class="s2">"</span> <span class="o">%</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s2">"acc"</span><span class="p">:</span> <span class="s2">"</span><span class="si">%.03f</span><span class="s2">"</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">},</span>
                    <span class="n">refresh</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># Evaluate on only the first GPU.</span>
        <span class="k">if</span> <span class="n">proc_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">with</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">valid_dataloader</span><span class="p">)</span> <span class="k">as</span> <span class="n">tq</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">mfgs</span> <span class="ow">in</span> <span class="n">tq</span><span class="p">:</span>
                    <span class="n">inputs</span> <span class="o">=</span> <span class="n">mfgs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">srcdata</span><span class="p">[</span><span class="s2">"feat"</span><span class="p">]</span>
                    <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mfgs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dstdata</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
                    <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">model</span><span class="p">(</span><span class="n">mfgs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                    <span class="p">)</span>
                <span class="n">predictions</span> <span class="o">=</span> <a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function" href="https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html#numpy.concatenate" title="numpy.concatenate"><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span></a><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
                <span class="n">labels</span> <span class="o">=</span> <a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function" href="https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html#numpy.concatenate" title="numpy.concatenate"><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span></a><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
                <span class="n">accuracy</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch </span><span class="si">{}</span><span class="s2"> Validation Accuracy </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">best_accuracy</span> <span class="o">&lt;</span> <span class="n">accuracy</span><span class="p">:</span>
                    <span class="n">best_accuracy</span> <span class="o">=</span> <span class="n">accuracy</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">best_model_path</span><span class="p">)</span>

        <span class="c1"># Note that this tutorial does not train the whole model to the end.</span>
        <span class="k">break</span>
</pre></div>
</div>
</section>
<section id="spawning-trainer-processes">
<h2>Spawning Trainer Processes<a class="headerlink" href="#spawning-trainer-processes" title="Link to this heading">ÔÉÅ</a></h2>
<p>A typical scenario for multi-GPU training with DDP is to replicate the
model once per GPU, and spawn one trainer process per GPU.</p>
<p>Normally, DGL maintains only one sparse matrix representation (usually COO)
for each graph, and will create new formats when some APIs are called for
efficiency.  For instance, calling <code class="docutils literal notranslate"><span class="pre">in_degrees</span></code> will create a CSC
representation for the graph, and calling <code class="docutils literal notranslate"><span class="pre">out_degrees</span></code> will create a
CSR representation.  A consequence is that if a graph is shared to
trainer processes via copy-on-write <em>before</em> having its CSC/CSR
created, each trainer will create its own CSC/CSR replica once <code class="docutils literal notranslate"><span class="pre">in_degrees</span></code>
or <code class="docutils literal notranslate"><span class="pre">out_degrees</span></code> is called.  To avoid this, you need to create
all sparse matrix representations beforehand using the <code class="docutils literal notranslate"><span class="pre">create_formats_</span></code>
method:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">create_formats_</span><span class="p">()</span>
</pre></div>
</div>
<p>Then you can spawn the subprocesses to train with multiple GPUs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Say you have four GPUs.</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>
    <span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">run</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">)),),</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">num_gpus</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Thumbnail credits: Stanford CS224W Notes</span>
<span class="c1"># sphinx_gallery_thumbnail_path = '_static/blitz_1_introduction.png'</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 6.136 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-multi-2-node-classification-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/0c638c5fe1559d5c5412450f6d33520e/2_node_classification.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">2_node_classification.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/3aa3eaf84cee1a4ab77e086b67de8362/2_node_classification.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">2_node_classification.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/07263c83904e04064b4f22ef48036843/2_node_classification.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">2_node_classification.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</div>
</div>
<footer><div aria-label="Footer" class="rst-footer-buttons" role="navigation">
<a accesskey="p" class="btn btn-neutral float-left" href="1_graph_classification.html" rel="prev" title="Single Machine Multi-GPU Minibatch Graph Classification"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
<a accesskey="n" class="btn btn-neutral float-right" href="../dist/index.html" rel="next" title="Distributed training">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
</div>
<hr/>
<div role="contentinfo">
<p>¬© Copyright 2018, DGL Team.</p>
</div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
</div>
</div>
</section>
</div>
<div aria-label="Versions" class="rst-versions" data-toggle="rst-versions" role="note">
<span class="rst-current-version" data-toggle="rst-current-version">
<span class="fa fa-book"> Read the Docs</span>
<span id="version-placeholder">v: latest</span>
<span class="fa fa-caret-down"></span>
</span>
<div class="rst-other-versions">
<dl>
<dt>Versions</dt>
<div id="version-list">
<!-- Âä®ÊÄÅÊèíÂÖ•ÁöÑÁâàÊú¨ÂàóË°®Â∞ÜÂá∫Áé∞Âú®ËøôÈáå -->
</div>
</dl>
<dl>
<dt>Downloads</dt>
<!-- ‰∏ãËΩΩÂÜÖÂÆπ -->
</dl>
<dl>
<dt>On Read the Docs</dt>
<dd><a href="//doc-build.dgl.ai/projects/dgl/?fromdocs=dgl">Project Home</a></dd>
<dd><a href="//doc-build.dgl.ai/builds/dgl/?fromdocs=dgl">Builds</a></dd>
</dl>
</div>
</div>
<script>
        document.addEventListener("DOMContentLoaded", function() {
            fetch('/dgl_docs/branches.json')
                .then(response => response.json())
                .then(data => {
                    var versionListDiv = document.getElementById('version-list');
                    data.branches.forEach(function(branch) {
                        var dd = document.createElement('dd');
                        var a = document.createElement('a');
                        a.href = branch.url;
                        a.textContent = branch.name;
                        dd.appendChild(a);
                        versionListDiv.appendChild(dd);
                    });
                })
                .catch(error => console.error('Error loading branches:', error));
        });
        document.addEventListener("DOMContentLoaded", function() {
            // Ëé∑ÂèñÂΩìÂâçË∑ØÂæÑ
            var path = window.location.pathname;
            var versionPlaceholder = document.getElementById('version-placeholder');

            // Ê£ÄÊü•Ë∑ØÂæÑ‰∏≠ÊòØÂê¶ÂåÖÂê´ 'en'
            if (path.includes('/en/')) {
                // ÊèêÂèñ 'en' ÂêéÁöÑÊñá‰ª∂Â§π‰Ωú‰∏∫ÁâàÊú¨Âè∑
                var parts = path.split('/en/');
                if (parts[1]) {
                    var folders = parts[1].split('/');
                    if (folders.length > 0 && folders[0]) {
                        versionPlaceholder.textContent = 'v: ' + folders[0];
                    } else {
                        versionPlaceholder.textContent = 'v: latest';
                    }
                } else {
                    versionPlaceholder.textContent = 'v: latest';
                }
            } else {
                versionPlaceholder.textContent = 'v: latest';
            }
        });
    </script>

<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
</body>
</html>