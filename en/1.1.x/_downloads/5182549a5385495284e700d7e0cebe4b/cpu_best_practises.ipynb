{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# CPU Best Practices\n\nThis chapter focus on providing best practises for environment setup\nto get the best performance during training and inference on the CPU.\n\n## Intel\n\n### Hyper-treading\n\nFor specific workloads as GNN\u2019s domain, suggested default setting for having best performance\nis to turn off hyperthreading.\nTurning off the hyper threading feature can be done at BIOS [#f1]_ or operating system level [#f2]_ [#f3]_ .\n\n\n### OpenMP settings\n\nDuring training on CPU, the training and dataloading part need to be maintained simultaneously.\nBest performance of parallelization in OpenMP\ncan be achieved by setting up the optimal number of working threads and dataloading workers.\nNodes with high number of CPU cores may benefit from higher number of dataloading workers.\nA good starting point could be setting num_threads=4 in Dataloader constructor for nodes with 32 cores or more.\nIf number of cores is rather small, the best performance might be achieved with just one\ndataloader worker or even with dataloader num_threads=0 for dataloading and trainig performed\nin the same process\n\n**Dataloader CPU affinity**\n\nIf number of dataloader workers is more than 0, please consider using **use_cpu_affinity()** method\nof DGL Dataloader class, it will generally result in significant performance improvement for training.\n\n*use_cpu_affinity* will set the proper OpenMP thread count (equal to the number of CPU cores allocated for main process),\naffinitize dataloader workers for separate CPU cores and restrict the main process to remaining cores\n\nIn multiple NUMA nodes setups *use_cpu_affinity* will only use cores of NUMA node 0 by default\nwith an assumption, that the workload is scaling poorly across multiple NUMA nodes. If you believe\nyour workload will have better performance utilizing more than one NUMA node, you can pass\nthe list of cores to use for dataloading (loader_cores) and for compute (compute_cores).\n\nloader_cores and compute_cores arguments (list of CPU cores) can be passed to *enable_cpu_affinity* for more\ncontrol over which cores should be used, e.g. in case a workload scales well across multiple NUMA nodes.\n\nUsage:\n    .. code:: python\n\n        dataloader = dgl.dataloading.DataLoader(...)\n        ...\n        with dataloader.enable_cpu_affinity():\n            <training loop or inferencing>\n\n**Manual control**\n\nFor advanced and more fine-grained control over OpenMP settings please refer to Maximize Performance of Intel\u00ae Optimization for PyTorch* on CPU [#f4]_ article\n\n.. rubric:: Footnotes\n\n.. [#f1] https://www.intel.com/content/www/us/en/support/articles/000007645/boards-and-kits/desktop-boards.html\n.. [#f2] https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-linux/\n.. [#f3] https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-ec2-windows-instances/\n.. [#f4] https://software.intel.com/content/www/us/en/develop/articles/how-to-get-better-performance-on-pytorchcaffe2-with-intel-acceleration.html\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}